/**
* This file is part of ORB-SLAM.
*
* Copyright (C) 2014 Ra√∫l Mur-Artal <raulmur at unizar dot es> (University of Zaragoza)
* For more information see <http://webdiis.unizar.es/~raulmur/orbslam/>
*
* ORB-SLAM is free software: you can redistribute it and/or modify
* it under the terms of the GNU General Public License as published by
* the Free Software Foundation, either version 3 of the License, or
* (at your option) any later version.
*
* ORB-SLAM is distributed in the hope that it will be useful,
* but WITHOUT ANY WARRANTY; without even the implied warranty of
* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
* GNU General Public License for more details.
*
* You should have received a copy of the GNU General Public License
* along with ORB-SLAM. If not, see <http://www.gnu.org/licenses/>.
*/
#include "Tracking.h"
#include "g2o_types/global.h" //performance monitor
#include "g2o_types/IMU_constraint.h"

#include <g2o/core/block_solver.h>
#include <g2o/core/optimization_algorithm_levenberg.h>
#include <g2o/solvers/csparse/linear_solver_csparse.h>
#include <g2o/solvers/cholmod/linear_solver_cholmod.h> //for cholmod
#include <g2o/core/robust_kernel_impl.h>
#include <g2o/solvers/structure_only/structure_only_solver.h>

#include <opencv2/core/eigen.hpp>

//#include<ros/ros.h>
//#include <cv_bridge/cv_bridge.h>

#include "timegrabber.h"
#include<opencv2/opencv.hpp>

#include"ORBmatcher.h"
#include"FramePublisher.h"
#include"Converter.h"
#include"Map.h"
#include"Initializer.h"

#include"Optimizer.h"
#include"PnPsolver.h"

#include <vikit/pinhole_camera.h>

#include<iostream>
#include<fstream>
#include <queue>
#include <utility>

using namespace std;
using namespace Sophus;
using namespace ScaViSLAM;

namespace ORB_SLAM
{
#ifdef SLAM_TRACE
vk::PerformanceMonitor* g_permon = NULL;
#endif

static bool to_bool(std::string str) {
    std::transform(str.begin(), str.end(), str.begin(), ::tolower);
    std::istringstream is(str);
    bool b;
    is >> std::boolalpha >> b;
    return b;
}

Tracking::Tracking(ORBVocabulary* pVoc, FramePublisher*pFramePublisher, /*MapPublisher *pMapPublisher,*/ Map *pMap, string strSettingPath):
    mState(NO_IMAGES_YET),mpCurrentFrame(NULL),mpORBVocabulary(pVoc), mpInitializer(NULL), mpFramePublisher(pFramePublisher),/*mpMapPublisher(pMapPublisher),*/ mpMap(pMap),
    mfsSettings(strSettingPath, cv::FileStorage::READ),mpLastKeyFrame(NULL), mpLastFrame(NULL),
    mnLastRelocFrameId(0), mbPublisherStopped(false), mbReseting(false), mbForceRelocalisation(false), mbMotionModel(false),
    mbUseIMUData(false), mnStartId(mfsSettings["startIndex"]), mnFinishId(mfsSettings["finishIndex"]), mnFeatures(mfsSettings["ORBextractor.nFeatures"])
{
#ifdef SLAM_TRACE
  // Initialize Performance Monitor
  g_permon = new vk::PerformanceMonitor();
  g_permon->addTimer("extract_quadmatches");
  g_permon->addTimer("track_previous_frame");
  g_permon->addTimer("stereo_matching");
  g_permon->addTimer("create_frame");

  g_permon->addTimer("local_optimize");
  g_permon->addTimer("triangulate_new_mappoint");
  g_permon->addTimer("tot_time");
  g_permon->addLog("timestamp");
  string trace_name("svo");
  string trace_dir("/home/jhuai/ORB_SLAM/Data");
  g_permon->init(trace_name, trace_dir);
#endif
    // Load camera parameters from settings file

    float fx = mfsSettings["Camera.fx"];
    float fy = mfsSettings["Camera.fy"];
    float cx = mfsSettings["Camera.cx"];
    float cy = mfsSettings["Camera.cy"];
    Eigen::Matrix<double, 4, 1> distCoef;
    distCoef(0) = mfsSettings["Camera.k1"];
    distCoef(1) = mfsSettings["Camera.k2"];
    distCoef(2) = mfsSettings["Camera.p1"];
    distCoef(3) = mfsSettings["Camera.p2"];
    double width= mfsSettings["Camera.width"], height= mfsSettings["Camera.height"];
    cam_ = new vk::PinholeCamera( width, height, fx, fy, cx,cy,
            distCoef(0), distCoef(1),
            distCoef(2),distCoef(3));
  
 #ifndef MONO
        cv::Mat matTl2r;
        mfsSettings["Stereo.se3Left2Right"]>>matTl2r;    
        mTl2r= Converter::toSE3d(matTl2r);
        cv::Mat rightK, rightDistCoef;
        mfsSettings["Stereo.matRightK"]>>rightK;
        mfsSettings["Stereo.matRightDistCoef"]>>rightDistCoef;        

        right_cam_ = new vk::PinholeCamera(width, height,
                rightK.at<float>(0,0), rightK.at<float>(1,1),
                rightK.at<float>(0,2), rightK.at<float>(1,2),
                rightDistCoef.at<float>(0), rightDistCoef.at<float>(1),
                rightDistCoef.at<float>(2),rightDistCoef.at<float>(3));
 #endif
    mFps = mfsSettings["Camera.fps"];
    if(mFps==0)
        mFps=30;

    // Max/Min Frames to insert keyframes and to check relocalisation
    mMinFrames = 0;
    mMaxFrames = 3;//18*mFps/30;

    cout << "Camera Parameters: " << endl;
    cout << "- fx: " << fx << endl;
    cout << "- fy: " << fy << endl;
    cout << "- cx: " << cx << endl;
    cout << "- cy: " << cy << endl;
    cout << "- k1: " << distCoef(0) << endl;
    cout << "- k2: " << distCoef(1) << endl;
    cout << "- p1: " << distCoef(2) << endl;
    cout << "- p2: " << distCoef(3) << endl;
    cout << "- fps: " << mFps << endl;


    //IMU related parameters, assume the world frame is the LEFT camera frame at startIndex image, right down forward
    //given image indexed k and its timestamp t(k), assume we have IMU readings indexed p(k) s.t. t(p(k)-1)<=t(k)<t(p(k))
    mbUseIMUData=to_bool(mfsSettings["use_imu_data"]);
    ginw.setZero();
    if(mbUseIMUData)
    {
        imu_sample_interval=mfsSettings["sample_interval"];
        cv::Mat na, nw, acc_bias_var, gyro_bias_var;
        mfsSettings["na"]>>na;
        mfsSettings["nw"]>>nw;
        mfsSettings["acc_bias_var"]>>acc_bias_var;
        mfsSettings["gyro_bias_var"]>>gyro_bias_var;

        double acc_bias_Tc=mfsSettings["acc_bias_Tc"];        //half correlation time
        double gyro_bias_Tc=mfsSettings["gyro_bias_Tc"];

        Eigen::Vector3d q_noise_acc;
        cv::cv2eigen(na, q_noise_acc);
        q_noise_acc=q_noise_acc.cwiseAbs2();
        Eigen::Vector3d q_noise_gyr;
        cv::cv2eigen(nw, q_noise_gyr);
        q_noise_gyr*=(M_PI/180);
        q_noise_gyr=q_noise_gyr.cwiseAbs2();
        Eigen::Vector3d q_noise_accbias;
        cv::cv2eigen(acc_bias_var, q_noise_accbias);
        q_noise_accbias=q_noise_accbias.cwiseAbs2();
        q_noise_accbias*=(2/acc_bias_Tc);
        Eigen::Vector3d q_noise_gyrbias;
        cv::cv2eigen(gyro_bias_var, q_noise_gyrbias);
        q_noise_gyrbias*=(M_PI/180);
        q_noise_gyrbias=q_noise_gyrbias.cwiseAbs2();
        q_noise_gyrbias*=(2/gyro_bias_Tc);

        imu_.q_n_aw_babw.head(3)=q_noise_acc;
        imu_.q_n_aw_babw.segment(3,3)=q_noise_gyr;
        imu_.q_n_aw_babw.segment(6,3)=q_noise_accbias;
        imu_.q_n_aw_babw.tail(3)=q_noise_gyrbias;

        cv::Mat Rs2c, tsinc;
        mfsSettings["Rs2c"]>>Rs2c; mfsSettings["tsinc"]>>tsinc;
        Eigen::Matrix3d tempRs2c;
        Eigen::Vector3d tempVec3d;
        cv::cv2eigen(Rs2c, tempRs2c);
        cv::cv2eigen(tsinc, tempVec3d);
        SE3d T_s_2_c(tempRs2c, tempVec3d);
        imu_.T_imu_from_cam=T_s_2_c.inverse();

        cv::Mat matGinw, omegaew;
        mfsSettings["gw"]>>matGinw;
        mfsSettings["wiew"]>>omegaew;

        imu_.gwomegaw.setZero();
        cv::cv2eigen(matGinw, ginw);
        imu_.gwomegaw.head(3)=ginw;
        cv::cv2eigen(omegaew, tempVec3d);
        imu_.gwomegaw.tail(3)=tempVec3d;
    }

    // set most important visual odometry parameters
    // for a full parameter list, look at: viso_stereo.h
    libviso2::VisualOdometryStereo::parameters param;
    param.calib.f  = mfsSettings["Camera.fx"]; // focal length in pixels
    param.calib.cu = mfsSettings["Camera.cx"]; // principal point (u-coordinate) in pixels
    param.calib.cv = mfsSettings["Camera.cy"]; // principal point (v-coordinate) in pixels
    param.base     = -mTl2r.translation()[0]; // baseline in meters
    param.inlier_threshold =sqrt(5.991);
    mVisoStereo.setParameters(param);
    mPose=libviso2::Matrix::eye(4);

    int nRGB = mfsSettings["Camera.RGB"];
    mbRGB = nRGB;

    if(mbRGB)
        cout << "- color order: RGB (ignored if grayscale)" << endl;
    else
        cout << "- color order: BGR (ignored if grayscale)" << endl;

    // Load ORB parameters

    float fScaleFactor = mfsSettings["ORBextractor.scaleFactor"];
    int nLevels = mfsSettings["ORBextractor.nLevels"];
    int fastTh = mfsSettings["ORBextractor.fastTh"];
    int Score = mfsSettings["ORBextractor.nScoreType"];

    assert(Score==1 || Score==0);

    mpORBextractor = new ORBextractor(mnFeatures,fScaleFactor,nLevels,Score,fastTh);

    cout << endl  << "ORB Extractor Parameters: " << endl;
    cout << "- Number of Features: " << mnFeatures << endl;
    cout << "- Scale Levels: " << nLevels << endl;
    cout << "- Scale Factor: " << fScaleFactor << endl;
    cout << "- Fast Threshold: " << fastTh << endl;
    if(Score==0)
        cout << "- Score: HARRIS" << endl;
    else
        cout << "- Score: FAST" << endl;


    // ORB extractor for initialization
    // Initialization uses only points from the finest scale level
    mpIniORBextractor = new ORBextractor(mnFeatures*2,1.2,8,Score,fastTh);

    int nMotion = mfsSettings["UseMotionModel"];
    mbMotionModel = nMotion;

    if(mbMotionModel)
    {
        mVelocity = Sophus::SE3d();
        cout << endl << "Motion Model: Enabled" << endl << endl;
    }
    else
        cout << endl << "Motion Model: Disabled (not recommended, change settings UseMotionModel: 1)" << endl << endl;


//    tf::Transform tfT;
//    tfT.setIdentity();
//    mTfBr.sendTransform(tf::StampedTransform(tfT,ros::Time::now(), "/ORB_SLAM/World", "/ORB_SLAM/Camera"));
}
Tracking::~Tracking()
{
#ifdef SLAM_TRACE
  delete g_permon;
#endif
    delete cam_;
#ifndef MONO
    delete right_cam_;
#endif
}
void Tracking::SetLocalMapper(LocalMapping *pLocalMapper)
{
    mpLocalMapper=pLocalMapper;
}

void Tracking::SetLoopClosing(LoopClosing *pLoopClosing)
{
    mpLoopClosing=pLoopClosing;
}

void Tracking::SetKeyFrameDatabase(KeyFrameDatabase *pKFDB)
{
    mpKeyFrameDB = pKFDB;
}

void Tracking::Run()
{
    //    ros::NodeHandle nodeHandler;
    //    ros::Subscriber sub = nodeHandler.subscribe("/camera/image_raw", 1, &Tracking::GrabImage, this);

    //    ros::spin();
    enum dataset_type {KITTISeq00=0, Tsukuba, MalagaUrbanExtract6 } experim=Tsukuba;

    string dataset=mfsSettings["dataset"];
    if (dataset.compare("KITTISeq00")==0)
        experim =KITTISeq00;
    else if (dataset.compare("Tsukuba")==0)
        experim=Tsukuba;
    else if(dataset.compare("MalagaUrbanExtract6")==0)
        experim=MalagaUrbanExtract6;
    else{
        cerr<<"Unsupported dataset:"<<dataset<<endl;
        exit(-1);
    }

    int numImages=mnStartId;
    int totalImages=mnFinishId;
    string dir = mfsSettings["input_path"]; // sequence directory

    string output_file=mfsSettings["output_file"];
    ofstream out_stream(output_file.c_str(), std::ios::out);
    out_stream<<"%Each row is timestamp, pos of camera in world frame, rotation to world from camera frame in quaternion xyzw format"<<endl;
    out_stream << fixed;

    string time_filename=mfsSettings["time_file"]; //timestamps for frames
    TimeGrabber tg(time_filename);

    ScaViSLAM::IMUProcessor* imu_proc=NULL;
    Sophus::SE3d T_s1_to_w;
    Eigen::Matrix<double, 9, 1> speed_bias_1=Eigen::Matrix<double, 9, 1>::Zero();
    Sophus::SE3d predTcp;
    if(mbUseIMUData){
        string imu_file=mfsSettings["imu_file"];
        imu_proc=new ScaViSLAM::IMUProcessor(imu_file, imu_sample_interval, imu_);

        //initialize IMU states
        T_s1_to_w=imu_.T_imu_from_cam.inverse();
        cv::Mat vs0inw;
        mfsSettings["vs0inw"]>>vs0inw;

        speed_bias_1[0]=vs0inw.at<double>(0);
        speed_bias_1[1]=vs0inw.at<double>(1);
        speed_bias_1[2]=vs0inw.at<double>(2);
    }

    double time_pair[2]={-1,-1};              // timestamps of the previous and current images
    char base_name[256];                // input file names
    string left_img_file_name;
    string right_img_file_name;
    double time_frame(-1);                  //timestamp of current frame

    vk::Timer timer_;             //!< Stopwatch to measure time to process frame.
    timer_.start();
    while(numImages<=totalImages)
    {
        switch(experim){
        case KITTISeq00:
            sprintf(base_name,"%06d.png",numImages);
            left_img_file_name  = dir + "/image_0/" + base_name;
            right_img_file_name = dir + "/image_1/" + base_name;
            time_frame=tg.readTimestamp(numImages);
            break;
        case Tsukuba:
            sprintf(base_name,"%05d.png",numImages);
            left_img_file_name  = dir + "/tsukuba_daylight_L_" + base_name;
            right_img_file_name = dir + "/tsukuba_daylight_R_" + base_name;
            time_frame=(numImages-1)/30.0;
            break;
        case MalagaUrbanExtract6:
            time_frame=tg.extractTimestamp(numImages);
            left_img_file_name=tg.last_left_image_name;
            right_img_file_name=left_img_file_name.substr(0, 30)+ "right"+left_img_file_name.substr(left_img_file_name.length()-4, 4);
            left_img_file_name=dir+ "/"+ left_img_file_name;
            right_img_file_name=dir+ "/"+ right_img_file_name;
        default:
            cerr<<"Please implement interface fot this dataset!"<<endl;
            break;
        }
        time_pair[0]=time_pair[1];
        time_pair[1]=time_frame;
        cv::Mat left_img=cv::imread(left_img_file_name, 0);
        cv::Mat right_img=cv::imread(right_img_file_name, 0);
        cout<<"processing frame "<< numImages-mnStartId<<endl;
        SLAM_START_TIMER("tot_time");
#ifdef MONO
        if(mbUseIMUData){
            if(!imu_proc->bStatesInitialized){
                imu_proc->initStates(T_s1_to_w, speed_bias_1, time_frame);//ASSUME the IMU measurements are continuous and covers longer than camera data
                ProcessFrameMono(left_img, time_frame, std::vector<Eigen::Matrix<double, 7,1 > >(),
                             NULL, NULL, &speed_bias_1);
            }
            else{
                predTcp=imu_proc->propagate(time_frame);
                Sophus::SE3d tempTw2c;
                ProcessFrameMono(left_img, time_frame,
                                  imu_proc->getMeasurements(), &predTcp, &tempTw2c, &speed_bias_1);
                if(mState == WORKING){
                    imu_proc->resetStates((imu_.T_imu_from_cam*tempTw2c).inverse(), speed_bias_1);
                }
            }
        }
        else
            ProcessFrameMono(left_img, time_frame);
#else
        if(mbUseIMUData){
            Sophus::SE3d tempTw2c;
            if(!imu_proc->bStatesInitialized){
                imu_proc->initStates(T_s1_to_w, speed_bias_1, time_frame);//ASSUME the IMU measurements are continuous and covers longer than camera data
                ProcessFrame(left_img, right_img, time_frame, std::vector<Eigen::Matrix<double, 7,1 > >(),
                             NULL, &tempTw2c, &speed_bias_1);
            }
            else{
                predTcp=imu_proc->propagate(time_frame);                
                ProcessFrame(left_img, right_img, time_frame,  
                                imu_proc->getMeasurements(), &predTcp, &tempTw2c, &speed_bias_1);

            }
            if(mState == WORKING){
                imu_proc->resetStates((imu_.T_imu_from_cam*tempTw2c).inverse(), speed_bias_1);
            }
        }
        else{
            ProcessFrame(left_img, right_img, time_frame);
        }
#endif
        SLAM_STOP_TIMER("tot_time");
        //output position for debug
#ifdef SLAM_TRACE
  g_permon->writeToFile();
#endif
        if(mState == WORKING){
            Eigen::Matrix<double,4,1> q = mpLastFrame->GetPose().unit_quaternion().conjugate().coeffs();
            Eigen::Vector3d t = mpLastFrame->GetCameraCenter();
            out_stream << setprecision(6) << mpLastFrame->mTimeStamp << setprecision(7) << " " << t(0) << " " << t(1) << " " << t(2)
                       << " " << q[0] << " " << q[1] << " " << q[2] << " " << q[3] << endl;
            out_stream.flush();
        }
    	if(mpLocalMapper->isStopped())//TODO: disable local optimize when this happens
        {
            while(mpLocalMapper->isStopped())
            {
                boost::this_thread::sleep(boost::posix_time::milliseconds(5));
            }
        }
        ++numImages;
        mpFramePublisher->Refresh();
//        MapPub.Refresh();
        CheckResetByPublishers();
    }

    double calc_time =  timer_.stop();
    double time_per_frame=calc_time/(numImages-mnStartId+1);
    cout<<"Calc_time:"<<calc_time<<";"<<"time per frame:"<<time_per_frame<<endl;
    // Save keyframe poses at the end of the execution
    vector<ORB_SLAM::KeyFrame*> vpKFs = mpMap->GetAllKeyFrames();
    sort(vpKFs.begin(),vpKFs.end(),ORB_SLAM::KeyFrame::lId);

    cout << endl << "Saving Keyframe Trajectory to KeyFrameTrajectory.txt" << endl;

    for(size_t i=0; i<vpKFs.size(); i++)
    {
        ORB_SLAM::KeyFrame* pKF = vpKFs[i];

        if(pKF->isBad())
            continue;
        Eigen::Matrix<double,4,1> q = pKF->GetPose().unit_quaternion().conjugate().coeffs();
        Eigen::Vector3d t = pKF->GetCameraCenter();
        out_stream << setprecision(6) << pKF->mTimeStamp << setprecision(7) << " " << t(0) << " " << t(1) << " " << t(2)
                   << " " << q[0] << " " << q[1] << " " << q[2] << " " << q[3] << endl;

    }
    out_stream.close();
    cv::waitKey();
    exit(0);
}

void Tracking::GrabImage(cv::Mat& im, double timeStampSec)
{
    if(im.channels()==3)
    {
        cv::Mat temp;
        if(mbRGB)
            cvtColor(im, temp, CV_RGB2GRAY);
        else
            cvtColor(im, temp, CV_BGR2GRAY);
        im=temp;
    }    

    if(mState==WORKING || mState==LOST)
        mpCurrentFrame.reset(new Frame(im,timeStampSec,mpORBextractor,mpORBVocabulary, cam_));
    else
        mpCurrentFrame.reset(new Frame(im,timeStampSec,mpIniORBextractor,mpORBVocabulary, cam_));

    // Depending on the state of the Tracker we perform different tasks

    if(mState==NO_IMAGES_YET)
    {
        mState = NOT_INITIALIZED;
    }

    mLastProcessedState=mState;

    if(mState==NOT_INITIALIZED)
    {
        FirstInitialization();
    }
    else if(mState==INITIALIZING)
    {
        Initialize();
    }
    else
    {
        // System is initialized. Track Frame.
        bool bOK;

        // Initial Camera Pose Estimation from Previous Frame (Motion Model or Coarse) or Relocalisation
        if(mState==WORKING && !RelocalisationRequested())
        {
            //huai: TODO: predict poses from an IMU thread
            if(!mbMotionModel || mpMap->KeyFramesInMap()<4 || mpCurrentFrame->mnId<mnLastRelocFrameId+2)
                bOK = TrackPreviousFrame();
            else
            {
                bOK = TrackWithMotionModel();
                if(!bOK)
                    bOK = TrackPreviousFrame();
            }
        }
        else
        {
            bOK = Relocalisation();
        }

        // If we have an initial estimation of the camera pose and matching. Track the local map.
        if(bOK)
            bOK = TrackLocalMap();

        // If tracking were good, check if we insert a keyframe
        if(bOK)
        {
            //mpMapPublisher->SetCurrentCameraPose(mpCurrentFrame->mTcw);

            if(NeedNewKeyFrame())
                CreateNewKeyFrame();

            // We allow points with high innovation (considererd outliers by the Huber Function)
            // pass to the new keyframe, so that bundle adjustment will finally decide
            // if they are outliers or not. We don't want next frame to estimate its position
            // with those points so we discard them in the frame.
            for(size_t i=0; i<mpCurrentFrame->mvbOutlier.size();i++)
            {
                if(mpCurrentFrame->mvpMapPoints[i] && mpCurrentFrame->mvbOutlier[i])
                    mpCurrentFrame->mvpMapPoints[i]=NULL;
            }
        }

        if(bOK)
            mState = WORKING;
        else
            mState=LOST;

        // Reset if the camera get lost soon after initialization
        if(mState==LOST)
        {
            if(mpMap->KeyFramesInMap()<=5)
            {
                Reset();
                return;
            }
        }

        // Update motion model
        if(mbMotionModel)
        {
            if(bOK)
            {
                //huai: TODO: use a Kalman filter to update se3 velocity
                mVelocity = mpCurrentFrame->mTcw*mpLastFrame->mTcw.inverse();
            }
            else
                mVelocity = Sophus::SE3d();
        }

        mpLastFrame = mpCurrentFrame;
     }       

    // Update drawer
    mpFramePublisher->Update(this, im);

//    if(!mpCurrentFrame->mTcw.empty())
//    {
//        Eigen::Matrix3d Rwc = mpCurrentFrame->mTcw.rotationMatrix().transpose();
//        Eigen::Vector3d twc = -Rwc*mpCurrentFrame->mTcw.translation();
//        tf::Matrix3x3 M(Rwc(0,0),Rwc(0,1),Rwc(0,2),
//                        Rwc(1,0),Rwc(1,1),Rwc(1,2),
//                        Rwc(2,0),Rwc(2,1),Rwc(2,2));
//        tf::Vector3 V(twc(0), twc(1), twc(2));

//        tf::Transform tfTcw(M,V);

//        mTfBr.sendTransform(tf::StampedTransform(tfTcw,ros::Time::now(), "ORB_SLAM/World", "ORB_SLAM/Camera"));
//    }

}

// Triangulate new MapPoints given the local double window and the latest keyframe, i.e., the current frame
// TODO: map points can be created between current frame and other keyframes/frames
// for now, map points are created only between current frame and its preceding frame which is back of temporal window
// a grid is used to ensure uniform distribution of map points observed in current frame,
// alternatively, a quad tree as in scavislam by Strasdat or rslam by Mei may be used.
void Tracking::CreateNewMapPoints(const std::vector<p_match>& vQuadMatches)
{
    int numNewPoints=0;
  //mpLastKeyFrame is the new keyframe, mvpTemporalFrames.back() is the previous frame(can be a keyframe)
    const int nGridCols= (mpLastKeyFrame->mnMaxX - mpLastKeyFrame->mnMinX)/ 30 +1,
            nGridRows= (mpLastKeyFrame->mnMaxY - mpLastKeyFrame->mnMinY)/ 30 +1;
    float fGridElementWidthInv=static_cast<float>(nGridCols)/static_cast<float>(mpLastKeyFrame->mnMaxX-mpLastKeyFrame->mnMinX);
    float fGridElementHeightInv=static_cast<float>(nGridRows)/static_cast<float>(mpLastKeyFrame->mnMaxY-mpLastKeyFrame->mnMinY);
    size_t nPointPerCell=mnFeatures/(nGridCols*nGridRows) +1;

    std::vector< std::vector< std::vector<size_t> > > vMPGrid;// record observations of mappoints in the new keyframe
    vMPGrid.resize(nGridCols);
    for(int jack=0; jack< nGridCols;++jack)
    {
        vMPGrid[jack].resize(nGridRows);
        vMPGrid.reserve(nPointPerCell);
    }
    // put map points in grid
    int jim=0;
    for(vector<MapPoint*>::iterator vit=mpLastKeyFrame->mvpMapPoints.begin(), vend=mpLastKeyFrame->mvpMapPoints.end();
        vit!=vend; ++vit, ++jim)
    {
        MapPoint* pMP = *vit;
        if(pMP)
        {
            if(pMP->isBad())
            {
                *vit = NULL;
            }
            else
            {
               cv::KeyPoint kpUn=mpLastKeyFrame->mvKeysUn[jim];
               int posX = round((kpUn.pt.x-mpLastKeyFrame->mnMinX)*fGridElementWidthInv);
               int posY = round((kpUn.pt.y-mpLastKeyFrame->mnMinY)*fGridElementHeightInv);
               assert(!(posX<0 || posX>=nGridCols || posY<0 || posY>=nGridRows));
               vMPGrid[posX][posY].push_back(jim);
            }
        }
    }
    Frame* pPrevFrame = mvpTemporalFrames.back();
	bool isPrevFrameKeyFrame= pPrevFrame->isKeyFrame();    
	Sophus::SE3d proxy= mpLastKeyFrame->GetPose();
    Eigen::Matrix<double,3,4> Tw2c1= proxy.matrix3x4();
    Eigen::Matrix3d Rw2c1= Tw2c1.topLeftCorner<3,3>();
    Eigen::Vector3d twinc1=Tw2c1.col(3);

    proxy = pPrevFrame->GetPose();
    Eigen::Matrix<double,3,4> Tw2c2= proxy.matrix3x4();
    Eigen::Matrix3d Rw2c2= Tw2c2.topLeftCorner<3,3>();
    Eigen::Vector3d twinc2=Tw2c2.col(3);

    Eigen::Matrix<double,3,4> Tw2c1r= mpLastKeyFrame->GetPose(false).matrix3x4();
    Eigen::Matrix<double,3,4> Tw2c2r= pPrevFrame->GetPose(false).matrix3x4();

    const float ratioFactor = 1.5f*mpLastKeyFrame->GetScaleFactor();
    Eigen::Vector3d Ow1 = mpLastKeyFrame->GetCameraCenter();
    Eigen::Vector3d Ow2 = pPrevFrame->GetCameraCenter();
    for(size_t indigo=0; indigo< vQuadMatches.size();++indigo)
    {
        if(mpLastKeyFrame->mvpMapPoints[vQuadMatches[indigo].i1c]!=NULL ||
               mpLastKeyFrame->mvbOutlier[vQuadMatches[indigo].i1c] ||
                pPrevFrame->GetMapPoint(vQuadMatches[indigo].i1p))
            continue;
        cv::KeyPoint kpUn=mpLastKeyFrame->mvKeysUn[vQuadMatches[indigo].i1c];        
        int posX = round((kpUn.pt.x-mpLastKeyFrame->mnMinX)*fGridElementWidthInv);
        int posY = round((kpUn.pt.y-mpLastKeyFrame->mnMinY)*fGridElementHeightInv);
        if(!(posX<0 || posX>=nGridCols || posY<0 || posY>=nGridRows) && vMPGrid[posX][posY].size()< nPointPerCell)
        {
            // Triangulate each match
#if 0
            const cv::KeyPoint &kp1 = mpLastKeyFrame->mvKeysUn[vQuadMatches[indigo].i1c];
            const cv::KeyPoint &kp2 = mpLastKeyFrame->mvRightKeysUn[vQuadMatches[indigo].i1c];

            // Check parallax between left and right rays
            Vector3d xn1((kp1.pt.x-mpLastKeyFrame->cam_->cx())/mpLastKeyFrame->cam_->fx(),
                         (kp1.pt.y-mpLastKeyFrame->cam_->cy())/mpLastKeyFrame->cam_->fy(), 1.0 ),
                    ray1(xn1);

            Vector3d xn2((kp2.pt.x-mpLastKeyFrame->right_cam_->cx())/mpLastKeyFrame->right_cam_->fx(),
                         (kp2.pt.y-mpLastKeyFrame->right_cam_->cy())/mpLastKeyFrame->right_cam_->fy(), 1.0 );
            Vector3d ray2 = mpLastKeyFrame->mTl2r.rotationMatrix().transpose()*xn2;
            const float cosParallaxRays = ray1.dot(ray2)/(ray1.norm()*ray2.norm());

            if(cosParallaxRays<0 || cosParallaxRays>0.99995)
                continue;

            // Linear Triangulation Method
            const cv::KeyPoint &kp3 = pPrevFrame->mvKeysUn[vQuadMatches[indigo].i1p];
            const cv::KeyPoint &kp4 = pPrevFrame->mvRightKeysUn[vQuadMatches[indigo].i1p];
            Vector3d xn3((kp3.pt.x-pPrevFrame->cam_->cx())/pPrevFrame->cam_->fx(),
                         (kp3.pt.y-pPrevFrame->cam_->cy())/pPrevFrame->cam_->fy(), 1.0 );

            Vector3d xn4((kp4.pt.x-pPrevFrame->right_cam_->cx())/pPrevFrame->right_cam_->fx(),
                         (kp4.pt.y-pPrevFrame->right_cam_->cy())/pPrevFrame->right_cam_->fy(), 1.0 );

            Eigen::Matrix<double,8,4> A;
            A.row(0) = xn1(0)*Tw2c1.row(2)-Tw2c1.row(0);
            A.row(1) = xn1(1)*Tw2c1.row(2)-Tw2c1.row(1);
            A.row(2) = xn2(0)*Tw2c1r.row(2)-Tw2c1r.row(0);
            A.row(3) = xn2(1)*Tw2c1r.row(2)-Tw2c1r.row(1);
            A.row(4) = xn3(0)*Tw2c2.row(2)-Tw2c2.row(0);
            A.row(5) = xn3(1)*Tw2c2.row(2)-Tw2c2.row(1);
            A.row(6) = xn4(0)*Tw2c2r.row(2)-Tw2c2r.row(0);
            A.row(7) = xn4(1)*Tw2c2r.row(2)-Tw2c2r.row(1);
            cv::Mat Aprime, w,u,vt;
            cv::eigen2cv(A, Aprime);
            cv::SVD::compute(Aprime,w,u,vt,cv::SVD::MODIFY_A| cv::SVD::FULL_UV);

            cv::Mat x3D = vt.row(3).t();

            if(x3D.at<double>(3)==0)
                continue;

            // Euclidean coordinates
            x3D = x3D.rowRange(0,3)/x3D.at<double>(3);
            Eigen::Vector3d x3Dt;
            cv::cv2eigen (x3D, x3Dt);

            //Check triangulation in front of cameras
            float z1 = Rw2c1.row(2)*x3Dt+ twinc1(2);
            if(z1<=0)
                continue;
            float z2 = Rw2c2.row(2)*x3Dt+twinc2(2);
            if(z2<=0)
                continue;

            //Check reprojection error in first keyframe
            float sigmaSquare1 = mpLastKeyFrame->GetSigma2(kp1.octave);
            float x1 = Rw2c1.row(0)*x3Dt+twinc1(0);
            float y1 = Rw2c1.row(1)*x3Dt+twinc1(1);
            float invz1 = 1.0/z1;
            float u1 = mpLastKeyFrame->cam_->fx()*x1*invz1+mpLastKeyFrame->cam_->cx();
            float v1 = mpLastKeyFrame->cam_->fy()*y1*invz1+mpLastKeyFrame->cam_->cy();
            float errX1 = u1 - kp1.pt.x;
            float errY1 = v1 - kp1.pt.y;
            if((errX1*errX1+errY1*errY1)>5.991*sigmaSquare1)
                continue;

            //Check reprojection error in second frame
            float sigmaSquare2 = pPrevFrame->GetSigma2(kp3.octave);
            float x2 = Rw2c2.row(0)*x3Dt+twinc2(0);
            float y2 = Rw2c2.row(1)*x3Dt+twinc2(1);
            float invz2 = 1.0/z2;
            float u2 = pPrevFrame->cam_->fx()*x2*invz2+pPrevFrame->cam_->cx();
            float v2 = pPrevFrame->cam_->fy()*y2*invz2+pPrevFrame->cam_->cy();
            float errX2 = u2 - kp3.pt.x;
            float errY2 = v2 - kp3.pt.y;
            if((errX2*errX2+errY2*errY2)>5.991*sigmaSquare2)
                continue;

            //Check scale consistency
            Eigen::Vector3d normal1 = x3Dt-Ow1;
            float dist1 = normal1.norm();

            Eigen::Vector3d normal2 = x3Dt-Ow2;
            float dist2 = normal2.norm();

            if(dist1==0 || dist2==0)
                continue;

            float ratioDist = dist1/dist2;
            if(ratioDist*ratioFactor<1.f || ratioDist>ratioFactor)
                continue;
#else
            //Assume left right image rectified and no distortion
            const cv::KeyPoint &kp1 = mpLastKeyFrame->mvKeysUn[vQuadMatches[indigo].i1c];
            const cv::KeyPoint &kp2 = mpLastKeyFrame->mvRightKeysUn[vQuadMatches[indigo].i1c];
            const cv::KeyPoint &kp3 = pPrevFrame->mvKeysUn[vQuadMatches[indigo].i1p];
            const cv::KeyPoint &kp4 = pPrevFrame->mvRightKeysUn[vQuadMatches[indigo].i1p];
            if(kp1.pt.x -kp2.pt.x<2 || kp3.pt.x- kp4.pt.x<2)//parallax
                continue;

            float base= -mTl2r.translation()[0];
            float base_disp = base/(kp1.pt.x -kp2.pt.x);
            Eigen::Vector3d x3D1(3,1,CV_32F);
            x3D1(0) = (kp1.pt.x- mpLastKeyFrame->cam_->cx())*base_disp;
            x3D1(1) = ((kp1.pt.y+ kp2.pt.y)/2 - mpLastKeyFrame->cam_->cy())*base_disp;
            x3D1(2) = mpLastKeyFrame->cam_->fx()*base_disp;
            x3D1= Rw2c1.transpose()*(x3D1- twinc1);
            base_disp = base/(kp3.pt.x -kp4.pt.x);
            Eigen::Vector3d x3D2(3,1,CV_32F);
            x3D2(0) = (kp3.pt.x- pPrevFrame->cam_->cx())*base_disp;
            x3D2(1) = ((kp3.pt.y+ kp4.pt.y)/2 - pPrevFrame->cam_->cy())*base_disp;
            x3D2(2) = pPrevFrame->cam_->fx()*base_disp;
            x3D2= Rw2c2.transpose()*(x3D2 - twinc2);
            if(abs(x3D1(2)- x3D2(2))>0.2)
                continue;
            Eigen::Vector3d x3Dt= (x3D1+ x3D2)/2;
#endif
            // Triangulation is succesful
            MapPoint* pMP = new MapPoint(x3Dt,mpLastKeyFrame, mpMap);
   			if(isPrevFrameKeyFrame){
   			pMP->AddObservation(pPrevFrame,vQuadMatches[indigo].i1p);
            pMP->AddObservation(pPrevFrame,vQuadMatches[indigo].i2p, false);
            }
            pMP->AddObservation(mpLastKeyFrame,vQuadMatches[indigo].i1c);
            pMP->AddObservation(mpLastKeyFrame,vQuadMatches[indigo].i2c, false);

            mpLastKeyFrame->AddMapPoint(pMP,vQuadMatches[indigo].i1c);
            pPrevFrame->AddMapPoint(pMP,vQuadMatches[indigo].i1p);
            //Fill Current Frame structure
            mpCurrentFrame->mvpMapPoints[vQuadMatches[indigo].i1c] = pMP;

            pMP->ComputeDistinctiveDescriptors();
            pMP->UpdateNormalAndDepth();
            mpMap->AddMapPoint(pMP);
            vMPGrid[posX][posY].push_back(vQuadMatches[indigo].i1c);
            ++numNewPoints;

        }
    }
#endif
    cout<<"created new points from quad matches:"<<numNewPoints<<" "<<vQuadMatches.size()<<endl;

}
// for now, we only check left image to get matches
// Triangulate new MapPoints between the penultimate keyframe, pKF2, and the current frame which is the last keyframe
// TODO: map points can be created between current frame and other keyframes/frames
// for now, map points are created only between current frame and its preceding keyframe
// a grid is used to ensure uniform distribution of map points observed in current frame,
// alternatively, a quad tree as in scavislam by Strasdat or rslam by Mei may be used.
void Tracking::CreateNewMapPoints(KeyFrame* pKF2, KeyFrame* pCurrentKeyFrame)
{
    int counter=0;
    ORBmatcher matcher(0.6,false);


    Sophus::SE3d proxy= pCurrentKeyFrame->GetPose();
    Eigen::Matrix<double,3,4> Tw2c1=proxy.matrix3x4();
    Eigen::Matrix3d Rw2c1= Tw2c1.topLeftCorner<3,3>();
    Eigen::Vector3d twinc1= Tw2c1.col(3);

    Eigen::Matrix<double,3,4> Tw2c1r= pCurrentKeyFrame->GetPose(false).matrix3x4();

    Eigen::Vector3d Ow1 = pCurrentKeyFrame->GetCameraCenter();
    const float fx1 = pCurrentKeyFrame->cam_->fx();
    const float fy1 = pCurrentKeyFrame->cam_->fy();
    const float cx1 = pCurrentKeyFrame->cam_->cx();
    const float cy1 = pCurrentKeyFrame->cam_->cy();
    const float invfx1 = 1.0f/fx1;
    const float invfy1 = 1.0f/fy1;
    const float ratioFactor = 1.5f*pCurrentKeyFrame->GetScaleFactor();

    // Search matches with epipolar restriction and triangulate
    // Check first that baseline is not too short
    // Small translation errors for short baseline keyframes make scale to diverge
    Eigen::Vector3d Ow2 = pKF2->GetCameraCenter();
    Eigen::Vector3d vBaseline = Ow2-Ow1;
    const float baseline = vBaseline.norm();
    const float medianDepthKF2 = pKF2->ComputeSceneMedianDepth(2);
    const float ratioBaselineDepth = baseline/medianDepthKF2;

//    assert(ratioBaselineDepth>=0.01);

    // Compute Fundamental Matrix
    Eigen::Matrix3d F12 = ComputeF12(pCurrentKeyFrame,pKF2);

    // Search matches that fulfil epipolar constraint
    vector<cv::KeyPoint> vMatchedKeysUn1;
    vector<cv::KeyPoint> vMatchedKeysUn2;
    vector<pair<size_t,size_t> > vMatchedIndices;
    matcher.SearchForTriangulation(pCurrentKeyFrame,pKF2,F12,vMatchedKeysUn1,vMatchedKeysUn2,vMatchedIndices);

    proxy= pKF2->GetPose();
    Eigen::Matrix<double,3,4> Tw2c2=proxy.matrix3x4();
    Eigen::Matrix3d Rw2c2= Tw2c2.topLeftCorner<3,3>();
    Eigen::Vector3d twinc2=Tw2c2.col(3);

    Eigen::Matrix<double,3,4> Tw2c2r= pKF2->GetPose(false).matrix3x4();

    const float fx2 = pKF2->cam_->fx();
    const float fy2 = pKF2->cam_->fy();
    const float cx2 = pKF2->cam_->cx();
    const float cy2 = pKF2->cam_->cy();
    const float invfx2 = 1.0f/fx2;
    const float invfy2 = 1.0f/fy2;
//use a grid to control point initialization
    const int nGridCols= (pCurrentKeyFrame->mnMaxX - pCurrentKeyFrame->mnMinX)/ 30 +1,
            nGridRows= (pCurrentKeyFrame->mnMaxY - pCurrentKeyFrame->mnMinY)/ 30 +1;
    float fGridElementWidthInv=static_cast<float>(nGridCols)/static_cast<float>(pCurrentKeyFrame->mnMaxX-pCurrentKeyFrame->mnMinX);
    float fGridElementHeightInv=static_cast<float>(nGridRows)/static_cast<float>(pCurrentKeyFrame->mnMaxY-pCurrentKeyFrame->mnMinY);
    size_t nPointPerCell=mnFeatures/(nGridCols*nGridRows) +1;

    std::vector< std::vector< std::vector<size_t> > > vMPGrid;// record observations of mappoints in the new keyframe
    vMPGrid.resize(nGridCols);
    for(int jack=0; jack< nGridCols;++jack)
    {
        vMPGrid[jack].resize(nGridRows);
        vMPGrid.reserve(nPointPerCell);
    }
    // put map points in grid
    int jim=0;
    for(vector<MapPoint*>::iterator vit=pCurrentKeyFrame->mvpMapPoints.begin(), vend=pCurrentKeyFrame->mvpMapPoints.end();
        vit!=vend; ++vit, ++jim)
    {
        MapPoint* pMP = *vit;
        if(pMP)
        {
            if(pMP->isBad())
            {
                *vit = NULL;
            }
            else
            {
               cv::KeyPoint kpUn=pCurrentKeyFrame->mvKeysUn[jim];
               int posX = round((kpUn.pt.x-pCurrentKeyFrame->mnMinX)*fGridElementWidthInv);
               int posY = round((kpUn.pt.y-pCurrentKeyFrame->mnMinY)*fGridElementHeightInv);
               assert(!(posX<0 || posX>=nGridCols || posY<0 || posY>=nGridRows));
               vMPGrid[posX][posY].push_back(jim);
            }
        }
    }

    // Triangulate each match based on stereo observations
    for(size_t ikp=0, iendkp=vMatchedKeysUn1.size(); ikp<iendkp; ikp++)
    {
        const int idx1 = vMatchedIndices[ikp].first; // indices of features in current keyframe
        const int idx2 = vMatchedIndices[ikp].second;// indices of features in the other keyframe
        const cv::KeyPoint &kp1 = vMatchedKeysUn1[ikp];
        int posX = round((kp1.pt.x-pCurrentKeyFrame->mnMinX)*fGridElementWidthInv);
        int posY = round((kp1.pt.y-pCurrentKeyFrame->mnMinY)*fGridElementHeightInv);
        if(!(posX<0 || posX>=nGridCols || posY<0 || posY>=nGridRows) && vMPGrid[posX][posY].size()< nPointPerCell)
        {
#ifdef MONO
        const cv::KeyPoint &kp2 = vMatchedKeysUn2[ikp];

        // Check parallax between rays
        Eigen::Vector3d xn1((kp1.pt.x-cx1)*invfx1, (kp1.pt.y-cy1)*invfy1, 1.0 );
        Eigen::Vector3d ray1 = Rw2c1.transpose()*xn1;
        Eigen::Vector3d xn2((kp2.pt.x-cx2)*invfx2, (kp2.pt.y-cy2)*invfy2, 1.0 );
        Eigen::Vector3d ray2 = Rw2c2.transpose()*xn2;
        const float cosParallaxRays = ray1.dot(ray2)/(ray1.norm()*ray2.norm());

        if(cosParallaxRays<0 || cosParallaxRays>0.99995)
            continue;

        // Linear Triangulation Method
        Eigen::Matrix<double,4,4> A;
        A.row(0) = xn1(0)*Tw2c1.row(2)-Tw2c1.row(0);
        A.row(1) = xn1(1)*Tw2c1.row(2)-Tw2c1.row(1);
        A.row(2) = xn2(0)*Tw2c2.row(2)-Tw2c2.row(0);
        A.row(3) = xn2(1)*Tw2c2.row(2)-Tw2c2.row(1);

        cv::Mat Aprime, w,u,vt;
        cv::eigen2cv(A, Aprime);
        cv::SVD::compute(Aprime,w,u,vt,cv::SVD::MODIFY_A| cv::SVD::FULL_UV);

        cv::Mat x3D = vt.row(3).t();

        if(x3D.at<double>(3)==0)
            continue;

        // Euclidean coordinates
        x3D = x3D.rowRange(0,3)/x3D.at<double>(3);
        Eigen::Vector3d x3Dt;
        cv::cv2eigen( x3D, x3Dt);

        //Check triangulation in front of cameras
        float z1 = Rw2c1.row(2)*x3Dt+twinc1(2);
        if(z1<=0)
            continue;

        float z2 = Rw2c2.row(2)*x3Dt+twinc2(2);
        if(z2<=0)
            continue;

        //Check reprojection error in first keyframe
        float sigmaSquare1 = pCurrentKeyFrame->GetSigma2(kp1.octave);
        float x1 = Rw2c1.row(0)*x3Dt+twinc1(0);
        float y1 = Rw2c1.row(1)*x3Dt+twinc1(1);
        float invz1 = 1.0/z1;
        float u1 = fx1*x1*invz1+cx1;
        float v1 = fy1*y1*invz1+cy1;
        float errX1 = u1 - kp1.pt.x;
        float errY1 = v1 - kp1.pt.y;
        if((errX1*errX1+errY1*errY1)>5.991*sigmaSquare1)
            continue;

        //Check reprojection error in second keyframe
        float sigmaSquare2 = pKF2->GetSigma2(kp2.octave);
        float x2 = Rw2c2.row(0)*x3Dt+twinc2(0);
        float y2 = Rw2c2.row(1)*x3Dt+twinc2(1);
        float invz2 = 1.0/z2;
        float u2 = fx2*x2*invz2+cx2;
        float v2 = fy2*y2*invz2+cy2;
        float errX2 = u2 - kp2.pt.x;
        float errY2 = v2 - kp2.pt.y;
        if((errX2*errX2+errY2*errY2)>5.991*sigmaSquare2)
            continue;

        //Check scale consistency
        Eigen::Vector3d normal1 = x3Dt-Ow1;
        float dist1 = normal1.norm();

        Eigen::Vector3d normal2 = x3Dt-Ow2;
        float dist2 = normal2.norm();

        if(dist1==0 || dist2==0)
            continue;

        float ratioDist = dist1/dist2;
        float ratioOctave = pCurrentKeyFrame->GetScaleFactor(kp1.octave)/pKF2->GetScaleFactor(kp2.octave);
        if(ratioDist*2.f<ratioOctave || ratioDist>ratioOctave*2.f)
            continue;
#else
#if 0

        const cv::KeyPoint &kp2 = pCurrentKeyFrame->mvRightKeysUn[idx1];
        // Check parallax between left and right rays
        Eigen::Vector3d xn1((kp1.pt.x-cx1)*invfx1,
                            (kp1.pt.y-cy1)*invfy1, 1.0 ),
                ray1(xn1);

        Eigen::Vector3d xn2((kp2.pt.x-pCurrentKeyFrame->mRightK(0,2))/pCurrentKeyFrame->mRightK(0,0),
                            (kp2.pt.y-pCurrentKeyFrame->mRightK(1,2))/pCurrentKeyFrame->mRightK(1,1), 1.0 );
        Eigen::Vector3d ray2 = pCurrentKeyFrame->mTl2r.rotationMatrix().transpose()*xn2;
        const float cosParallaxRays = ray1.dot(ray2)/(ray1.norm()*ray2.norm());

        if(cosParallaxRays<0 || cosParallaxRays>0.99995)
            continue;
        // Linear Triangulation Method
        const cv::KeyPoint &kp3 = vMatchedKeysUn2[ikp];
        const cv::KeyPoint &kp4 = pKF2->mvRightKeysUn[idx2];
        Eigen::Vector3d xn3((kp3.pt.x-cx2)*invfx2,
                            (kp3.pt.y-cy2)*invfy2, 1.0 );

        Eigen::Vector3d xn4((kp4.pt.x-pKF2->mRightK(0,2))/pKF2->mRightK(0,0),
                            (kp4.pt.y-pKF2->mRightK(1,2))/pKF2->mRightK(1,1), 1.0 );

        Eigen::Matrix<double, 8,4> A;
        A.row(0) = xn1(0)*Tw2c1.row(2)-Tw2c1.row(0);
        A.row(1) = xn1(1)*Tw2c1.row(2)-Tw2c1.row(1);
        A.row(2) = xn2(0)*Tw2c1r.row(2)-Tw2c1r.row(0);
        A.row(3) = xn2(1)*Tw2c1r.row(2)-Tw2c1r.row(1);
        A.row(4) = xn3(0)*Tw2c2.row(2)-Tw2c2.row(0);
        A.row(5) = xn3(1)*Tw2c2.row(2)-Tw2c2.row(1);
        A.row(6) = xn4(0)*Tw2c2r.row(2)-Tw2c2r.row(0);
        A.row(7) = xn4(1)*Tw2c2r.row(2)-Tw2c2r.row(1);
        cv::Mat Aprime, w,u,vt;
        cv::eigen2cv(A,Aprime);
        cv::SVD::compute(Aprime,w,u,vt,cv::SVD::MODIFY_A| cv::SVD::FULL_UV);

        cv::Mat x3D = vt.row(3).t();
        if(x3D.at<double>(3)==0)
            continue;

        // Euclidean coordinates
        x3D = x3D.rowRange(0,3)/x3D.at<double>(3);
        Eigen::Vector3d x3Dt;
        cv::cv2eigen(x3D, x3Dt);

        //Check triangulation in front of cameras
        float z1 = Rw2c1.row(2)*x3Dt+ twinc1(2);
        if(z1<=0)
            continue;
        float z2 = Rw2c2.row(2)*x3Dt+twinc2(2);
        if(z2<=0)
            continue;

        //Check reprojection error in first keyframe
        float sigmaSquare1 = pCurrentKeyFrame->GetSigma2(kp1.octave);
        float x1 = Rw2c1.row(0)*x3Dt+twinc1(0);
        float y1 = Rw2c1.row(1)*x3Dt+twinc1(1);
        float invz1 = 1.0/z1;
        float u1 = fx1*x1*invz1 + cx1;
        float v1 = fy1*y1*invz1 + cy1;
        float errX1 = u1 - kp1.pt.x;
        float errY1 = v1 - kp1.pt.y;
        if((errX1*errX1+errY1*errY1)>5.991*sigmaSquare1)
            continue;

        //Check reprojection error in second frame
        float sigmaSquare2 = pKF2->GetSigma2(kp3.octave);
        float x2 = Rw2c2.row(0)*x3Dt+twinc2(0);
        float y2 = Rw2c2.row(1)*x3Dt+twinc2(1);
        float invz2 = 1.0/z2;
        float u2 = fx2*x2*invz2 + cx2;
        float v2 = fy2*y2*invz2 + cy2;
        float errX2 = u2 - kp3.pt.x;
        float errY2 = v2 - kp3.pt.y;
        if((errX2*errX2+errY2*errY2)>5.991*sigmaSquare2)
            continue;

        //Check scale consistency
        Eigen::Vector3d normal1 = x3Dt-Ow1;
        float dist1 = normal1.norm();

        Eigen::Vector3d normal2 = x3Dt-Ow2;
        float dist2 = normal2.norm();

        if(dist1==0 || dist2==0)
            continue;

        float ratioDist = dist1/dist2;
        if(ratioDist*ratioFactor<1.f || ratioDist>ratioFactor)
            continue;
#else
        //Assume left right image rectified and no distortion

        const cv::KeyPoint &kp2 = pCurrentKeyFrame->mvRightKeysUn[idx1];
        const cv::KeyPoint &kp3 = vMatchedKeysUn2[ikp];
        const cv::KeyPoint &kp4 = pKF2->mvRightKeysUn[idx2];
        if(kp1.pt.x -kp2.pt.x<3 || kp3.pt.x- kp4.pt.x<3)//parallax
            continue;

        float base= -mTl2r.translation()[0];
        float base_disp = base/(kp1.pt.x -kp2.pt.x);
        Eigen::Vector3d x3D1(3,1,CV_32F);
        x3D1(0) = (kp1.pt.x- pCurrentKeyFrame->cam_->cx())*base_disp;
        x3D1(1) = ((kp1.pt.y+ kp2.pt.y)/2 - pCurrentKeyFrame->cam_->cy())*base_disp;
        x3D1(2) = pCurrentKeyFrame->cam_->fx()*base_disp;
        x3D1= Rw2c1.transpose()*(x3D1- twinc1);
        base_disp = base/(kp3.pt.x -kp4.pt.x);
        Eigen::Vector3d x3D2(3,1,CV_32F);
        x3D2(0) = (kp3.pt.x- pKF2->cam_->cx())*base_disp;
        x3D2(1) = ((kp3.pt.y+ kp4.pt.y)/2 - pKF2->cam_->cy())*base_disp;
        x3D2(2) = pKF2->cam_->fx()*base_disp;
        x3D2= Rw2c2.transpose()*(x3D2 - twinc2);
        if(abs(x3D1(2)- x3D2(2))>0.2)
            continue;
        Eigen::Vector3d x3Dt= (x3D1+ x3D2)/2;
#endif
#endif
        // Triangulation is succesful
        MapPoint* pMP = new MapPoint(x3Dt,pCurrentKeyFrame, idx1, mpMap);

        pMP->AddObservation(pKF2, idx2);
#ifndef MONO
        pMP->AddObservation(pKF2,idx2, false);
        pMP->AddObservation(pCurrentKeyFrame,idx1, false);
#endif
        pCurrentKeyFrame->AddMapPoint(pMP,idx1);
        pKF2->AddMapPoint(pMP,idx2);
        //Fill Current Frame structure
        mpCurrentFrame->mvpMapPoints[idx1] = pMP;

        pMP->ComputeDistinctiveDescriptors();
        pMP->UpdateNormalAndDepth();
        mpMap->AddMapPoint(pMP);
        vMPGrid[posX][posY].push_back(idx1);
        ++counter;
        }
    }
    cout<< "created new points in cur KF "<<counter<<" out of matches:"<<vMatchedKeysUn1.size()<<endl;
}

//using semi direct visual odometry for stereo cameras
// for right image, we only do map point reprojection
// assume exactly two stereo cameras, scale factor is 2 in building pyramid
void  Tracking::ProcessFrameSVO(cv::Mat &im, cv::Mat& right_im, double timeStampSec,
                                const Sophus::SE3d *pred_Tr_delta,
                             const std::vector<Eigen::Matrix<double, 7,1> >& imu_measurements,
                             Sophus::SE3d* Tw2c, Eigen::Matrix<double, 9,1>* sb)
{
    Sophus::SE3d Tcp =(pred_Tr_delta==NULL? Sophus::SE3d(): (*pred_Tr_delta)); // current left frame from previous frame

    // compute gravity direction in current camera frame
    Eigen::Vector3d ginc= ginw;
    if(mpLastFrame!=NULL && ginw.norm()>1e-6){
        Eigen::Matrix3d Rw2p= mpLastFrame->GetRotation();
        ginc= Tcp.rotationMatrix()*Rw2p*ginw;
    }
    // Depending on the state of the Tracker we perform different tasks
    if(mState==NO_IMAGES_YET)
    {
        mState = NOT_INITIALIZED;
    }
    mpCurrentFrame.reset(new Frame(im, timeStampSec, 0, mpORBextractor, mpORBVocabulary, cam_, ginc, sb));
    mpCurrentFrame->SetIMUObservations(imu_measurements);

    mLastProcessedState=mState;
    if(mState==NOT_INITIALIZED)
    {
        //extract uniform keypoints
        mpCurrentFrame->mpORBextractor->ComputeBlurredPyramid(mpCurrentFrame->img_pyr_, mpCurrentFrame->blurred_img_pyr_);
        // create keyframe and initializeSeeds
        mpCurrentFrame->SetPose( Sophus::SE3d());
        mState = INITIALIZING;
        mpLastKeyFrame=new KeyFrame(*mpCurrentFrame,mpMap,mpKeyFrameDB);
        mnLastKeyFrameId= mpLastKeyFrame->mnId;

        mpCurrentRightFrame.reset(new Frame(right_im, timeStampSec, 1, mpORBextractor, mpORBVocabulary, right_cam_, mTl2r.rotationMatrix()*ginc, NULL));
        mpCurrentRightFrame->SetPose(mTl2r);

        std::vector<cv::KeyPoint> vNewRightKeys=
                mpLocalMapper->initializeSeeds(mpLastKeyFrame, mpCurrentRightFrame.get(), mvIniMatches);

#if 0
        cv::Mat outimg;
        cv::drawKeypoints(im, mpLastKeyFrame->mvKeys,outimg);
        cv::imshow("candidate points of first frame", outimg);
        cv::waitKey();
#endif
        // Check if current frame has enough keypoints, otherwise reset initialization process
        if(mpLastKeyFrame->mvKeysUn.size()<=100)
        {
            fill(mvIniMatches.begin(),mvIniMatches.end(),-1);
            mState = NOT_INITIALIZED;
            return;
        }
        assert(mpCurrentRightFrame->mvKeys.size()==0 && mpLastKeyFrame->mvKeys.size()==mvIniMatches.size());
        mpCurrentRightFrame->mvKeys= vNewRightKeys;
        // keys(Un) without angle are computed, we need to compute angle, descriptors, and fts_
        mpCurrentRightFrame->CreateFtsFromKPs(true);
#if 0
        int nmatches= mvIniMatches.size();
        cv::Mat drawImg3;
        vector<cv::DMatch> matches122_cv; matches122_cv.reserve(nmatches);
        for(size_t i=0; i<mvIniMatches.size(); ++i)
        {
            if(mvIniMatches[i]==-1) continue;
            cv::DMatch temp;
            temp.queryIdx= i;
            temp.trainIdx= mvIniMatches[i];
            temp.distance =1;
            matches122_cv.push_back(temp);
        }
        drawMatches(im, mpLastKeyFrame->mvKeys,
                    right_im, mpCurrentRightFrame->mvKeys, matches122_cv,
                    drawImg3, cv::Scalar(255, 0, 0), cv::Scalar(0, 0, 255));
        cv::imshow("matched", drawImg3);
        cv::waitKey();
#endif
        // Check if there are enough correspondences
        if(vNewRightKeys.size()<100)
        {
            mState = NOT_INITIALIZED;
            return;
        }

        mpReferenceKF= mpLastKeyFrame;
        mpLastFrame.reset(new Frame(*mpLastKeyFrame));
        mpLastKeyFrame->ComputeBoW();
        mpMap->AddKeyFrame(mpLastKeyFrame);

        mvpTemporalFrames.push_back(mpLastKeyFrame);
        mvpTemporalFrames.push_back(new Frame(*mpCurrentRightFrame));
        mState = WORKING;
    }
    else
    {

    //    mpMap->point_candidates_.emptyTrash();
        // System is initialized. Track Frame.
        bool bOK(false);
        // Initial Camera Pose Estimation from Previous Frame (Motion Model or Coarse) or Relocalisation
        // Huai: if loop closure is just finished, relocalize
        if(mState==WORKING && !RelocalisationRequested())
        {
            // Set initial pose use prior
            if(!mbMotionModel || mpMap->KeyFramesInMap()<4 || mpCurrentFrame->mnId<mnLastRelocFrameId+4)
                mpCurrentFrame->SetPose(mpLastFrame->GetPose());
            else
            {
               // mVelocity= Tcp;// use IMU prediction or stereo relative pose
                mpCurrentFrame->SetPose(mVelocity*mpLastFrame->GetPose());
            }
            // sparse image align for left image
            SLAM_START_TIMER("sparse_img_align");
            SparseImgAlign img_align(Config::kltMaxLevel(), Config::kltMinLevel(),
                                     30, SparseImgAlign::GaussNewton, false, false);
            size_t img_align_n_tracked = img_align.run(mpLastFrame, mpCurrentFrame);
            SLAM_STOP_TIMER("sparse_img_align");
            SLAM_LOG(img_align_n_tracked);
            SLAM_DEBUG_STREAM("Img Align:\t Tracked = " << img_align_n_tracked);

            // This is for visualization
            mpMap->SetReferenceMapPoints(mvpLocalMapPoints);
            // Update Local Map only for left image
            UpdateReferenceKeyFramesAndPoints();
            setCoreKfs(core_kfs_);

            // map reprojection & feature alignment for left frame
            SLAM_START_TIMER("reproject");
            reprojector_->reprojectMap(mpCurrentFrame, mvpLocalMapPoints);
            SLAM_STOP_TIMER("reproject");
            size_t repr_n_new_references = reprojector_->n_matches_;
            size_t repr_n_mps = reprojector_->n_trials_;
            SLAM_LOG2(repr_n_mps, repr_n_new_references);
            SLAM_DEBUG_STREAM("Reprojection left image:\t nPoints = "<<repr_n_mps<<"\t \t nMatches = "<<repr_n_new_references);

            if(repr_n_new_references < Config::qualityMinFts())
            {
                SLAM_WARN_STREAM_THROTTLE(1.0, "Not enough matched features.");

                mpCurrentFrame->SetPose( mpLastFrame->GetPose()); // reset to avoid crazy pose jumps
                mState=LOST;
                if(mpMap->KeyFramesInMap()<=5)
                {
                    Reset();
                }
                return;
            }
            bOK=true;
            // pose optimization for left image
            SLAM_START_TIMER("pose_optimizer");
            size_t sfba_n_edges_final;
            double sfba_thresh, sfba_error_init, sfba_error_final;
            pose_optimizer::optimizeGaussNewton(
                        Config::poseOptimThresh(), Config::poseOptimNumIter(), false,
                        mpCurrentFrame, sfba_thresh, sfba_error_init, sfba_error_final, sfba_n_edges_final);
            SLAM_STOP_TIMER("pose_optimizer");
            SLAM_LOG4(sfba_thresh, sfba_error_init, sfba_error_final, sfba_n_edges_final);
            SLAM_DEBUG_STREAM("PoseOptimizer:\t ErrInit = "<<sfba_error_init<<"px\t thresh = "<<sfba_thresh);
            SLAM_DEBUG_STREAM("PoseOptimizer:\t ErrFin. = "<<sfba_error_final<<"px\t nObsFin. = "<<sfba_n_edges_final);
            mnMatchesInliers= sfba_n_edges_final;
            if(sfba_n_edges_final < 20)
            {
                mState=LOST;
                if(mpMap->KeyFramesInMap()<=5)
                {
                    Reset();
                }
                return;
            }
            mpCurrentFrame->CreateKPsFromFts();
            // structure optimization with left image
            if((!mpLocalMapper->isStopped()) && (!mpLocalMapper->stopRequested()))
            {
                SLAM_START_TIMER("point_optimizer");
                optimizeStructure(mpCurrentFrame, Config::structureOptimMaxPts(), Config::structureOptimNumIter());
                SLAM_STOP_TIMER("point_optimizer");
            }
            //process right image
            mpCurrentRightFrame.reset(new Frame(right_im, timeStampSec, 1, mpORBextractor, mpORBVocabulary, right_cam_, mTl2r.rotationMatrix()*ginc, NULL));
            mpCurrentRightFrame->SetPose(mTl2r*mpCurrentFrame->GetPose());
            // map reprojection & feature alignment for right frame
            SLAM_START_TIMER("reproject");
            reprojector_->reprojectMap(mpCurrentRightFrame, mvpLocalMapPoints);
            SLAM_STOP_TIMER("reproject");
            repr_n_new_references = reprojector_->n_matches_;
            repr_n_mps = reprojector_->n_trials_;
            SLAM_LOG2(repr_n_mps, repr_n_new_references);
            SLAM_DEBUG_STREAM("Right reprojection:\t nPoints = "<<repr_n_mps<<"\t \t nMatches = "<<repr_n_new_references);

            mpCurrentRightFrame->CreateKPsFromFts();
            if((!mpLocalMapper->isStopped()) && (!mpLocalMapper->stopRequested()))
            {
                int nBad = LocalOptimizeSVO();
                mnMatchesInliers= repr_n_new_references-nBad;
                cout<<"Inliers after LO:"<< mnMatchesInliers<<endl;

                // Decide if the tracking was successful
                // More restrictive if there was a relocalization recently
                if(mpCurrentFrame->mnId<mnLastRelocFrameId+mMaxFrames && mnMatchesInliers<50)
                    bOK= false;

                if(mnMatchesInliers<12)
                    bOK=false;
            }
            point_stats.updatePointStatistics(mpCurrentFrame.get());
        }
        else
        {
            // only use left image
            //TODO: use semi direct approach for relocalisation
            //extract uniform keypoints
            mpCurrentFrame->mpORBextractor->ComputeBlurredPyramid(mpCurrentFrame->img_pyr_, mpCurrentFrame->blurred_img_pyr_);

            (*(mpCurrentFrame->mpORBextractor))(mpCurrentFrame->mvKeys,mpCurrentFrame->img_pyr_,
                                                mpCurrentFrame->blurred_img_pyr_, mpCurrentFrame->mDescriptors, 20.0);

            // keys with angle and descriptors are computed, we only need to compute fts_
            mpCurrentFrame->CreateFtsFromKPs(false);
            bOK = Relocalisation();
        }

        // If tracking were good, check if we insert a keyframe
        if(bOK)
        {
            //mpMapPublisher->SetCurrentCameraPose(mpCurrentFrame->mTcw);

            Frame * pLastFrame(NULL), *pLastRightFrame(NULL);
            bool bNeedMorePoints= NeedNewKeyFrameStereo();

            if(bNeedMorePoints){
                mpCurrentFrame->MakeKeyFrame_Rest();
                CreateNewKeyFrame();
                pLastFrame=mpLastKeyFrame;

                // add observations for points observed by the new keyframe
                size_t jack=0, zinc=0;
                for(auto it=mpCurrentFrame->mvpMapPoints.begin();
                    it!=mpCurrentFrame->mvpMapPoints.end(); ++it, ++jack){
                    if(*it){
                        (*it)->AddObservation(mpLastKeyFrame, jack);
                        if((*it)->GetType()==MapPoint::TYPE_CANDIDATE)
                            ++zinc;
                    }
                }
                cout<<"add observations for candiate points:"<< zinc<<endl;
                //we also update reference map points which are used in double window optimizer
                mpMap->point_candidates_.addCandidatePointToFrame(mpLastKeyFrame);

                // optional bundle adjustment
                if(Config::lobaNumIter() > 0 && mvpLocalMapPoints.size())
                {
                    SLAM_START_TIMER("local_ba");
                    core_kfs_.push_back(mpLastKeyFrame);
                    size_t loba_n_erredges_init, loba_n_erredges_fin;
                    double loba_err_init, loba_err_fin;
                    ba::localBA(mpLastKeyFrame, &core_kfs_,mvpLocalMapPoints, mpMap,
                                loba_n_erredges_init, loba_n_erredges_fin,
                                loba_err_init, loba_err_fin);
                    SLAM_STOP_TIMER("local_ba");
                    SLAM_LOG4(loba_n_erredges_init, loba_n_erredges_fin, loba_err_init, loba_err_fin);
                    SLAM_DEBUG_STREAM("Local BA:\t RemovedEdges {"<<loba_n_erredges_init<<", "<<loba_n_erredges_fin<<"} \t "
                                                                                                                     "Error {"<<loba_err_init<<", "<<loba_err_fin<<"}");
                }
                pLastRightFrame= new Frame(*mpCurrentRightFrame);
                mpLocalMapper->addFrame(pLastRightFrame);
                // init new depth-filters
                double depth_mean, depth_min;
                getSceneDepth(*mpLastKeyFrame, depth_mean, depth_min);
                mpLocalMapper->addKeyframe(mpLastKeyFrame, depth_mean, 0.5*depth_min);

            }
            else{            
                pLastFrame= new Frame(*mpCurrentFrame);
                pLastRightFrame= new Frame(*mpCurrentRightFrame);
            }
            if(mvpTemporalFrames.size()>=2){
                Frame * pTempF=*(++mvpTemporalFrames.rbegin());
                assert( pTempF->cam_id_ ==0);
                pLastFrame->SetPrevNextFrame( pTempF);
            }
            mvpTemporalFrames.push_back(pLastFrame);
            mvpTemporalFrames.push_back(pLastRightFrame);
        }

        if(bOK)
            mState = WORKING;
        else
            mState=LOST;

        // Reset if the camera get lost soon after initialization
        if(mState==LOST)
        {
            if(mpMap->KeyFramesInMap()<=5)
            {
                Reset();
                return;
            }
        }

        // Update motion model
        if(mbMotionModel)
        {
            if(bOK)
            {
                mVelocity = mpCurrentFrame->mTcw*mpLastFrame->GetPose().inverse();
            }
            else
                mVelocity = Sophus::SE3d();
        }
        mpLastFrame = mpCurrentFrame;
    }
    if(mbUseIMUData &&  mState == WORKING){
        (*Tw2c)= mpLastFrame->GetPose();
        (*sb)=mpLastFrame->speed_bias;
    }
    // Update drawer
    mpFramePublisher->Update(this, im);
}
// im is left image at frame k+1, imu_measurements from k to k+1,
// Tw2c initially is predicted Tw2c(k+1), later updates to corrected Tw2c(k+1),
// so does sb(speed of imu sensor in world frame and bias)
// make sure it works for the starting frame
// use only ORB features and stereo matches based on ORB, drawback: few matches
void  Tracking::ProcessFrameMono(cv::Mat &im, double timeStampSec,
                             const std::vector<Eigen::Matrix<double, 7,1> >& imu_measurements,
                             const Sophus::SE3d *pred_Tr_delta, Sophus::SE3d* Tw2c, Eigen::Matrix<double, 9,1>* sb)
{   /*
    Sophus::SE3d Tcp =(pred_Tr_delta==NULL? Sophus::SE3d(): (*pred_Tr_delta)); // current frame from previous frame

    // compute gravity direction in current camera frame
    Eigen::Vector3d ginc= ginw;
    if(mvpTemporalFrames.size() && (ginw.norm()>1e-6)){
        Eigen::Matrix3d Rw2p= mvpTemporalFrames.back()->GetRotation(); // world to previous left frame
        ginc= Tcp.rotationMatrix()*Rw2p*ginw;
    }
    //compute ORB descriptors

    if(mState==WORKING || mState==LOST)
            mpCurrentFrame.reset(new Frame(im,timeStampSec,mpORBextractor,mpORBVocabulary, cam_, ginc));
    else
            mpCurrentFrame.reset(new Frame(im,timeStampSec,mpIniORBextractor,mpORBVocabulary,cam_, ginc));

    if(mbUseIMUData && mvpTemporalFrames.size()){
        mpCurrentFrame->SetIMUObservations(imu_measurements);
    }
    // Depending on the state of the Tracker we perform different tasks
    if(mState==NO_IMAGES_YET)
    {
        mState = NOT_INITIALIZED;
    }
    mLastProcessedState=mState;
    if(mState==NOT_INITIALIZED)
    {
        FirstInitialization();
    }
    else if(mState==INITIALIZING)
    {
        // Check if current frame has enough keypoints, otherwise reset initialization process
        if(mpCurrentFrame->mvKeysUn.size()<=100)
        {
            fill(mvIniMatches.begin(),mvIniMatches.end(),-1);
            mState = NOT_INITIALIZED;
            return;
        }

        // Find correspondences
        ORBmatcher matcher(0.9,true);
        int nmatches = matcher.SearchForInitialization(*mpInitialFrame, *mpCurrentFrame,
                                                       mvbPrevMatched,mvIniMatches,100);

        // Check if there are enough correspondences
        if(nmatches<100)
        {
            mState = NOT_INITIALIZED;
            return;
        }

        cv::Mat Rcw; // Current Camera Rotation
        cv::Mat tcw; // Current Camera Translation
        vector<bool> vbTriangulated; // Triangulated Correspondences (mvIniMatches)

        if(mpInitializer->Initialize(*mpCurrentFrame, mvIniMatches, Rcw, tcw, mvIniP3D, vbTriangulated))
        {
            for(size_t i=0, iend=mvIniMatches.size(); i<iend;i++)
            {
                if(mvIniMatches[i]>=0 && !vbTriangulated[i])
                {
                    mvIniMatches[i]=-1;
                    nmatches--;
                }
            }
            float scale=0.f;
            if(mbUseIMUData){
                double maxVal1, maxVal2, minVal1, minVal2;
                cv::Mat tcp= Converter::toCvMat(Tcp.translation());
                cv::minMaxLoc(tcp, &minVal1, &maxVal1);
                cv::minMaxLoc(tcw, &minVal2, &maxVal2);

                if(abs(maxVal1)>abs(minVal1)){
                    assert(abs(maxVal2)>abs(minVal2));
                    scale=(float)maxVal1/maxVal2;
                }
                else
                {
                    assert(abs(maxVal2)<=abs(minVal2));
                    scale=(float)minVal1/minVal2;
                }
                scale*=mpCurrentFrame->mnId;
                assert(scale>0.f);
                cv::Mat residualR= Converter::toCvMat(Tcp.rotationMatrix()) - Rcw;
                cv::minMaxLoc(residualR, &minVal1, &maxVal1);
                assert(maxVal1<0.2 && minVal1>-0.2);
            }          
            CreateInitialMap(Converter::toMatrix3d(Rcw),Converter::toVector3d(tcw), scale);
        }
    }
    else
    {
        // System is initialized. Track Frame.
        bool bOK(false);
        // Initial Camera Pose Estimation from Previous Frame (Motion Model or Coarse) or Relocalisation
        // Huai: if loop closure is just finished, relocalize
        if(mState==WORKING && !RelocalisationRequested())
        {
            if(!mbMotionModel || mpMap->KeyFramesInMap()<4 || mpCurrentFrame->mnId<mnLastRelocFrameId+2)
                bOK = TrackPreviousFrame();
            else
            {
                if(pred_Tr_delta!=NULL)
                    mVelocity= Tcp;// use IMU prediction
                bOK = TrackWithMotionModel();
                if(!bOK)
                    bOK = TrackPreviousFrame();
            }
        }
        else
        {
            bOK = Relocalisation();
        }

        // If we have an initial estimation of the camera pose and matching. Track the local map.
        if(bOK && mnLastRelocFrameId!= mpCurrentFrame->mnId)
            bOK = TrackLocalMapDWO();
        // If tracking were good, check if we insert a keyframe
        if(bOK)
        {
            //mpMapPublisher->SetCurrentCameraPose(mpCurrentFrame->mTcw);

            // We DO NOT allow points with high innovation (considererd outliers by the Huber Function)
            // pass to the new keyframe, so that bundle adjustment will finally decide
            // if they are outliers or not. We don't want next frame to estimate its position
            // with those points so we discard them in the frame.
            for(size_t i=0; i<mpCurrentFrame->mvbOutlier.size();i++)
            {
                if(mpCurrentFrame->mvpMapPoints[i] && mpCurrentFrame->mvbOutlier[i])
                    mpCurrentFrame->mvpMapPoints[i]=NULL;
            }

            Frame * pLastFrame(NULL);
            KeyFrame *pPenultimateKeyFrame(mpLastKeyFrame);
            bool bNeedMorePoints=NeedNewKeyFrameStereo();
            bNeedMorePoints = bNeedMorePoints && (mnLastRelocFrameId!= mpCurrentFrame->mnId);

            if(bNeedMorePoints){
                CreateNewKeyFrame();
                pLastFrame=mpLastKeyFrame;
            }
            else
                pLastFrame= new Frame(*mpCurrentFrame);

            // add observations in current frame for matched map points
            vector<MapPoint*> vpMapPointMatches = pLastFrame->GetMapPointMatches();
            for(size_t i=0; i<vpMapPointMatches.size(); i++)
            {
                MapPoint* pMP = vpMapPointMatches[i];
                if(pMP&&(!pMP->isBad()))
                {
                    pMP->AddObservation(pLastFrame, i);
                }
            }
            if(bNeedMorePoints){
                mpLastKeyFrame->ComputeBoW(); //because bow is used in matching of createnewmappoints
                CreateNewMapPoints(pPenultimateKeyFrame, mpLastKeyFrame);
                mpLocalMapper->InsertKeyFrame(mpLastKeyFrame);
            }
            if(mvpTemporalFrames.size())
                pLastFrame->SetPrevNextFrame( mvpTemporalFrames.back());
            mvpTemporalFrames.push_back(pLastFrame);
        }

        if(bOK)
            mState = WORKING;
        else
            mState=LOST;

        // Reset if the camera get lost soon after initialization
        if(mState==LOST)
        {
            if(mpMap->KeyFramesInMap()<=5)
            {
                Reset();
                return;
            }
        }

        // Update motion model
        if(mbMotionModel)
        {
            if(bOK)
            {            
                mVelocity = mpCurrentFrame->mTcw*mpLastFrame->mTcw.inverse();
            }
            else
                mVelocity = Sophus::SE3d();
        }
        mpLastFrame = mpCurrentFrame;
    }
    if(mbUseIMUData &&  mState == WORKING){
        (*Tw2c)= mpLastFrame->mTcw;// this function handles type conversion
        (*sb)=mpLastFrame->speed_bias;
    }
    // Update drawer
    mpFramePublisher->Update(this, im);*/
}
// im is left image at frame k+1, imu_measurements from k to k+1,
// Tw2c initially is predicted Tw2c(k+1), later updates to corrected Tw2c(k+1),
// so does sb(speed of imu sensor in world frame and bias)
// using viso2 quadmatches saved viso2 stereo matching in contrast to ProcessFrame()

// for each current frame k, keypoints are created from quad matches between k-1 and k, and between k and k+1
// mvpTemporalFrames only contains frame up to previous frame, excluding it.
// The previous frame will be added as a keyframe, if there are not enough tracked mappoints in current frame.
// New map points are created using quad matches between the previous frame k-1 and current frame k
// Therefore, the last frame cannot be a keyframe and used in local mapping until the current frame is processed
void  Tracking::ProcessFrameViso2(cv::Mat &im, cv::Mat &right_img, double timeStampSec,
                             const std::vector<Eigen::Matrix<double, 7,1> >& imu_measurements,
                             const Sophus::SE3d *pred_Tr_delta, Sophus::SE3d* Tw2c,
                              Eigen::Matrix<double, 9,1>* sb)
{
    assert(im.channels()==1);
    Sophus::SE3d Tcp =(pred_Tr_delta==NULL? Sophus::SE3d():(*pred_Tr_delta)); // current frame from previous frame

    // compute visual odometry with libviso2
    int32_t dims[] = {im.cols,im.rows,im.cols};
    SLAM_START_TIMER("extract_quadmatches");
    // push back images, compute features
    mVisoStereo.matcher->pushBack(im.data,right_img.data,dims,false);

//    if(pred_Tr_delta){
//        mVisoStereo.setMotion(pred_Tr_delta->matrix().data());          //use IMU to aid feature tracking
//    }
    // match features
    mVisoStereo.Tr_valid=false; //do we use stereo or IMU prior information for tracking?
    // TODO: Prior motion from IMU noisy data leads to worse results; Strangely, stereo prior sometimes leads to worse result
    if (mVisoStereo.Tr_valid) mVisoStereo.matcher->matchFeatures(2,&mVisoStereo.Tr_delta);
    else          mVisoStereo.matcher->matchFeatures(2);

    vector<p_match> vQuadMatches = mVisoStereo.matcher->getMatches();
    libviso2::VisualOdometryStereo::parameters param=mVisoStereo.getParameters();
    mVisoStereo.matcher->bucketFeatures(param.bucket.max_features, param.bucket.bucket_width,param.bucket.bucket_height);
    vector<p_match> p_matched = mVisoStereo.matcher->getMatches();
    SLAM_STOP_TIMER("extract_quadmatches");
    vector<double> tr_delta;
    SLAM_START_TIMER("track_previous_frame");
    const enum Optimizer {RANSAC_Geiger, ROBUST_Badino, ROBUST_Klein} approach=RANSAC_Geiger;
    switch (approach){
    case RANSAC_Geiger:
        tr_delta= mVisoStereo.estimateMotion(p_matched);
        // on failure
        if (tr_delta.size()!=6)
          mVisoStereo.Tr_valid=false;
        else{
        // set transformation matrix (previous to current frame)
            mVisoStereo.Tr_delta = mVisoStereo.transformationVectorToMatrix(tr_delta);
            mVisoStereo.Tr_valid = true;
        }
        break;
    case ROBUST_Badino:
        tr_delta=mVisoStereo.estimateMotionBadino(p_matched);
        // on failure
        if (tr_delta.size()!=6)
          mVisoStereo.Tr_valid=false;
        else{
        // set transformation matrix (previous to current frame)
            mVisoStereo.Tr_delta = mVisoStereo.transformationVectorToMatrix(tr_delta);
            mVisoStereo.Tr_valid = true;
        }
        break;
    case ROBUST_Klein:
        mVisoStereo.estimateMotionKlein(p_matched, vector<vector<float> >());
        break;
    }

    if (mVisoStereo.Tr_valid) {
        mPose = mPose * libviso2::Matrix::inv(mVisoStereo.Tr_delta);     // update current pose
        Tcp= Converter::toSE3d(mVisoStereo.Tr_delta);
        //sieve inliers from quad matches, this does not matter much
//        vector<bool> vInliers;
//        mVisoStereo.getAllInlier(vQuadMatches, mVisoStereo.Tr_delta, vInliers);
//        cout<<"Inliers out of quad matches :"<< std::accumulate(vInliers.begin(), vInliers.end(), 0)<<" of "<< vQuadMatches.size()<<endl;
//        //apply vInliers
//        size_t jack=0;
//        for(vector<p_match>::iterator vIt=vQuadMatches.begin(); vIt!=vQuadMatches.end(); ++vIt, ++jack)
//        {
//            if(!vInliers[jack])
//                vIt->i1c=-1;
//        }
//        assert(jack==vInliers.size());

    }
    else{
        cerr<<"libviso2 odometry failed for images at time:"<<timeStampSec
           <<" of quad matches:"<<vQuadMatches.size()<<endl;
    }
    SLAM_STOP_TIMER("track_previous_frame");
    // compute gravity direction in current camera frame
    Eigen::Vector3d ginc=ginw;
    if(mvpTemporalFrames.size() && (ginw.norm()>1e-6)){
        Eigen::Matrix3d Rw2p= mvpTemporalFrames.back()->GetRotation(); // world to previous left frame
        ginc= Tcp.rotationMatrix()*Rw2p*ginw;
    }
    //compute ORB descriptors of vQuadMatches, map viso2 feature id to keypoint id
    SLAM_START_TIMER("create_frame");
    mpCurrentFrame.reset(new Frame(im, timeStampSec, mVisoStereo.matcher->getNumDenseFeatures(true),
                         right_img, mVisoStereo.matcher->getNumDenseFeatures(false),
                         mpORBextractor, mpORBVocabulary,cam_, right_cam_,
                             vQuadMatches, mTl2r, ginc, sb==NULL?Eigen::Matrix<double, 9,1>::Zero():(*sb)));

    if(mbUseIMUData && mVisoStereo.Tr_valid){
        mpCurrentFrame->SetIMUObservations(imu_measurements);
    }
    SLAM_STOP_TIMER("create_frame");
    // Depending on the state of the Tracker we perform different tasks
    if(mState==NO_IMAGES_YET)
    {
        mState = NOT_INITIALIZED;
    }
    mLastProcessedState=mState;
    mLastLeftImg= im.clone();
    mLastRightImg= right_img.clone();
    if(mState==NOT_INITIALIZED)
    {   
        FirstInitialization();
    }
    else if(mState==INITIALIZING)
    {
        //use quadmatches to fill keypoints in the first frame
        KeyFrame* pFirstKF=(KeyFrame*)mvpTemporalFrames.back();
        Eigen::Matrix3d Rw2p= mvpTemporalFrames.back()->GetRotation(); // world to previous left frame
        if(ginw.norm()<1e-6)
            ginc=Eigen::Vector3d::Zero();
        else
            ginc= Rw2p*ginw;
        pFirstKF->RefillKeyPoints(mLastLeftImg, mLastRightImg, vQuadMatches, ginc);

        //map quadmatches from viso2 id to keypoints index
        remapQuadMatches(vQuadMatches, mpCurrentFrame->viso2LeftId2StereoId,   mpCurrentFrame->viso2RightId2StereoId,
                         pFirstKF->viso2LeftId2StereoId,   pFirstKF->viso2RightId2StereoId);

        // Check if current frame has enough matches with its previous frame, otherwise reset initialization process
        if(vQuadMatches.size()<=100)
        {
            fill(mvIniMatches.begin(),mvIniMatches.end(),-1);
            mState = NOT_INITIALIZED;
            return;
        }
        CreateInitialMapStereo(Tcp, vQuadMatches);
    }
    else
    {
       
        // System is initialized. Track Frame.
        bool bOK(false);
        // Initial Camera Pose Estimation from Previous Frame (Motion Model or Coarse) or Relocalisation
        // Huai: if loop closure is just finished, relocalize
        if(mState==WORKING && !RelocalisationRequested())
        {
    		mLastFrame.RefillKeyPoints(mLastLeftImg, mLastRightImg, vQuadMatches, ginc);
            remapQuadMatches(vQuadMatches, mCurrentFrame.viso2LeftId2StereoId,   mCurrentFrame.viso2RightId2StereoId,
                             mLastFrame.viso2LeftId2StereoId,   mLastFrame.viso2RightId2StereoId);
            bOK = TrackPreviousFrame(Tcp, vQuadMatches);
        }
        else
        {
            bOK = Relocalisation();
        }
//        if(mpLoopClosing->UpdateTemporalWinPoses() ==false)
//            UpdatePoseOfFrameInTemporalWindow();
        // If we have an initial estimation of the camera pose and matching. Track the local map.
        SLAM_START_TIMER("local_optimize");
        if(bOK && mnLastRelocFrameId!= mpCurrentFrame->mnId)
            bOK = TrackLocalMapDWO();
        SLAM_STOP_TIMER("local_optimize");
        // If tracking were good, check if we insert a keyframe
        if(bOK)
        {
            //mpMapPublisher->SetCurrentCameraPose(mpCurrentFrame->mTcw);
            // We DO NOT allow points with high innovation (considererd outliers by the Huber Function)
            // pass to the new keyframe, so that bundle adjustment will finally decide
            // if they are outliers or not. We don't want next frame to estimate its position
            // with those points so we discard them in the frame.
            for(size_t i=0; i<mpCurrentFrame->mvbOutlier.size();i++)
            {
                if(mpCurrentFrame->mvpMapPoints[i] && mpCurrentFrame->mvbOutlier[i])
                    mpCurrentFrame->mvpMapPoints[i]=NULL;
            }

            Frame * pLastFrame(NULL);
            bool bNeedMorePoints=NeedNewKeyFrameStereo();
            bNeedMorePoints = bNeedMorePoints && (mnLastRelocFrameId!= mpCurrentFrame->mnId);

            if(bNeedMorePoints){
           //     CreateNewKeyFrame();
           //     pLastFrame=mpLastKeyFrame;
    			assert(mpLastFrame->mnId>1);
                KeyFrame* pKF = new KeyFrame(*mpLastFrame,mpMap,mpKeyFrameDB);
                pKF->SetNotErase();//because we use it in double window based optimization

                mnLastKeyFrameId = mpLastFrame.mnId;
                mpLastKeyFrame = pKF;
                pLastFrame=pKF;
            }
            else
                pLastFrame= new Frame(*mpLastFrame);
 			if(bNeedMorePoints){
            // add observations in current frame for matched map points
            vector<MapPoint*> vpMapPointMatches = pLastFrame->GetMapPointMatches();
            for(size_t i=0; i<vpMapPointMatches.size(); ++i)
            {
                MapPoint* pMP = vpMapPointMatches[i];
                if(pMP&&(!pMP->isBad()))
                {
                    pMP->AddObservation(pLastFrame, i);
                    pMP->AddObservation(pLastFrame, i, false);
                }
            
                SLAM_START_TIMER("triangulate_new_mappoint");
                mpLastKeyFrame->ComputeBoW();
                CreateNewMapPoints(vQuadMatches);
                SLAM_STOP_TIMER("triangulate_new_mappoint");
                mpLocalMapper->InsertKeyFrame(mpLastKeyFrame);
            }
            if(mvpTemporalFrames.size())
                pLastFrame->SetPrevNextFrame( mvpTemporalFrames.back());
			if(pLastFrame->mnId!=1)// skip the second frame which is already in temporal window
            mvpTemporalFrames.push_back(pLastFrame);
        }

        if(bOK)
            mState = WORKING;
        else
            mState=LOST;

        // Reset if the camera get lost soon after initialization
        if(mState==LOST)
        {
            if(mpMap->KeyFramesInMap()<=5)
            {
                Reset();
                return;
            }
        }

        // Update motion model
        if(mbMotionModel)
        {
            if(bOK)
            {         
                mVelocity = mpCurrentFrame->mTcw*mpLastFrame->mTcw.inverse();
            }
            else
                mVelocity = Sophus::SE3d();
        }
        mpLastFrame = mpCurrentFrame;
    }
    if(mbUseIMUData &&  mState == WORKING){
        (*Tw2c)= mpLastFrame->mTcw;// this function handles type conversion
        (*sb)=mpLastFrame->speed_bias;
    }
    // Update drawer
    mpFramePublisher->Update(this, im);
}
// im is left image at frame k+1, imu_measurements from k to k+1,
// Tw2c initially is predicted Tw2c(k+1), later updates to corrected Tw2c(k+1),
// so does sb(speed of imu sensor in world frame and bias)
// make sure ProcessFrame work for the starting frame
void  Tracking::ProcessFrame(cv::Mat &im, cv::Mat &right_img, double timeStampSec,
                             const std::vector<Eigen::Matrix<double, 7,1> >& imu_measurements,
                             const Sophus::SE3d *pred_Tr_delta, Sophus::SE3d* Tw2c, Eigen::Matrix<double, 9,1>* sb)
{
    assert(im.channels()==1);
    Sophus::SE3d Tcp =pred_Tr_delta==NULL? Sophus::SE3d(): (*pred_Tr_delta); // current frame from previous frame

    // compute visual odometry with libviso2
    int32_t dims[] = {im.cols,im.rows,im.cols};
    SLAM_START_TIMER("extract_quadmatches");
    // push back images, compute features
    mVisoStereo.matcher->pushBack(im.data,right_img.data,dims,false);

//    if(pred_Tr_delta){
//        mVisoStereo.setMotion(pred_Tr_delta->matrix().data());          //use IMU to aid feature tracking
//    }
    // match features  
    mVisoStereo.Tr_valid=false; //do we use stereo or IMU prior information for tracking?
    // TODO: Prior motion from IMU noisy data leads to worse results; Strangely, stereo prior sometimes leads to worse result
    if (mVisoStereo.Tr_valid) mVisoStereo.matcher->matchFeatures(2,&mVisoStereo.Tr_delta);
    else          mVisoStereo.matcher->matchFeatures(2);

    vector<p_match> vQuadMatches = mVisoStereo.matcher->getMatches();
    libviso2::VisualOdometryStereo::parameters param=mVisoStereo.getParameters();
    mVisoStereo.matcher->bucketFeatures(param.bucket.max_features, param.bucket.bucket_width,param.bucket.bucket_height);
    vector<p_match> p_matched = mVisoStereo.matcher->getMatches();
    SLAM_STOP_TIMER("extract_quadmatches");

    SLAM_START_TIMER("track_previous_frame");
    vector<double> tr_delta;
    const enum Optimizer {RANSAC_Geiger, ROBUST_Badino, ROBUST_Klein} approach=RANSAC_Geiger;
    switch (approach){
    case RANSAC_Geiger:
        tr_delta= mVisoStereo.estimateMotion(p_matched);
        // on failure
        if (tr_delta.size()!=6)
          mVisoStereo.Tr_valid=false;
        else{            
        // set transformation matrix (previous to current frame)
            mVisoStereo.Tr_delta = mVisoStereo.transformationVectorToMatrix(tr_delta);
            mVisoStereo.Tr_valid = true;
        }
        break;
    case ROBUST_Badino:
        tr_delta=mVisoStereo.estimateMotionBadino(p_matched);
        // on failure
        if (tr_delta.size()!=6)
          mVisoStereo.Tr_valid=false;
        else{
        // set transformation matrix (previous to current frame)
            mVisoStereo.Tr_delta = mVisoStereo.transformationVectorToMatrix(tr_delta);
            mVisoStereo.Tr_valid = true;
        }
        break;
    case ROBUST_Klein:
        mVisoStereo.estimateMotionKlein(p_matched, vector<vector<float> >());
        break;
    }

    if (mVisoStereo.Tr_valid) {
        Tcp= Converter::toSE3d(mVisoStereo.Tr_delta);
        mPose = mPose * libviso2::Matrix::inv(mVisoStereo.Tr_delta);     // update current pose
        //sieve inliers from quad matches, does not matter much
//        vector<bool> vInliers;
//        mVisoStereo.getAllInlier(vQuadMatches, mVisoStereo.Tr_delta, vInliers);
//        cout<<"Inliers out of quad matches :"<< std::accumulate(vInliers.begin(), vInliers.end(), 0)<<" of "<< vQuadMatches.size()<<endl;
//        //apply vInliers
//        size_t jack=0;
//        for(vector<p_match>::iterator vIt=vQuadMatches.begin(); vIt!=vQuadMatches.end();++jack, ++vIt )
//        {
//            if(!vInliers[jack])
//                vIt->i1c=-1;
//        }
//        assert(jack==vInliers.size());

    }
    else{    
        cerr<<"libviso2 odometry failed for images at time:"<<timeStampSec
           <<" of quad matches:"<<vQuadMatches.size()<<endl;
    }
    SLAM_STOP_TIMER("track_previous_frame");
    //stereo matching
    SLAM_START_TIMER("stereo_matching");
    mVisoStereo.matcher->matchFeatures(1);
    vector<p_match> vStereoMatches = mVisoStereo.matcher->getMatches();
//    cout<<"stereo matches in image:"<< vStereoMatches.size() <<endl;
    // mVisoStereo.matcher->refineFeatures(vStereoMatches);
    SLAM_STOP_TIMER("stereo_matching");
    // compute gravity direction in current camera frame
    Eigen::Vector3d ginc= ginw;
    if(mvpTemporalFrames.size() && (ginw.norm()>1e-6)){
        Eigen::Matrix3d Rw2p= mvpTemporalFrames.back()->GetRotation(); // world to previous left frame
        ginc= Tcp.rotationMatrix()*Rw2p*ginw;
    }
    SLAM_START_TIMER("create_frame");
    //compute ORB descriptors of vStereoMatches
    mpCurrentFrame.reset(new Frame(im, timeStampSec, mVisoStereo.matcher->getNumDenseFeatures(true),
                         right_img, mVisoStereo.matcher->getNumDenseFeatures(false),
                         vStereoMatches, mpORBextractor, mpORBVocabulary, cam_, right_cam_,
                         mTl2r, ginc, sb==NULL?Eigen::Matrix<double, 9,1>::Zero():(*sb)));

    // also test whether a quad match satisfy stereo matches
    remapQuadMatches(vQuadMatches,
                     mpCurrentFrame->viso2LeftId2StereoId,   mpCurrentFrame->viso2RightId2StereoId,
                     mpLastFrame->viso2LeftId2StereoId,   mpLastFrame->viso2RightId2StereoId);

    if(mbUseIMUData && mVisoStereo.Tr_valid){
        mpCurrentFrame->SetIMUObservations(imu_measurements);
    }
    SLAM_STOP_TIMER("create_frame");
    // Depending on the state of the Tracker we perform different tasks
    if(mState==NO_IMAGES_YET)
    {
        mState = NOT_INITIALIZED;
    }
    mLastProcessedState=mState;
    if(mState==NOT_INITIALIZED)
    {
        FirstInitialization();
    }
    else if(mState==INITIALIZING)
    {
        // Check if current frame has enough matches with its previous frame, otherwise reset initialization process
        if(vQuadMatches.size()<=100)
        {
            fill(mvIniMatches.begin(),mvIniMatches.end(),-1);
            mState = NOT_INITIALIZED;
            return;
        }
        CreateInitialMapStereo(Tcp, vQuadMatches);
    }
    else
    {
        // System is initialized. Track Frame.
        bool bOK(false);
        // Initial Camera Pose Estimation from Previous Frame (Motion Model or Coarse) or Relocalisation
        // Huai: if loop closure is just finished, relocalize
        if(mState==WORKING && !RelocalisationRequested())
        {
            bOK = TrackPreviousFrame(Tcp, vQuadMatches);
        }
        else
        {
            bOK = Relocalisation();
        }
//        if(mpLoopClosing->UpdateTemporalWinPoses() ==false)
//            UpdatePoseOfFrameInTemporalWindow();
        // If we have an initial estimation of the camera pose and matching. Track the local map.
        if(bOK && mnLastRelocFrameId!= mpCurrentFrame->mnId){
            SLAM_START_TIMER("local_optimize");
            bOK = TrackLocalMapDWO();
            SLAM_STOP_TIMER("local_optimize");
        }
        // If tracking were good, check if we insert a keyframe
        if(bOK)
        {
            //mpMapPublisher->SetCurrentCameraPose(mpCurrentFrame->mTcw);
            // We DO NOT allow points with high innovation (considererd outliers by the Huber Function)
            // pass to the new keyframe, so that bundle adjustment will finally decide
            // if they are outliers or not. We don't want next frame to estimate its position
            // with those points so we discard them in the frame.
            for(size_t i=0; i<mpCurrentFrame->mvbOutlier.size();i++)
            {
                if(mpCurrentFrame->mvpMapPoints[i] && mpCurrentFrame->mvbOutlier[i])
                    mpCurrentFrame->mvpMapPoints[i]=NULL;
            }

            Frame * pLastFrame(NULL);
#ifdef HUAI_DEBUG
            if(mpCurrentFrame->mnId>460)
            {
                int nRefMatches = mpReferenceKF->TrackedMapPoints();
                cout<<"ref id and matches:"<< mpReferenceKF->mnId<<" "<<nRefMatches<<endl;
            }
#endif
            bool bNeedMorePoints=NeedNewKeyFrameStereo();
            bNeedMorePoints = bNeedMorePoints && (mnLastRelocFrameId!= mpCurrentFrame->mnId);

            if(bNeedMorePoints){
                CreateNewKeyFrame();                
                pLastFrame=mpLastKeyFrame;
            }
            else
                pLastFrame= new Frame(*mpCurrentFrame);
			if(bNeedMorePoints){
            // add observations in current frame for matched map points
			size_t iota=0;            
			for(auto it= mpLastKeyFrame->mvpMapPoints.begin(), ite= mpLastKeyFrame->mvpMapPoints.end(); it!=ite; ++it,++iota)
            {
                MapPoint* pMP =*it;
                if(pMP&&(!pMP->isBad()))
                {
                    pMP->AddObservation(mpLastKeyFrame, iota);
                    pMP->AddObservation(mpLastKeyFrame, iota, false);
                }
            }
            
                SLAM_START_TIMER("triangulate_new_mappoint");
                mpLastKeyFrame->ComputeBoW();
                CreateNewMapPoints(vQuadMatches);
                SLAM_STOP_TIMER("triangulate_new_mappoint");
                mpLocalMapper->InsertKeyFrame(mpLastKeyFrame);
            }
            if(mvpTemporalFrames.size())
                pLastFrame->SetPrevNextFrame( mvpTemporalFrames.back());
            mvpTemporalFrames.push_back(pLastFrame);
        }

        if(bOK)
            mState = WORKING;
        else
            mState=LOST;

        // Reset if the camera get lost soon after initialization
        if(mState==LOST)
        {
            if(mpMap->KeyFramesInMap()<=5)
            {
                Reset();
                return;
            }
        }

        // Update motion model
        if(mbMotionModel)
        {
            if(bOK)
            {              
                mVelocity = mpCurrentFrame->mTcw*mpLastFrame->mTcw.inverse();
            }
            else
                mVelocity = Sophus::SE3d();
        }
        mpLastFrame = mpCurrentFrame;
    }
    if(mbUseIMUData &&  mState == WORKING){
        (*Tw2c)= mpLastFrame->mTcw;// this function handles type conversion
        (*sb)=mpLastFrame->speed_bias;
    }
    // Update drawer
    mpFramePublisher->Update(this, im);*/
}

void Tracking::FirstInitialization()
{    
    //We ensure a minimum ORB features to continue, otherwise discard frame
    if(mpCurrentFrame->mvKeysUn.size()>100 || mpCurrentFrame->mvKeysUn.size()==0) //second condition for ProcessFrameViso2
    {
        mpCurrentFrame->SetPose( Sophus::SE3d());
        mpInitialFrame=mpCurrentFrame;
        mpLastFrame = mpCurrentFrame;
        mvbPrevMatched.resize(mpInitialFrame->mvKeysUn.size());
        for(size_t i=0; i<mpInitialFrame->mvKeysUn.size(); i++)
            mvbPrevMatched[i]=mpInitialFrame->mvKeysUn[i].pt;

        if(mpInitializer)
            delete mpInitializer;

        mpInitializer =  new Initializer(*mpInitialFrame,1.0,200);
        mState = INITIALIZING;

        mvpTemporalFrames.push_back(new KeyFrame(*mpInitialFrame,mpMap,mpKeyFrameDB));
    }
}
// not used for stereo processing case
void Tracking::Initialize()
{
    // Check if current frame has enough keypoints, otherwise reset initialization process
    if(mpCurrentFrame->mvKeysUn.size()<=100)
    {
        fill(mvIniMatches.begin(),mvIniMatches.end(),-1);
        mState = NOT_INITIALIZED;
        return;
    }    

    // Find correspondences
    ORBmatcher matcher(0.9,true);
    int nmatches = matcher.SearchForInitialization(*mpInitialFrame,*mpCurrentFrame,mvbPrevMatched,mvIniMatches,100);

    // Check if there are enough correspondences
    if(nmatches<100)
    {
        mState = NOT_INITIALIZED;
        return;
    }  

    cv::Mat Rcw; // Current Camera Rotation
    cv::Mat tcw; // Current Camera Translation
    vector<bool> vbTriangulated; // Triangulated Correspondences (mvIniMatches)

    if(mpInitializer->Initialize(*mpCurrentFrame, mvIniMatches, Rcw, tcw, mvIniP3D, vbTriangulated))
    {
        for(size_t i=0, iend=mvIniMatches.size(); i<iend;i++)
        {
            if(mvIniMatches[i]>=0 && !vbTriangulated[i])
            {
                mvIniMatches[i]=-1;
                nmatches--;
            }           
        }

        CreateInitialMap(Converter::toMatrix3d(Rcw),Converter::toVector3d(tcw));
    }
}
//create initial map with two consecutive pairs of frames of the stereo sequence
// the first frame is an ordinary frame, the second frame becomes a keyframe
void Tracking::CreateInitialMapStereo(const Sophus::SE3d & Tcw, const std::vector<int> &vIniMatches)
{
    // Set Frame Poses
    mpCurrentFrame->SetPose(Tcw);

    // Create KeyFrames
    KeyFrame* pPrevFrame = (KeyFrame*)(mvpTemporalFrames.back());   
    KeyFrame* pKFcur = new KeyFrame(*mpCurrentFrame,mpMap,mpKeyFrameDB);

    pPrevFrame->ComputeBoW();
    pKFcur->ComputeBoW();
    // Insert KFs in the map
    mpMap->AddKeyFrame(pPrevFrame);
    mpMap->AddKeyFrame(pKFcur);
    assert(pKFcur->mnFrameId==1);

    Sophus::SE3d proxy=pKFcur->GetPose();
    Eigen::Matrix<double,3,4> Tw2c1= proxy.matrix3x4();
    Eigen::Matrix3d Rw2c1= Tw2c1.topLeftCorner<3,3>();
    Eigen::Vector3d twinc1=Tw2c1.col(3);

    proxy= pPrevFrame->GetPose();
    Eigen::Matrix<double,3,4> Tw2c2= proxy.matrix3x4();
    Eigen::Matrix3d Rw2c2= Tw2c2.topLeftCorner<3,3>();
    Eigen::Vector3d twinc2=Tw2c2.col(3);

    const float ratioFactor = 1.5f*pKFcur->GetScaleFactor();
    Eigen::Vector3d Ow1 = pKFcur->GetCameraCenter();
    Eigen::Vector3d Ow2 = pPrevFrame->GetCameraCenter();
    for(size_t indigo=0; indigo< vIniMatches.size(); ++indigo){
        if(vIniMatches[indigo]==-1) continue;
        // Triangulate each match
        const cv::KeyPoint &kp1 = pKFcur->mvKeysUn[vIniMatches[indigo]];
        const cv::KeyPoint &kp2 = pPrevFrame->mvKeysUn[indigo];

            // Check parallax between rays
            Vector3d xn1((kp1.pt.x-pKFcur->cam_->cx())/pKFcur->cam_->fx(),
                         (kp1.pt.y-pKFcur->cam_->cy())/pKFcur->cam_->fy(), 1.0 ),
                    ray1(pKFcur->fts_[vIniMatches[indigo]]->f);

            Vector3d xn2((kp2.pt.x-pPrevFrame->cam_->cx())/pPrevFrame->cam_->fx(),
                         (kp2.pt.y-pPrevFrame->cam_->cy())/pPrevFrame->cam_->fy(), 1.0 );

            Vector3d ray2 = mTl2r.rotationMatrix().transpose()* (pPrevFrame->fts_[indigo]->f);
            assert(abs(ray1.norm()*ray2.norm()-1)<1e-6);
            const float cosParallaxRays = ray1.dot(ray2);

            if(cosParallaxRays<0 || cosParallaxRays>0.9998)
                continue;

            // Linear Triangulation Method        
            Eigen::Matrix<double,4,4> A;
            A.row(0) = xn1(0)*Tw2c1.row(2)-Tw2c1.row(0);
            A.row(1) = xn1(1)*Tw2c1.row(2)-Tw2c1.row(1);

            A.row(2) = xn2(0)*Tw2c2.row(2)-Tw2c2.row(0);
            A.row(3) = xn2(1)*Tw2c2.row(2)-Tw2c2.row(1);

            cv::Mat Aprime, w,u,vt;
            cv::eigen2cv(A, Aprime);
            cv::SVD::compute(Aprime,w,u,vt,cv::SVD::MODIFY_A| cv::SVD::FULL_UV);
            cv::Mat x3D = vt.row(3).t();
            if(abs(x3D.at<double>(3))<1e-6)
                continue;

            // Euclidean coordinates
            x3D = x3D.rowRange(0,3)/x3D.at<double>(3);
            Eigen::Vector3d x3Dt;
            cv::cv2eigen (x3D, x3Dt);

            //Check triangulation in front of cameras
            float z1 = Rw2c1.row(2)*x3Dt+ twinc1(2);
            if(z1<=0)
                continue;
            float z2 = Rw2c2.row(2)*x3Dt+twinc2(2);
            if(z2<=0)
                continue;

            //Check reprojection error in first keyframe
            float sigmaSquare1 = pKFcur->GetSigma2(kp1.octave);
            float x1 = Rw2c1.row(0)*x3Dt+twinc1(0);
            float y1 = Rw2c1.row(1)*x3Dt+twinc1(1);
            float invz1 = 1.0/z1;
            float u1 = pKFcur->cam_->fx()*x1*invz1+pKFcur->cam_->cx();
            float v1 = pKFcur->cam_->fy()*y1*invz1+pKFcur->cam_->cy();
            float errX1 = u1 - kp1.pt.x;
            float errY1 = v1 - kp1.pt.y;
            if((errX1*errX1+errY1*errY1)>5.991*sigmaSquare1)
                continue;

            //Check reprojection error in second frame
            float sigmaSquare2 = pPrevFrame->GetSigma2(kp2.octave);
            float x2 = Rw2c2.row(0)*x3Dt+twinc2(0);
            float y2 = Rw2c2.row(1)*x3Dt+twinc2(1);
            float invz2 = 1.0/z2;
            float u2 = pPrevFrame->cam_->fx()*x2*invz2+pPrevFrame->cam_->cx();
            float v2 = pPrevFrame->cam_->fy()*y2*invz2+pPrevFrame->cam_->cy();
            float errX2 = u2 - kp2.pt.x;
            float errY2 = v2 - kp2.pt.y;
            if((errX2*errX2+errY2*errY2)>5.991*sigmaSquare2)
                continue;

            //Check scale consistency
            Eigen::Vector3d normal1 = x3Dt-Ow1;
            float dist1 = normal1.norm();

            Eigen::Vector3d normal2 = x3Dt-Ow2;
            float dist2 = normal2.norm();

            if(dist1==0 || dist2==0)
                continue;

            float ratioDist = dist1/dist2;
            if(ratioDist*ratioFactor<1.f || ratioDist>ratioFactor)
                continue;

            // Triangulation is succesful
            MapPoint* pMP = new MapPoint(x3Dt,pKFcur, vIniMatches[indigo], mpMap);

            pMP->AddObservation(pPrevFrame,indigo);

            pPrevFrame->AddMapPoint(pMP,indigo);
            pKFcur->AddMapPoint(pMP,vIniMatches[indigo]);

            pMP->ComputeDistinctiveDescriptors();
            pMP->UpdateNormalAndDepth();
            //Fill Current Frame structure
            mpCurrentFrame->AddMapPoint(pMP,vIniMatches[indigo]);
            mpMap->AddMapPoint(pMP);
    }

    // Bundle Adjustment
    char buffer[300];
    sprintf(buffer, "New Map created with %d points",mpMap->MapPointsInMap());
    cout<<buffer<<endl;

//    Optimizer::GlobalBundleAdjustemnt(mpMap,20);
    float medianDepth = pKFcur->ComputeSceneMedianDepth(2);

    if(medianDepth<0 || pKFcur->TrackedMapPoints()<50)
    {
        cout<<("Wrong initialization, reseting...");
        Reset();
        return;
    }
    // Update Connections
    pPrevFrame->UpdateConnections();
    pKFcur->UpdateConnections();

    double depth_mean, depth_min;
    getSceneDepth(*pKFcur, depth_mean, depth_min);
    mpLocalMapper->addKeyframe(pKFcur, depth_mean, 0.5*depth_min);

    mpCurrentFrame->SetPose(pKFcur->GetPose());
    if(mpCurrentFrame->cam_id_==0){
    mpLastFrame = mpCurrentFrame;
    }
    mnLastKeyFrameId=pKFcur->mnId;
    mpLastKeyFrame = pKFcur;

    mvpLocalMapPoints=mpMap->GetAllMapPoints();
    mpReferenceKF = pPrevFrame;

    mpMap->SetReferenceMapPoints(mvpLocalMapPoints);

//    mvpTemporalFrames.push_back(pPrevFrame);
//    if(pPrevFrame->mnId+2==pKFcur->mnId){
//        pKFcur->SetPrevNextFrame( pPrevFrame);
        mvpTemporalFrames.push_back(pKFcur);
//    }
//    else{
//        mvpTemporalFrames.pop_front();
//        mvpTemporalFrames.push_back(pKFcur);
//        assert(mvpTemporalFrames.size()==1);
//    }
    //mpMapPublisher->SetCurrentCameraPose(pKFcur->GetPose());
    mState=WORKING;
}

//create initial map with two consecutive pairs of frames of the stereo sequence
// the first frame is an ordinary frame, the second frame becomes a keyframe
void Tracking::CreateInitialMapStereo(const Sophus::SE3d &Tcw, const std::vector<p_match> &vQuadMatches )
{
    // Set Frame Poses   
    mpCurrentFrame->SetPose(Tcw);

    // Create KeyFrames
    KeyFrame* pPrevFrame = (KeyFrame*) mvpTemporalFrames.back();
    KeyFrame* pKFcur = new KeyFrame(*mpCurrentFrame,mpMap,mpKeyFrameDB);
    pPrevFrame->ComputeBoW();
    pKFcur->ComputeBoW();
    // Insert KFs in the map
    mpMap->AddKeyFrame(pPrevFrame);
    mpMap->AddKeyFrame(pKFcur);
    assert(pKFcur->mnFrameId==1);
//use vMPGrid to control map point distribution and number
    const int nGridCols= (pKFcur->mnMaxX - pKFcur->mnMinX)/ 30 +1,
            nGridRows= (pKFcur->mnMaxY - pKFcur->mnMinY)/ 30 +1;
    float fGridElementWidthInv=static_cast<float>(nGridCols)/static_cast<float>(pKFcur->mnMaxX-pKFcur->mnMinX);
    float fGridElementHeightInv=static_cast<float>(nGridRows)/static_cast<float>(pKFcur->mnMaxY-pKFcur->mnMinY);
    const size_t nPointPerCell=mnFeatures/(nGridCols*nGridRows) +1;
    std::vector< std::vector< std::vector<size_t> > > vMPGrid;// record observations of mappoints in the new keyframe
    vMPGrid.resize(nGridCols);
    for(int jack=0; jack< nGridCols;++jack)
    {
        vMPGrid[jack].resize(nGridRows);
        vMPGrid.reserve(nPointPerCell);
    }

    Sophus::SE3d proxy=pKFcur->GetPose();
    Eigen::Matrix<double,3,4> Tw2c1= proxy.matrix3x4();
    Eigen::Matrix3d Rw2c1= Tw2c1.topLeftCorner<3,3>();
    Eigen::Vector3d twinc1=Tw2c1.col(3);

    proxy= pPrevFrame->GetPose();
    Eigen::Matrix<double,3,4> Tw2c2= proxy.matrix3x4();
    Eigen::Matrix3d Rw2c2= Tw2c2.topLeftCorner<3,3>();
    Eigen::Vector3d twinc2=Tw2c2.col(3);

    Eigen::Matrix<double,3,4> Tw2c1r= pKFcur->GetPose(false).matrix3x4();
    Eigen::Matrix<double,3,4> Tw2c2r= pPrevFrame->GetPose(false).matrix3x4();

    const float ratioFactor = 1.5f*pKFcur->GetScaleFactor();
    Eigen::Vector3d Ow1 = pKFcur->GetCameraCenter();
    Eigen::Vector3d Ow2 = pPrevFrame->GetCameraCenter();
    for(size_t indigo=0; indigo< vQuadMatches.size(); ++indigo){
        // Create MapPoints and asscoiate to keyframes
        cv::KeyPoint kpUn=pKFcur->mvKeysUn[vQuadMatches[indigo].i1c];
        int posX = round((kpUn.pt.x-pKFcur->mnMinX)*fGridElementWidthInv);
        int posY = round((kpUn.pt.y-pKFcur->mnMinY)*fGridElementHeightInv);
        if(!(posX<0 || posX>=nGridCols || posY<0 || posY>=nGridRows) && vMPGrid[posX][posY].size()< nPointPerCell)
        {
            // Triangulate each match
            const cv::KeyPoint &kp1 = pKFcur->mvKeysUn[vQuadMatches[indigo].i1c];
            const cv::KeyPoint &kp2 = pKFcur->mvRightKeysUn[vQuadMatches[indigo].i1c];

            // Check parallax between left and right rays
            Vector3d xn1((kp1.pt.x-pKFcur->cam_->cx())/pKFcur->cam_->fx(),
                         (kp1.pt.y-pKFcur->cam_->cy())/pKFcur->cam_->fy(), 1.0 ),
                    ray1(xn1);

            Vector3d xn2((kp2.pt.x-pKFcur->right_cam_->cx())/pKFcur->right_cam_->fx(),
                         (kp2.pt.y-pKFcur->right_cam_->cy())/pKFcur->right_cam_->fy(), 1.0 );
            Vector3d ray2 = mTl2r.rotationMatrix().transpose()*xn2;
            const float cosParallaxRays = ray1.dot(ray2)/(ray1.norm()*ray2.norm());

            if(cosParallaxRays<0 || cosParallaxRays>0.9998)
                continue;

            // Linear Triangulation Method
            const cv::KeyPoint &kp3 = pPrevFrame->mvKeysUn[vQuadMatches[indigo].i1p];
            const cv::KeyPoint &kp4 = pPrevFrame->mvRightKeysUn[vQuadMatches[indigo].i1p];
            Vector3d xn3((kp3.pt.x-pPrevFrame->cam_->cx())/pPrevFrame->cam_->fx(),
                         (kp3.pt.y-pPrevFrame->cam_->cy())/pPrevFrame->cam_->fy(), 1.0 );

            Vector3d xn4((kp4.pt.x-pPrevFrame->right_cam_->cx())/pPrevFrame->right_cam_->fx(),
                         (kp4.pt.y-pPrevFrame->right_cam_->cy())/pPrevFrame->right_cam_->fy(), 1.0 );

            Eigen::Matrix<double,8,4> A;
            A.row(0) = xn1(0)*Tw2c1.row(2)-Tw2c1.row(0);
            A.row(1) = xn1(1)*Tw2c1.row(2)-Tw2c1.row(1);
            A.row(2) = xn2(0)*Tw2c1r.row(2)-Tw2c1r.row(0);
            A.row(3) = xn2(1)*Tw2c1r.row(2)-Tw2c1r.row(1);
            A.row(4) = xn3(0)*Tw2c2.row(2)-Tw2c2.row(0);
            A.row(5) = xn3(1)*Tw2c2.row(2)-Tw2c2.row(1);
            A.row(6) = xn4(0)*Tw2c2r.row(2)-Tw2c2r.row(0);
            A.row(7) = xn4(1)*Tw2c2r.row(2)-Tw2c2r.row(1);
            cv::Mat Aprime, w,u,vt;
            cv::eigen2cv(A, Aprime);
            cv::SVD::compute(Aprime,w,u,vt,cv::SVD::MODIFY_A| cv::SVD::FULL_UV);
            cv::Mat x3D = vt.row(3).t();
            if(x3D.at<double>(3)==0)
                continue;

            // Euclidean coordinates
            x3D = x3D.rowRange(0,3)/x3D.at<double>(3);
            Eigen::Vector3d x3Dt;
            cv::cv2eigen (x3D, x3Dt);

            //Check triangulation in front of cameras
            float z1 = Rw2c1.row(2)*x3Dt+ twinc1(2);
            if(z1<=0)
                continue;
            float z2 = Rw2c2.row(2)*x3Dt+twinc2(2);
            if(z2<=0)
                continue;

            //Check reprojection error in first keyframe
            float sigmaSquare1 = pKFcur->GetSigma2(kp1.octave);
            float x1 = Rw2c1.row(0)*x3Dt+twinc1(0);
            float y1 = Rw2c1.row(1)*x3Dt+twinc1(1);
            float invz1 = 1.0/z1;
            float u1 = pKFcur->cam_->fx()*x1*invz1+pKFcur->cam_->cx();
            float v1 = pKFcur->cam_->fy()*y1*invz1+pKFcur->cam_->cy();
            float errX1 = u1 - kp1.pt.x;
            float errY1 = v1 - kp1.pt.y;
            if((errX1*errX1+errY1*errY1)>5.991*sigmaSquare1)
                continue;

            //Check reprojection error in second frame
            float sigmaSquare2 = pPrevFrame->GetSigma2(kp3.octave);
            float x2 = Rw2c2.row(0)*x3Dt+twinc2(0);
            float y2 = Rw2c2.row(1)*x3Dt+twinc2(1);
            float invz2 = 1.0/z2;
            float u2 = pPrevFrame->cam_->fx()*x2*invz2+pPrevFrame->cam_->cx();
            float v2 = pPrevFrame->cam_->fy()*y2*invz2+pPrevFrame->cam_->cy();
            float errX2 = u2 - kp3.pt.x;
            float errY2 = v2 - kp3.pt.y;
            if((errX2*errX2+errY2*errY2)>5.991*sigmaSquare2)
                continue;

            //Check scale consistency
            Eigen::Vector3d normal1 = x3Dt-Ow1;
            float dist1 = normal1.norm();

            Eigen::Vector3d normal2 = x3Dt-Ow2;
            float dist2 = normal2.norm();

            if(dist1==0 || dist2==0)
                continue;

            float ratioDist = dist1/dist2;
            if(ratioDist*ratioFactor<1.f || ratioDist>ratioFactor)
                continue;

            // Triangulation is succesful
            MapPoint* pMP = new MapPoint(x3Dt,pKFcur,vQuadMatches[indigo].i1c, mpMap);

            pMP->AddObservation(pPrevFrame,vQuadMatches[indigo].i1p);

            pMP->AddObservation(pPrevFrame,vQuadMatches[indigo].i2p, false);
            pMP->AddObservation(pKFcur,vQuadMatches[indigo].i2c, false);
            pKFcur->AddMapPoint(pMP,vQuadMatches[indigo].i1c);
            pPrevFrame->AddMapPoint(pMP,vQuadMatches[indigo].i1p);

            pMP->ComputeDistinctiveDescriptors();

            pMP->UpdateNormalAndDepth();
            //Fill Current Frame structure
            mpCurrentFrame->mvpMapPoints[vQuadMatches[indigo].i1c] = pMP;

            mpMap->AddMapPoint(pMP);
            vMPGrid[posX][posY].push_back(vQuadMatches[indigo].i1c);
        }
    }

    // Bundle Adjustment
    char buffer[300];
    sprintf(buffer, "New Map created with %d points",mpMap->MapPointsInMap());
    cout<<buffer<<endl;

//    Optimizer::GlobalBundleAdjustemnt(mpMap,20);
    float medianDepth = pKFcur->ComputeSceneMedianDepth(2);

    if(medianDepth<0 || pKFcur->TrackedMapPoints()<50)
    {
        cout<<("Wrong initialization, reseting...");
        Reset();
        return;
    }  
    // Update Connections
    pPrevFrame->UpdateConnections();
    pKFcur->UpdateConnections();
    mpLocalMapper->InsertKeyFrame(pPrevFrame);

    mpLocalMapper->InsertKeyFrame(pKFcur);

    mpCurrentFrame->SetPose(pKFcur->GetPose());
    mpLastFrame = mpCurrentFrame;
    mnLastKeyFrameId=mpLastFrame->mnId;
    mpLastKeyFrame = pKFcur;

    mvpLocalMapPoints=mpMap->GetAllMapPoints();
    mpReferenceKF = pKFcur;

    mpMap->SetReferenceMapPoints(mvpLocalMapPoints);

//    mvpTemporalFrames.push_back(pPrevFrame);    
    if(pPrevFrame->mnId+1==pKFcur->mnId){
        pKFcur->SetPrevNextFrame( pPrevFrame);
        mvpTemporalFrames.push_back(pKFcur);
    }
    else{
        mvpTemporalFrames.pop_front();
        mvpTemporalFrames.push_back(pKFcur);
        assert(mvpTemporalFrames.size()==1);
    }
    //mpMapPublisher->SetCurrentCameraPose(pKFcur->GetPose());
    mState=WORKING;
}


void Tracking::CreateInitialMap(Eigen::Matrix3d Rcw, Eigen::Vector3d tcw, float upscale)
{
    // Set Frame Poses
    mpCurrentFrame->SetPose(Rcw, tcw);
    // Create KeyFrames
    KeyFrame* pKFini = (KeyFrame*) mvpTemporalFrames.back();
    KeyFrame* pKFcur = new KeyFrame(*mpCurrentFrame,mpMap,mpKeyFrameDB);

    pKFini->ComputeBoW();
    pKFcur->ComputeBoW();
    // Insert KFs in the map
    mpMap->AddKeyFrame(pKFini);
    mpMap->AddKeyFrame(pKFcur);
    assert(pKFcur->mnFrameId==1);

    // Create MapPoints and asscoiate to keyframes
    for(size_t i=0; i<mvIniMatches.size();i++)
    {
        if(mvIniMatches[i]<0)
            continue;

        //Create MapPoint.
        Eigen::Vector3d worldPos; worldPos<<mvIniP3D[i].x, mvIniP3D[i].y, mvIniP3D[i].z;

        MapPoint* pMP = new MapPoint(worldPos,pKFcur, mvIniMatches[i], mpMap);

        pKFini->AddMapPoint(pMP,i);
        pKFcur->AddMapPoint(pMP,mvIniMatches[i]);

        pMP->AddObservation(pKFini,i);

        pMP->ComputeDistinctiveDescriptors();
        pMP->UpdateNormalAndDepth();

        //Fill Current Frame structure
        mpCurrentFrame->mvpMapPoints[mvIniMatches[i]] = pMP;

        //Add to Map
        mpMap->AddMapPoint(pMP);
    }

    // Update Connections
    pKFini->UpdateConnections();
    pKFcur->UpdateConnections();

    // Bundle Adjustment
    char buffer[300];
    sprintf(buffer, "New Map created with %d points",mpMap->MapPointsInMap());
    cout<<buffer<<endl;

    Optimizer::GlobalBundleAdjustemnt(mpMap,20);

    // Set median depth to 1
    float medianDepth = pKFini->ComputeSceneMedianDepth(2);
    assert((pKFini->GetCameraCenter().norm())==0);
#if 1
    float invMedianDepth = 1.0f/medianDepth;
#else
    float invMedianDepth = 1.0f/(pKFcur->GetCameraCenter().norm());
#endif
    if(upscale!=0.f)
        invMedianDepth=upscale;

    if(medianDepth<0 || pKFcur->TrackedMapPoints()<100)
    {
        cout<<("Wrong initialization, reseting...");
        Reset();
        return;
    }

    // Scale initial baseline
    Sophus::SE3d Tc2w = pKFcur->GetPose();
    Tc2w.translation()*=invMedianDepth;
    pKFcur->SetPose(Tc2w);

    // Scale points
    vector<MapPoint*> vpAllMapPoints = pKFini->GetMapPointMatches();
    for(size_t iMP=0; iMP<vpAllMapPoints.size(); iMP++)
    {
        if(vpAllMapPoints[iMP])
        {
            MapPoint* pMP = vpAllMapPoints[iMP];
            pMP->SetWorldPos(pMP->GetWorldPos()*invMedianDepth);
            pMP->UpdateNormalAndDepth();
        }
    }

    mpLocalMapper->InsertKeyFrame(pKFini);
    mpLocalMapper->InsertKeyFrame(pKFcur);

    mpCurrentFrame->SetPose(pKFcur->GetPose());
    mpLastFrame = mpCurrentFrame;
    mnLastKeyFrameId=mpLastFrame->mnId;
    mpLastKeyFrame = pKFcur;

//    mvpLocalKeyFrames.push_back(pKFcur);
//    mvpLocalKeyFrames.push_back(pKFini);
    mvpLocalMapPoints=mpMap->GetAllMapPoints();
    mpReferenceKF = pKFcur;

    mpMap->SetReferenceMapPoints(mvpLocalMapPoints);
    if(pKFini->mnId+1==pKFcur->mnId){
        pKFcur->SetPrevNextFrame( pKFini);
        mvpTemporalFrames.push_back(pKFcur);
    }
    else{
        mvpTemporalFrames.pop_front();
        mvpTemporalFrames.push_back(pKFcur);
        assert(mvpTemporalFrames.size()==1);
    }
    //mpMapPublisher->SetCurrentCameraPose(pKFcur->GetPose());

    mState=WORKING;
}

// extend trails/ associate map points from previous frame to current frame
bool Tracking::TrackPreviousFrame(const Sophus::SE3d & Tcp,
                                  const std::vector<p_match>&vQuadMatches)
{
    vector<MapPoint*> vpMapPointMatches( mpCurrentFrame->mvpMapPoints.size(),static_cast<MapPoint*>(NULL));
    int nmatches=0;
    for (unsigned int jade=0; jade< vQuadMatches.size() ;++jade)
    {        
        MapPoint* pMP1 =  mpLastFrame->mvpMapPoints[vQuadMatches[jade].i1p];
        if(!pMP1 || pMP1->isBad())
            continue;        
        vpMapPointMatches[vQuadMatches[jade].i1c] = pMP1;
        ++nmatches;
    }

    if(nmatches<10)
    {
        vpMapPointMatches=vector<MapPoint*>(mpCurrentFrame->mvpMapPoints.size(),static_cast<MapPoint*>(NULL));
        nmatches=0;
    }

    mpCurrentFrame->SetPose(Tcp*mpLastFrame->mTcw);

    mpCurrentFrame->mvpMapPoints=vpMapPointMatches;
    return nmatches>=10;
}
bool Tracking::TrackPreviousFrame()
{
    ORBmatcher matcher(0.9,true);
    vector<MapPoint*> vpMapPointMatches;

    // Search first points at coarse scale levels to get a rough initial estimate
    int minOctave = 0;
    int maxOctave = mpCurrentFrame->GetScaleLevels()-1;
    if(mpMap->KeyFramesInMap()>5)
        minOctave = maxOctave/2+1;

    int nmatches = matcher.WindowSearch(*mpLastFrame,*mpCurrentFrame,200,vpMapPointMatches,minOctave);

    // If not enough matches, search again without scale constraint
    if(nmatches<10)
    {
        nmatches = matcher.WindowSearch(*mpLastFrame,*mpCurrentFrame,100,vpMapPointMatches,0);
        if(nmatches<10)
        {
            vpMapPointMatches=vector<MapPoint*>(mpCurrentFrame->mvpMapPoints.size(),static_cast<MapPoint*>(NULL));
            nmatches=0;
        }
    }

    mpCurrentFrame->SetPose(mpLastFrame->mTcw);
    mpCurrentFrame->mvpMapPoints=vpMapPointMatches;

    // If enough correspondeces, optimize pose and project points from previous frame to search more correspondences
    if(nmatches>=10)
    {
        // Optimize pose with correspondences
        Optimizer::PoseOptimization(mpCurrentFrame.get());

        for(size_t i =0; i<mpCurrentFrame->mvbOutlier.size(); i++)
            if(mpCurrentFrame->mvbOutlier[i])
            {
                mpCurrentFrame->mvpMapPoints[i]=NULL;
                mpCurrentFrame->mvbOutlier[i]=false;
                nmatches--;
            }

        // Search by projection with the estimated pose
        nmatches += matcher.SearchByProjection(*mpLastFrame,*mpCurrentFrame,15,vpMapPointMatches);
    }
    else //Last opportunity
        nmatches = matcher.SearchByProjection(*mpLastFrame,*mpCurrentFrame,50,vpMapPointMatches);


    mpCurrentFrame->mvpMapPoints=vpMapPointMatches;

    if(nmatches<10)
        return false;

    // Optimize pose again with all correspondences
    Optimizer::PoseOptimization(mpCurrentFrame.get());

    // Discard outliers
    for(size_t i =0; i<mpCurrentFrame->mvbOutlier.size(); i++)
        if(mpCurrentFrame->mvbOutlier[i])
        {
            mpCurrentFrame->mvpMapPoints[i]=NULL;
            mpCurrentFrame->mvbOutlier[i]=false;
            nmatches--;
        }

    return nmatches>=10;
}

bool Tracking::TrackWithMotionModel()
{
    ORBmatcher matcher(0.9,true);
    vector<MapPoint*> vpMapPointMatches;

    // Compute current pose by motion model
    mpCurrentFrame->SetPose( mVelocity*mpLastFrame->mTcw);

    fill(mpCurrentFrame->mvpMapPoints.begin(),mpCurrentFrame->mvpMapPoints.end(),static_cast<MapPoint*>(NULL));

    // Project points seen in previous frame
    int nmatches = matcher.SearchByProjection(*mpCurrentFrame,*mpLastFrame,15);

    if(nmatches<20)
       return false;

    // Optimize pose with all correspondences
    Optimizer::PoseOptimization(mpCurrentFrame.get());

    // Discard outliers
    for(size_t i =0; i<mpCurrentFrame->mvpMapPoints.size(); i++)
    {
        if(mpCurrentFrame->mvpMapPoints[i])
        {
            if(mpCurrentFrame->mvbOutlier[i])
            {
                mpCurrentFrame->mvpMapPoints[i]=NULL;
                mpCurrentFrame->mvbOutlier[i]=false;
                nmatches--;
            }
        }
    }

    return nmatches>=10;
}

bool Tracking::TrackLocalMap()
{
    // Tracking from previous frame or relocalisation was succesfull and we have an estimation
    // of the camera pose and some map points tracked in the frame.
    // Update Local Map and Track

    // Update Local Map
    UpdateReference();

    // Search Local MapPoints
    SearchReferencePointsInFrustum();

    // Optimize Pose
    mnMatchesInliers = Optimizer::PoseOptimization(mpCurrentFrame.get());

    // Update MapPoints Statistics
    for(size_t i=0; i<mpCurrentFrame->mvpMapPoints.size(); i++)
        if(mpCurrentFrame->mvpMapPoints[i])
        {
            if(!mpCurrentFrame->mvbOutlier[i])
                mpCurrentFrame->mvpMapPoints[i]->IncreaseFound();
        }

    // Decide if the tracking was succesful
    // More restrictive if there was a relocalization recently
    if(mpCurrentFrame->mnId<mnLastRelocFrameId+mMaxFrames && mnMatchesInliers<50)
        return false;

    if(mnMatchesInliers<30)
        return false;
    else
        return true;
}
void Tracking::setupG2o(G2oCameraParameters * g2o_cam,
                        G2oCameraParameters*g2o_cam_right,
           G2oIMUParameters * g2o_imu,
           g2o::SparseOptimizer * optimizer) const
{
    if(mbUseIMUData)
    {   //block solver_6_3 has errors in eigen in optimizing stereo+IMU observations,
        // About choosing block solvers, g2o has some examples
        //in g2o/examples/ba_anchored_inverse_depth/ba_anchored_inverse_depth_demo.cpp,
        // schur trick case: BlockSolver_6_3 and marginalizing 3D point vertices
        // otherwise: BlockSolverX and no marginalization
        // in g2o/examples/ba/ba_demo.cpp, BlockSolver_6_3 and marginalizing 3D point vertices
        //in g2o/examples/icp/gicp_sba_demo.cpp, BlockSolverX are used together with marginalizing 3D point vertices
        // in g2o/examples/bal/bal_example.cpp, g2o::BlockSolver< g2o::BlockSolverTraits<9, 3> > is used with marginalization
        // About linear solvers, g2o provides LinearSolverDense, LinearSolverCSparse, LinearSolverCholmod, and LinearSolverPCG

        optimizer->setVerbose(false);
        g2o::BlockSolverX::LinearSolverType * linearSolver=
                new g2o::LinearSolverCholmod<g2o::BlockSolverX::PoseMatrixType>();
        g2o::BlockSolverX * block_solver =
                new g2o::BlockSolverX(linearSolver);
        g2o::OptimizationAlgorithmLevenberg* lm =
                new g2o::OptimizationAlgorithmLevenberg(block_solver);

        lm->setMaxTrialsAfterFailure(5);
        optimizer->setAlgorithm(lm);
    }else{
        optimizer->setVerbose(false);
        typename g2o::BlockSolver_6_3::LinearSolverType * linearSolver
                = new g2o::LinearSolverCSparse<g2o::BlockSolver_6_3::PoseMatrixType>();

        g2o::BlockSolver_6_3 * block_solver
                = new g2o::BlockSolver_6_3(linearSolver);
        g2o::OptimizationAlgorithmLevenberg * lm
                = new g2o::OptimizationAlgorithmLevenberg(block_solver);
        lm->setMaxTrialsAfterFailure(5);
        optimizer->setAlgorithm(lm);
    }
    if (!optimizer->addParameter(g2o_cam))
    {
        assert(false);
    }
#ifndef MONO
    assert(g2o_cam_right!=NULL);

        if (!optimizer->addParameter(g2o_cam_right))
        {
            assert(false);
        }

#endif
    if(mbUseIMUData&&g2o_imu!=NULL){
        if (!optimizer->addParameter(g2o_imu))
        {
            assert(false);
        }
    }
}

G2oVertexSE3* Tracking::addPoseToG2o(const SE3d & T_me_from_w,
               int pose_id,
               bool fixed,
               g2o::SparseOptimizer * optimizer,
               const Sophus::SE3d* first_estimate) const
{
  G2oVertexSE3 * v_se3 = new G2oVertexSE3();

  v_se3->setId(pose_id);
  v_se3->setEstimate(T_me_from_w);
  v_se3->setFixed(fixed);
  if(first_estimate!=NULL)
      v_se3->setFirstEstimate(*first_estimate);
  optimizer->addVertex(v_se3);
  return v_se3;
}
G2oVertexSpeedBias* Tracking::addSpeedBiasToG2o(const Matrix<double, 9,1> & vinw_bias,
               int sb_id,
               bool fixed,
               g2o::SparseOptimizer * optimizer,
               const Eigen::Matrix<double, 9,1> * first_estimate) const
{
  G2oVertexSpeedBias * v_sb = new G2oVertexSpeedBias();
  v_sb->setId(sb_id);
  v_sb->setEstimate(vinw_bias);
  v_sb->setFixed(fixed);
  // v_sb->setMarginalized(true); // this line causes error
  if(first_estimate!=NULL)
      v_sb->setFirstEstimate(*first_estimate);
  optimizer->addVertex(v_sb);
  return v_sb;
}

//N.B. g2o refuses negative indices
// copy frames in temporal and spatial window and the current frame to g2o
// return next unused possible vertex id
size_t Tracking::copyAllPosesToG2o(g2o::SparseOptimizer * optimizer)
{
    size_t max_vertex_id=0;
    size_t vertexid= 0;
    for (vector <KeyFrame*>::const_iterator  it_win = mvpLocalKeyFrames.begin(), it_end_win=mvpLocalKeyFrames.end();
         it_win!=it_end_win; ++it_win)
    {
      SE3d Tw2cref= (*it_win)->GetPose(); //transform from world to reference camera (i.e. left camera) frame
      vertexid= (*it_win)->mnId*MAGIC2;
      if(max_vertex_id< vertexid)
          max_vertex_id=vertexid;
      if((*it_win)->mbFixedLinearizationPoint){
          SE3d Tw2cref_fe=(*it_win)->mTcw_first_estimate;
          (*it_win)->v_kf_ = addPoseToG2o(Tw2cref, vertexid, false, optimizer, &Tw2cref_fe);
      }
      else
      {
          (*it_win)->v_kf_ = addPoseToG2o(Tw2cref, vertexid, false, optimizer);       
      }
   
  }
  for (deque<Frame*>::const_iterator  it_win = mvpTemporalFrames.begin(), it_end_win=mvpTemporalFrames.end();
       it_win!=it_end_win; ++it_win)
  {  
    SE3d Tw2cref= (*it_win)->GetPose();  //transform from world to reference camera (i.e. left camera) frame

    vertexid= (*it_win)->mnId*MAGIC2;
    if(max_vertex_id< vertexid)
        max_vertex_id=vertexid;
    if((*it_win)->mbFixedLinearizationPoint){
        SE3d Tw2cref_fe=(*it_win)->mTcw_first_estimate;
         (*it_win)->v_kf_ = addPoseToG2o(Tw2cref, vertexid, false, optimizer, &Tw2cref_fe);
        if(mbUseIMUData)
        {
            (*it_win)->v_sb_=addSpeedBiasToG2o((*it_win)->speed_bias, vertexid+1, false,
                              optimizer, &(*it_win)->speed_bias_first_estimate);
        }
    }
    else
    {
         (*it_win)->v_kf_ = addPoseToG2o(Tw2cref, vertexid, false, optimizer);
        if(mbUseIMUData)
        {
            (*it_win)->v_sb_=addSpeedBiasToG2o((*it_win)->speed_bias, vertexid+1, false,
                              optimizer);
        }
    }
  }

   // previous frame
  if(mpLastFrame.mnId!=1){//is not second keyframe
      assert(mvpTemporalFrames.back()->mnId!= mpLastFrame->mnId);
      SE3d Tw2cref= mpLastFrame->mTcw; //transform from world to reference camera (i.e. left camera) frame
      vertexid= mpLastFrame.mnId*MAGIC2;
      if(max_vertex_id< vertexid)
          max_vertex_id=vertexid;
      assert(!mpLastFrame.mbFixedLinearizationPoint);
      mpLastFrame->v_kf_=addPoseToG2o(Tw2cref, vertexid, false, optimizer);
      if(mbUseIMUData)
      {
          mpLastFrame->v_sb_=addSpeedBiasToG2o(mpLastFrame->speed_bias, vertexid+1, false,
                                             optimizer);
      }
  }
  else
      assert(mvpTemporalFrames.back()->mnId== mpLastFrame->mnId);
  //current frame
  SE3d Tw2cref= mpCurrentFrame.mTcw; //transform from world to reference camera (i.e. left camera) frame
  vertexid= mpCurrentFrame->mnId*MAGIC2;
  if(max_vertex_id< vertexid)
      max_vertex_id=vertexid;
  assert(!mpCurrentFrame->mbFixedLinearizationPoint);
  mpCurrentFrame->v_kf_=addPoseToG2o(Tw2cref, vertexid, false, optimizer);
  if(mbUseIMUData)
  {
      mpCurrentFrame->v_sb_=addSpeedBiasToG2o(mpCurrentFrame->speed_bias, vertexid+1, false,
                                            optimizer);
  }

  return max_vertex_id+2;
}

inline G2oVertexPointXYZ* Tracking::addPointToG2o( MapPoint* pPoint,
                int g2o_point_id, bool fixed,
                g2o::SparseOptimizer * optimizer) const
{
  G2oVertexPointXYZ * v_point = new G2oVertexPointXYZ;
  v_point->setId(g2o_point_id);
  v_point->setFixed(fixed);
  v_point->setEstimate(pPoint->GetWorldPos());
  v_point->setMarginalized(true);//if true, Schur trick is used to marginalize this vertex during optimization
  if(pPoint->mbFixedLinearizationPoint)
      v_point->setFirstEstimate(pPoint->mWorldPos_first_estimate);
  optimizer->addVertex(v_point);
  return v_point;
}

static g2o::OptimizableGraph::Vertex*  GET_MAP_ELEM(const int & key,
                          const g2o::OptimizableGraph::VertexIDMap & m)
{
  g2o::OptimizableGraph::VertexIDMap::const_iterator it = m.find(key);
  assert(it!=m.end());
  return dynamic_cast<g2o::OptimizableGraph::Vertex*>(it->second);
}

G2oEdgeProjectXYZ2UV* Tracking::addObsToG2o(const Vector2d & obs,
              const Matrix2d & Lambda,
              int g2o_point_id,
              int pose_id,
              bool robustify,
              double huber_kernel_width,
              g2o::SparseOptimizer * optimizer, SE3d * pTs2c)
{
    G2oEdgeProjectXYZ2UV * e=NULL;
    if(pTs2c)
        e = new G2oExEdgeProjectXYZ2UV(*pTs2c);
    else
        e = new G2oEdgeProjectXYZ2UV();
    e->resize(2);
    g2o::OptimizableGraph::Vertex * point_vertex
            = GET_MAP_ELEM(g2o_point_id, optimizer->vertices());
    g2o::OptimizableGraph::Vertex * pose_vertex
            = GET_MAP_ELEM(pose_id, optimizer->vertices());

    assert(point_vertex!=NULL);
    assert(point_vertex->dimension()==3);
    e->vertices()[0] = point_vertex;

    assert(pose_vertex!=NULL);
    assert(pose_vertex->dimension()==6);
    e->vertices()[1] = pose_vertex;

    e->setMeasurement(obs);
    e->information() = Lambda;

    if (robustify)
    {
        g2o::RobustKernelHuber* rk = new g2o::RobustKernelHuber;
        e->setRobustKernel(rk);
        rk->setDelta(huber_kernel_width);
    }
    e->resizeParameters(1);
    bool param_status = e->setParameterId(0, pTs2c==NULL ? 0:1);
    assert(param_status);
    optimizer->addEdge(e);
    return e;
}
G2oEdgeProjectXYZ2UV* Tracking::addObsToG2o(const Vector2d & obs,
              const Matrix2d & Lambda,
              g2o::OptimizableGraph::Vertex * point_vertex,
              g2o::OptimizableGraph::Vertex * pose_vertex,
              bool robustify,
              double huber_kernel_width,
              g2o::SparseOptimizer * optimizer, SE3d * pTs2c)
{
    G2oEdgeProjectXYZ2UV * e=NULL;
    if(pTs2c)
        e = new G2oExEdgeProjectXYZ2UV(*pTs2c);
    else
        e = new G2oEdgeProjectXYZ2UV();
    e->resize(2);

    assert(point_vertex->dimension()==3);
    e->vertices()[0] = point_vertex;

    assert(pose_vertex->dimension()==6);
    e->vertices()[1] = pose_vertex;

    e->setMeasurement(obs);
    e->information() = Lambda;

    if (robustify)
    {
        g2o::RobustKernelHuber* rk = new g2o::RobustKernelHuber;
        e->setRobustKernel(rk);
        rk->setDelta(huber_kernel_width);
    }
    e->resizeParameters(1);
    bool param_status = e->setParameterId(0, pTs2c==NULL ? 0:1);
    assert(param_status);
    optimizer->addEdge(e);
    return e;
}
/// Temporary container to hold the g2o edge with reference to frame and point.
struct EdgeContainerSE3d
{
  G2oEdgeProjectXYZ2UV*     edge;
  Frame*          frame;
  size_t id_; // id in frame feature vector
  bool            is_deleted;
  EdgeContainerSE3d(G2oEdgeProjectXYZ2UV* e, Frame* frame, size_t id) :
    edge(e), frame(frame),id_(id),is_deleted(false)
  {}
};
// given the local temporal frames (left and right frame in stereo case), the local keyframes,
// and the current frame, optimize poses of left frames of temporal frames and local keyframes,
// and positions of points observed in both windows.
// we do not need to check pMP->isBad() or pKF->isBad,
// because localMapper will not setbadflag for points and keyframes in double window
//note this function only works for stereo case
int Tracking::LocalOptimizeSVO()
{
    g2o::SparseOptimizer optimizer;
    ScaViSLAM::G2oCameraParameters  * g2o_cam
            = new G2oCameraParameters(Vector2d(cam_->cx(), cam_->cy()),
                                      Vector2d(cam_->fx(), cam_->fy()));
    g2o_cam->setId(0);
    ScaViSLAM::G2oCameraParameters  * g2o_cam_right
            = new G2oCameraParameters(Vector2d(right_cam_->cx(), right_cam_->cy()),
                                      Vector2d(right_cam_->fx(), right_cam_->fy()));
    g2o_cam_right->setId(1);
    G2oIMUParameters  * g2o_imu  = new  G2oIMUParameters(imu_);
    g2o_imu->setId(2);
    setupG2o(g2o_cam, g2o_cam_right, g2o_imu, &optimizer);
    std::map<unsigned int, Frame*> vertexid_2_frame;
    std::map<unsigned int, MapPoint*> vertexid_2_mappoint;
    size_t nOffset=copyAllPosesToG2o(&optimizer, vertexid_2_frame);
#ifdef HUAI_DEBUG
    map<MapPoint*, size_t> histogram;
#endif    
    // add constraint by IMU observations for frames in the temporal window and the current frame
    if(mbUseIMUData){
        for (std::deque<Frame*>::const_iterator  it_win = mvpTemporalFrames.begin(), it_end_win=mvpTemporalFrames.end();
             it_win!=it_end_win; ++it_win)
        {
            if((*it_win)->cam_id_!=0 ) continue;
            G2oEdgeIMUConstraint * e = new G2oEdgeIMUConstraint();
            e->setParameterId(0,2);
            e->resize(4);
            if((*it_win)->next_frame ==NULL) // the last frame in the temporal window
            {
                e->vertices()[0]
                        = (*it_win)->v_kf_;
                e->vertices()[1]
                        = (*it_win)->v_sb_;
                e->vertices()[2]
                        = mpCurrentFrame->v_kf_;
                e->vertices()[3]
                        = mpCurrentFrame->v_sb_;
                e->time_frames[0]= (*it_win)->mTimeStamp;
                e->time_frames[1]= mpCurrentFrame->mTimeStamp;
                e->setMeasurement(mpCurrentFrame->imu_observ);
            }
            else{
                assert((*it_win)->next_frame== (*(it_win+2)));
                e->vertices()[0]
                        = (*it_win)->v_kf_;
                e->vertices()[1]
                        = (*it_win)->v_sb_;
                e->vertices()[2]
                        = (*it_win)->next_frame->v_kf_;
                e->vertices()[3]
                        = (*it_win)->next_frame->v_sb_;
                e->time_frames[0]= (*it_win)->mTimeStamp;
                e->time_frames[1]= (*it_win)->next_frame->mTimeStamp;
                e->setMeasurement((*it_win)->next_frame->imu_observ);
            }
            e->calcAndSetInformation(*g2o_imu);
            optimizer.addEdge(e);
        }
    }

    // Now go throug all the points and add a measurement. Add a fixed neighbour
    // Keyframe if it is not in the set of core kfs
    // TODO: check that mvpLocalMapPoints contains all mappoints observed in double window

    int nInitialCorrespondences=0; //how many map points are tracked in current frame

    list<EdgeContainerSE3d> edges;

    const double delta2 = 5.991;
    const double delta = sqrt(delta2);

    size_t n_mps = 0;
    size_t n_fix_kfs=0;
    list<KeyFrame*> neib_kfs;
    for(auto it_pt = mvpLocalMapPoints.begin(), ite=mvpLocalMapPoints.end(); it_pt!=ite; ++it_pt)
    {
        // Create point vertex
        MapPoint* pMP = *it_pt;
        if(pMP->isBad()) continue;
        G2oVertexPointXYZ*v_pt= addPointToG2o( pMP, pMP->mnId + nOffset, false, &optimizer, vertexid_2_mappoint);
        pMP->v_pt_= v_pt;
        ++n_mps;
        // Add edges
        std::map<KeyFrame*, size_t> obs_copy= (*it_pt)->GetObservations();
        std::map<KeyFrame*, size_t>::const_iterator mit_obs=obs_copy.begin();
        while(mit_obs!=obs_copy.end())
        {
            KeyFrame * pKF= mit_obs->first;
            if(pKF->v_kf_ == NULL)
            {
                // frame does not have a vertex yet -> it belongs to the neib kfs and
                // is fixed. create one:
                ++n_fix_kfs;
                SE3d Tw2cref= pKF->GetPose(); //transform from world to reference camera (i.e. left camera) frame
                size_t vertexid= pKF->mnId*MAGIC2;

                if( pKF->mbFixedLinearizationPoint){
                    SE3d Tw2cref_fe= pKF->mTcw_first_estimate;
                    pKF->v_kf_=addPoseToG2o(Tw2cref, vertexid, true, &optimizer, &Tw2cref_fe);
                }
                else
                {
                    pKF->v_kf_=addPoseToG2o(Tw2cref, vertexid, true, &optimizer);
                }
                neib_kfs.push_back(pKF);
            }
            // create edge
            Eigen::Matrix<double,2,1> obs;
            cv::KeyPoint kpUn = pKF->mvKeysUn[mit_obs->second];
            obs << kpUn.pt.x, kpUn.pt.y;
            const float invSigma2 = pKF->GetInvSigma2(kpUn.octave);

            G2oEdgeProjectXYZ2UV* porter=addObsToG2o(obs, Eigen::Matrix2d::Identity()*invSigma2,
                                                     v_pt, pKF->v_kf_, true, delta, &optimizer);
            histogram[pMP]++;
            edges.push_back(EdgeContainerSE3d(porter, pKF, mit_obs->second));
            ++mit_obs;
        }
    }

    // add points' observations for each non keyframe in the temporal window and the current frame
    // add observations of frames in the temporal window
    Frame * pLastRefFrame=NULL;
    for(deque<Frame*>::const_iterator qIt= mvpTemporalFrames.begin(), qItEnd=mvpTemporalFrames.end();
        qIt!=qItEnd; ++qIt)
    {
        if((*qIt)->isKeyFrame())
        {
            assert((*qIt)->cam_id_==0);
            pLastRefFrame= *qIt;
            continue; //keyframe observations are already added for mappoints
        }
        vector<MapPoint*> vpMPs = (*qIt)->GetMapPointMatches();
        size_t jim=0;
        if((*qIt)->cam_id_==0){
            pLastRefFrame= *qIt;
            for(vector<MapPoint*>::const_iterator itMP=vpMPs.begin(), itEndMP=vpMPs.end();
                itMP!=itEndMP; ++itMP, ++jim)
            {
                MapPoint* pMP = *itMP;
                if(pMP && (!pMP->isBad()) && pMP->v_pt_!=NULL)
                    // if valid pointer, and not marked as bad by optimizer,
                    // and has been added to optimizer, i.e., in mvpLocalMapPoints and is used as track reference for current frame
                {
                    Eigen::Matrix<double,2,1> obs;
                    cv::KeyPoint kpUn = pLastRefFrame->mvKeysUn[jim];
                    obs << kpUn.pt.x, kpUn.pt.y;
                    const float invSigma2 = pLastRefFrame->GetInvSigma2(kpUn.octave);
                    G2oEdgeProjectXYZ2UV* porter=addObsToG2o(obs, Eigen::Matrix2d::Identity()*invSigma2,
                                                             pMP->v_pt_, pLastRefFrame->v_kf_, true, delta, &optimizer);
                    histogram[pMP]++;
                    edges.push_back(EdgeContainerSE3d(porter, pLastRefFrame, jim));
                }
            }
        }
        else
        {
            assert((*qIt)->mnId-1==pLastRefFrame->mnId);
            for(vector<MapPoint*>::const_iterator itMP=vpMPs.begin(), itEndMP=vpMPs.end();
                itMP!=itEndMP; ++itMP, ++jim)
            {
                MapPoint* pMP = *itMP;
                if(pMP && !pMP->isBad() && pMP->v_pt_!=NULL)
                    // if valid pointer, and not marked as bad by optimizer, and has been added to optimizer
                {
                    Eigen::Matrix<double,2,1> obs;
                    cv::KeyPoint kpUn = (*qIt)->mvKeysUn[jim];
                    obs << kpUn.pt.x, kpUn.pt.y;
                    const float invSigma2 = (*qIt)->GetInvSigma2(kpUn.octave);
                    G2oEdgeProjectXYZ2UV* porter=addObsToG2o(obs, Eigen::Matrix2d::Identity()*invSigma2, pMP->v_pt_,
                                                             pLastRefFrame->v_kf_, true, delta, &optimizer, &mTl2r );
                    histogram[pMP]++;
                    edges.push_back(EdgeContainerSE3d(porter, *qIt, jim));
                }
            }
        }
    }
    assert(mpCurrentFrame->cam_id_==0);
    pLastRefFrame= mpCurrentFrame.get();

    vector<MapPoint*> vpMPs = mpCurrentFrame->GetMapPointMatches();
    size_t jim=0;
    for(vector<MapPoint*>::const_iterator itMP=vpMPs.begin(), itEndMP=vpMPs.end();
        itMP!=itEndMP; ++itMP, ++jim)
    {
        MapPoint* pMP = *itMP;
        if(pMP && (!(pMP->isBad())))// if valid pointer, and not marked as bad by optimizer
        {
            Eigen::Matrix<double,2,1> obs;
            cv::KeyPoint kpUn = pLastRefFrame->mvKeysUn[jim];
            obs << kpUn.pt.x, kpUn.pt.y;
            const float invSigma2 = pLastRefFrame->GetInvSigma2(kpUn.octave);
            G2oEdgeProjectXYZ2UV* porter=addObsToG2o(obs, Eigen::Matrix2d::Identity()*invSigma2,
                                                     pMP->v_pt_, pLastRefFrame->v_kf_, true, delta, &optimizer);
            histogram[pMP]++;
            edges.push_back(EdgeContainerSE3d(porter, pLastRefFrame, jim));
            ++nInitialCorrespondences;
        }
    }
    vpMPs = mpCurrentRightFrame->GetMapPointMatches();
    jim=0;
    for(vector<MapPoint*>::const_iterator itMP=vpMPs.begin(), itEndMP=vpMPs.end();
        itMP!=itEndMP; ++itMP, ++jim)
    {
        MapPoint* pMP = *itMP;
        if(pMP && (!(pMP->isBad())))
            // if valid pointer, and not marked as bad by optimizer
        {
            Eigen::Matrix<double,2,1> obs;
            cv::KeyPoint kpUn = mpCurrentRightFrame->mvKeysUn[jim];
            obs << kpUn.pt.x, kpUn.pt.y;
            const float invSigma2 = mpCurrentRightFrame->GetInvSigma2(kpUn.octave);
            Eigen::Matrix2d infoMat=Eigen::Matrix2d::Identity()*invSigma2;

            // set right edge
            G2oEdgeProjectXYZ2UV* porter=addObsToG2o(obs, infoMat, pMP->v_pt_,
                                                     pLastRefFrame->v_kf_, true, delta, &optimizer, &mTl2r );
            histogram[pMP]++;
            edges.push_back(EdgeContainerSE3d(porter, mpCurrentRightFrame.get(), jim));

        }
    }
#if 1
    for(auto it= histogram.begin(), ite=histogram.end(); it!=ite; ++it)
    {
        MapPoint *pMP=it->first;
        assert(it->second>=2);
    }
#endif
    optimizer.initializeOptimization();

    //    cout << "Performing structure-only BA:" << endl;
    g2o::StructureOnlySolver<3> structure_only_ba;
    g2o::OptimizableGraph::VertexContainer points;
    for (g2o::OptimizableGraph::VertexIDMap::const_iterator it = optimizer.vertices().begin(); it != optimizer.vertices().end(); ++it) {
        g2o::OptimizableGraph::Vertex* v = static_cast<g2o::OptimizableGraph::Vertex*>(it->second);
        if (v->dimension() == 3)
            points.push_back(v);
    }
    structure_only_ba.calc(points, 5);

    optimizer.optimize(5);

    int nBad=0;
    // Check inlier observations, this section draws inspiration from LocalBundleAdjustment and PoseOptimization by Raul
#ifdef MONO
    for(size_t i=0, iend=vpEdges.size(); i<iend;i++)
    {
        G2oEdgeProjectXYZ2UV* e = vpEdges[i];
        MapPoint* pMP = vpMapPointEdge[i];
        if(!e)
            continue;
        if(pMP->isBad())
            continue;
        Frame* pFi = vnIndexEdge[i].first;
        int idx= vnIndexEdge[i].second;
        if(pFi==mpCurrentFrame.get())
        {
            if(pFi->mvbOutlier[idx]){
                const float invSigma2 = pFi->GetInvSigma2(0);
                e->setInformation(Eigen::Matrix2d::Identity()*invSigma2);
                e->computeError();
            }
            if(e->chi2()>delta2)
            {
                pFi->mvbOutlier[idx]=true;
                // e->setInformation(Eigen::Matrix2d::Identity()*1e-10);
                nBad++;
            }
            else
            {
                pFi->mvbOutlier[idx]=false;
            }
        }
        else{
            if(e->chi2()>delta2 || !e->isDepthPositive())
            {
                pFi->EraseMapPointMatch(idx);
                pMP->EraseObservation(pFi);
            }
        }
    }
#else   
    for(list<EdgeContainerSE3d>::iterator it = edges.begin(); it != edges.end(); ++it)
    {
        G2oEdgeProjectXYZ2UV* e = it->edge;
        if(e->chi2()>delta2)// || !e->isDepthPositive())
        {
            Frame* pFi = it->frame;
            cout<< it->id_<<" "<< pFi->mvpMapPoints.size()<<endl;
            MapPoint* pMP= pFi->GetMapPoint(it->id_);
            if(pMP==NULL || (pMP->isBad()))
                continue;

            if(pFi->isKeyFrame())
            {
                pFi->EraseMapPointMatch(it->id_);
                pMP->EraseObservation((KeyFrame*)pFi);             
            }
            else{
                pFi->EraseMapPointMatch(it->id_);
                if(pFi==mpCurrentFrame.get())
                    ++nBad;
            }
        }
    }
#endif    
    //update current frame and current right frame
    mpCurrentFrame->SetPose((static_cast<G2oVertexSE3*>(mpCurrentFrame->v_kf_))->estimate());
    mpCurrentFrame->v_kf_ = NULL;
    mpCurrentFrame->speed_bias = (static_cast<G2oVertexSpeedBias*>(mpCurrentFrame->v_sb_))->estimate();
    mpCurrentFrame->v_sb_=NULL;
    if(mpLocalMapper->isStopped() || mpLocalMapper->stopRequested())
        return nBad;
    // Update Keyframes
    for(deque<Frame*>::iterator it = mvpTemporalFrames.begin(); it != mvpTemporalFrames.end(); ++it)
    {
        if((*it)->v_kf_==NULL) continue;
        (*it)->SetPose((static_cast<G2oVertexSE3*>((*it)->v_kf_))->estimate());
        (*it)->v_kf_ = NULL;
        if((*it)->v_sb_!=NULL)
        {
            (*it)->speed_bias = (static_cast<G2oVertexSpeedBias*>((*it)->v_sb_))->estimate();
            (*it)->v_sb_=NULL;
        }
    }
    for(vector<KeyFrame*>::iterator it = mvpLocalKeyFrames.begin(); it != mvpLocalKeyFrames.end(); ++it)
    {
        (*it)->SetPose((static_cast<G2oVertexSE3*>((*it)->v_kf_))->estimate());
        (*it)->v_kf_ = NULL;
        assert((*it)->v_sb_==NULL);
    }
    for(list<KeyFrame*>::iterator it = neib_kfs.begin(); it != neib_kfs.end(); ++it)
        (*it)->v_kf_ = NULL;

    // Update Mappoints
    for(vector<MapPoint*>::iterator it = mvpLocalMapPoints.begin(); it != mvpLocalMapPoints.end(); ++it)
    {
        if(((*it)==NULL) || ((*it)->isBad())) continue;
        (*it)->SetWorldPos((static_cast<G2oVertexPointXYZ*>((*it)->v_pt_))->estimate());
        (*it)->v_pt_ = NULL;
    }
    return nBad;
    //N.B. g2o will delete _algorithm, _parameters, EdgeSet and VertexIDMap upon destruction of SparseOptimizer
}

// For all map points in the local map, add them
// A map point will be marked as having first estimate as soon as
// the last keyframe that observes it slips out of the spatial window
// return the number of good observations made by the current frame
// we do not need to check pKF->isBad, because localMapper will not setbadflag for keyframes in double window
int Tracking::LocalOptimize()
{
    g2o::SparseOptimizer optimizer;
    ScaViSLAM::G2oCameraParameters  * g2o_cam
            = new G2oCameraParameters(Vector2d(cam_->cx(), cam_->cy()),
                                      Vector2d(cam_->fx(), cam_->fy()));
    g2o_cam->setId(0);
    ScaViSLAM::G2oCameraParameters  * g2o_cam_right
            = new G2oCameraParameters(Vector2d(right_cam_->cx(), right_cam_->cy()),
                                      Vector2d(right_cam_->fx(), right_cam_->fy()));
    g2o_cam_right->setId(1);
    G2oIMUParameters  * g2o_imu  = new  G2oIMUParameters(imu_);
    g2o_imu->setId(2);
    setupG2o(g2o_cam, g2o_cam_right, g2o_imu, &optimizer);
 	size_t nOffset=copyAllPosesToG2o(&optimizer);
    // add constraint by IMU observations for frames in the temporal window and the current frame
    if(mbUseIMUData){
    for (std::deque<Frame*>::const_iterator  it_win = mvpTemporalFrames.begin(), it_end_win=mvpTemporalFrames.end();
         it_win!=it_end_win; ++it_win)
    {
        G2oEdgeIMUConstraint * e = new G2oEdgeIMUConstraint();
        e->setParameterId(0,2);
        e->resize(4);
        if((*it_win)->next_frame ==NULL) // the last frame in the temporal window
        {
            e->vertices()[0]
                    = (*it_win)->v_kf_;
            e->vertices()[1]
                    = (*it_win)->v_sb_;
            e->vertices()[2]
                    = mLastFrame->v_kf_;
            e->vertices()[3]
                    = mLastFrame->v_sb_;
            e->time_frames[0]= (*it_win)->mTimeStamp;
            e->time_frames[1]= mpLastFrame->mTimeStamp;
            e->setMeasurement(mpLastFrame->imu_observ);
        }
        else{
            assert((*it_win)->next_frame== (*(it_win+1)));
            e->vertices()[0]
                    = (*it_win)->v_kf_;
            e->vertices()[1]
                    = (*it_win)->v_sb_;
            e->vertices()[2]
                    = (*it_win)->next_frame->v_kf_;
            e->vertices()[3]
                    = (*it_win)->next_frame->v_sb_;
            e->time_frames[0]= (*it_win)->mTimeStamp;
            e->time_frames[1]= (*it_win)->next_frame->mTimeStamp;
            e->setMeasurement((*it_win)->next_frame->imu_observ);

        }
        e->calcAndSetInformation(*g2o_imu);
        optimizer.addEdge(e);
    }
    G2oEdgeIMUConstraint * e = new G2oEdgeIMUConstraint();
    e->setParameterId(0,2);
    e->resize(4);
    // the previous frame and current frame
    e->vertices()[0]
            = mpLastFrame->v_kf_;
    e->vertices()[1]
            = mpLastFrame->v_sb_;
    e->vertices()[2]
            = mpCurrentFrame->v_kf_;
    e->vertices()[3]
            = mpCurrentFrame->v_sb_;
    e->time_frames[0]= mpLastFrame->mTimeStamp;
    e->time_frames[1]= mpCurrentFrame->mTimeStamp;
    e->setMeasurement(mpCurrentFrame.imu_observ);

    e->calcAndSetInformation(*g2o_imu);
    optimizer.addEdge(e);
    }
    // Now go throug all the points and add observations. Add a fixed neighbour
     // Keyframe if it is not in the set of core kfs
    const double delta2 = 5.991;
    const double delta = sqrt(delta2);
     list<EdgeContainerSE3d> edges;
     size_t n_mps = 0;
     size_t n_fix_kfs=0;
     list<KeyFrame*> neib_kfs;
     for(auto it_pt = mvpLocalMapPoints.begin(), ite=mvpLocalMapPoints.end(); it_pt!=ite; ++it_pt)
     {
         // Create point vertex
         MapPoint* pMP = *it_pt;
         if(pMP->isBad()) { pMP=NULL; continue;}
         std::map<KeyFrame*, size_t> obs_copy= pMP->GetObservations();
         if(obs_copy.size()<2) { pMP=NULL; continue;}
         G2oVertexPointXYZ*v_pt= addPointToG2o(pMP, pMP->mnId + nOffset, false, &optimizer);
         pMP->v_pt_= v_pt;
         ++n_mps;
         // Add edges       
         std::map<KeyFrame*, size_t>::const_iterator mit_obs=obs_copy.begin();
         while(mit_obs!=obs_copy.end())
         {
             KeyFrame * pKF= mit_obs->first;
             if(pKF->v_kf_ == NULL)
             {
                 // frame does not have a vertex yet -> it belongs to the neib kfs and
                 // is fixed. create one:
                 ++n_fix_kfs;
                 SE3d Tw2cref= pKF->GetPose(); //transform from world to reference camera (i.e. left camera) frame
                 size_t vertexid= pKF->mnId*MAGIC2;

                 if( pKF->mbFixedLinearizationPoint){
                     SE3d Tw2cref_fe= pKF->mTcw_first_estimate;
                     pKF->v_kf_=addPoseToG2o(Tw2cref, vertexid, true, &optimizer, &Tw2cref_fe);
                 }
                 else
                 {
                     pKF->v_kf_=addPoseToG2o(Tw2cref, vertexid, true, &optimizer);
                 }
                 neib_kfs.push_back(pKF);
             }
             // create edge
             Eigen::Matrix<double,2,1> obs;
             cv::KeyPoint kpUn = pKF->mvKeysUn[mit_obs->second];
             obs << kpUn.pt.x, kpUn.pt.y;
             const float invSigma2 = pKF->GetInvSigma2(kpUn.octave);

             G2oEdgeProjectXYZ2UV* porter=addObsToG2o(obs, Eigen::Matrix2d::Identity()*invSigma2,
                                                      v_pt, pKF->v_kf_, true, delta, &optimizer);          
             edges.push_back(EdgeContainerSE3d(porter, pKF, mit_obs->second));
             ++mit_obs;
         }
     }

     // add points' observations for each non keyframe in the temporal window and the previous and current frame
     // add observations of frames in the temporal window  
     for(deque<Frame*>::const_iterator qIt= mvpTemporalFrames.begin(), qItEnd=mvpTemporalFrames.end();
         qIt!=qItEnd; ++qIt)
     {
         if((*qIt)->isKeyFrame())
         {
             continue; //keyframe observations are already added for mappoints
         }

         size_t jim=0;         
             for(vector<MapPoint*>::const_iterator itMP=(*qIt)->mvpMapPoints.begin(), itEndMP=(*qIt)->mvpMapPoints.end();
                 itMP!=itEndMP; ++itMP, ++jim)
             {
                 MapPoint* pMP = *itMP;
                 if(pMP && pMP->v_pt_!=NULL)
                     // if valid pointer, and has been added to optimizer
                 {
                     Eigen::Matrix<double,2,1> obs;
                     cv::KeyPoint kpUn = (*qIt)->mvKeysUn[jim];
                     obs << kpUn.pt.x, kpUn.pt.y;
                     const float invSigma2 = (*qIt)->GetInvSigma2(kpUn.octave);
                     G2oEdgeProjectXYZ2UV* porter=addObsToG2o(obs, Eigen::Matrix2d::Identity()*invSigma2, pMP->v_pt_,
                                                              (*qIt)->v_kf_, true, delta, &optimizer, &mTl2r );
                     edges.push_back(EdgeContainerSE3d(porter, *qIt, jim));
                 }
             }

     }

     size_t jim=0;
     for(vector<MapPoint*>::const_iterator itMP=mpLastFrame->mvpMapPoints.begin(), itEndMP=mpLastFrame->mvpMapPoints.end();
         itMP!=itEndMP; ++itMP, ++jim)
     {
         MapPoint* pMP = *itMP;
         if(pMP && pMP->v_pt_!=NULL)
         {
             Eigen::Matrix<double,2,1> obs;
             cv::KeyPoint kpUn = mpLastFrame->mvKeysUn[jim];
             obs << kpUn.pt.x, kpUn.pt.y;
             const float invSigma2 = mpLastFrame->GetInvSigma2(kpUn.octave);
             G2oEdgeProjectXYZ2UV* porter=addObsToG2o(obs, Eigen::Matrix2d::Identity()*invSigma2,
                                                      pMP->v_pt_, mpLastFrame->v_kf_, true, delta, &optimizer);

             edges.push_back(EdgeContainerSE3d(porter, mpLastFrame, jim));
         }
     }
     jim=0;
     for(vector<MapPoint*>::const_iterator itMP=mpCurrentFrame->mvpMapPoints.begin(), itEndMP=mpCurrentFrame->mvpMapPoints.end();
         itMP!=itEndMP; ++itMP, ++jim)
     {
         MapPoint* pMP = *itMP;
         if(pMP && pMP->v_pt_!=NULL)
         {
             Eigen::Matrix<double,2,1> obs;
             cv::KeyPoint kpUn = mpCurrentFrame->mvKeysUn[jim];
             obs << kpUn.pt.x, kpUn.pt.y;
             const float invSigma2 = mpCurrentFrame->GetInvSigma2(kpUn.octave);
             G2oEdgeProjectXYZ2UV* porter=addObsToG2o(obs, Eigen::Matrix2d::Identity()*invSigma2,
                                                      pMP->v_pt_, mpCurrentFrame->v_kf_, true, delta, &optimizer);

             edges.push_back(EdgeContainerSE3d(porter, mpCurrentFrame, jim));
             ++nInitialCorrespondences;
         }
     }


   // add points from the local map, i.e., the double window
    const size_t num_mps= mvpLocalMapPoints.size();
    const unsigned long nFrontId= mvpTemporalFrames.front()->mnId;

    for(size_t i=0; i<num_mps; ++i)
    {
        MapPoint* pMP = mvpLocalMapPoints[i];
        if(pMP->mnObservationsInDoubleWindow>=2)
        {
//            if(pMP->mpRefKF->mnId< nFrontId)
//            addPointToG2o( pMP, pMP->mnId + nOffset, true, &optimizer, vertexid_2_mappoint);
//            else
            addPointToG2o( pMP, pMP->mnId + nOffset, false, &optimizer, vertexid_2_mappoint);
        }
    }

    // add points' observations for each frame in the double window and the current frame
    int nInitialCorrespondences=0; //how many map points are tracked in current frame
    vector<G2oEdgeProjectXYZ2UV*> vpEdges; // only include pose-point constraints, for both left and right observations
    vector<pair<Frame*, size_t> > vnIndexEdge;  // thalf length of vpEdges, each element is
    //the frame in which the mappoint is observed and the feature index in that frame,
    // because left and right observations have the same frame and idx
    int nExpectedSize= num_mps*8;
    vpEdges.reserve(nExpectedSize);
    vnIndexEdge.reserve(nExpectedSize/2);

    vector<MapPoint*> vpMapPointEdge;  // the same length as vpIndexEdge, each element is the map point an edge is involved with
    vpMapPointEdge.reserve(nExpectedSize/2);

    // add observations of frames in the temporal window
    for(deque<Frame*>::const_iterator qIt= mvpTemporalFrames.begin(), qItEnd=mvpTemporalFrames.end();
        qIt!=qItEnd; ++qIt)
    {
        vector<MapPoint*> vpMPs = (*qIt)->GetMapPointMatches();
        int jim=0;
        for(vector<MapPoint*>::const_iterator itMP=vpMPs.begin(), itEndMP=vpMPs.end();
            itMP!=itEndMP; ++itMP, ++jim)
        {
            MapPoint* pMP = *itMP;
            if(pMP && (pMP->mnTrackReferenceForFrame==mpCurrentFrame->mnId) //&& (!pMP->isBad())
                    && (pMP->mnObservationsInDoubleWindow>=2) )
                // if valid pointer, and in local map of points, and not marked as bad by optimizer
                // and has been observed AND used for optimization by at least X keyframes
            {
                //SET left EDGE
                Eigen::Matrix<double,2,1> obs;
                cv::KeyPoint kpUn = (*qIt)->mvKeysUn[jim];
                obs << kpUn.pt.x, kpUn.pt.y;
//                assert(kpUn.octave==0);
                const float invSigma2 = (*qIt)->GetInvSigma2(kpUn.octave);

                G2oEdgeProjectXYZ2UV* porter=addObsToG2o(obs, Eigen::Matrix2d::Identity()*invSigma2,
                                                         pMP->mnId + nOffset, (*qIt)->mnId*MAGIC2, true, delta, &optimizer);

                vpEdges.push_back(porter);
#ifndef MONO
                // set right edge
                kpUn = (*qIt)->mvRightKeysUn[jim];
                obs << kpUn.pt.x, kpUn.pt.y;
//                assert(kpUn.octave==0);
                porter=addObsToG2o(obs, Eigen::Matrix2d::Identity()*invSigma2,
                                   pMP->mnId + nOffset, (*qIt)->mnId*MAGIC2, true, delta, &optimizer, &mTl2r );
                vpEdges.push_back(porter);
#endif
                vnIndexEdge.push_back(make_pair(*qIt, jim));
                vpMapPointEdge.push_back(pMP);
            }
        }
    }
    // add observations from frames in the spatial window
    for(vector<KeyFrame*>::const_iterator vIt= mvpLocalKeyFrames.begin(), vEndIt=mvpLocalKeyFrames.end(); vIt!=vEndIt; ++vIt)
    {
        if((*vIt)->isBad())
            continue;
        vector<MapPoint*> vpMPs = (*vIt)->GetMapPointMatches();
        int jim=0;
        for(vector<MapPoint*>::const_iterator itMP=vpMPs.begin(), itEndMP=vpMPs.end();
            itMP!=itEndMP; ++itMP, ++jim)
        {
            MapPoint* pMP = *itMP;
            if(pMP && (pMP->mnTrackReferenceForFrame==mpCurrentFrame->mnId) //&& (!pMP->isBad())
                    && (pMP->mnObservationsInDoubleWindow>=2) )
                // if valid pointer, and in local map of points, and not marked as bad by optimizer
                // and has been observed AND used for optimization by at least X keyframes
            {
                //SET left EDGE
                Eigen::Matrix<double,2,1> obs;
                cv::KeyPoint kpUn = (*vIt)->mvKeysUn[jim];
                obs << kpUn.pt.x, kpUn.pt.y;

                const float invSigma2 = (*vIt)->GetInvSigma2(kpUn.octave);
                G2oEdgeProjectXYZ2UV* porter=addObsToG2o(obs, Eigen::Matrix2d::Identity()*invSigma2,
                                                         pMP->mnId + nOffset, (*vIt)->mnId*MAGIC2, true, delta, &optimizer );
                vpEdges.push_back(porter);
#ifndef MONO
                // set right edge
                kpUn = (*vIt)->mvRightKeysUn[jim];
                obs << kpUn.pt.x, kpUn.pt.y;

                porter=addObsToG2o(obs, Eigen::Matrix2d::Identity()*invSigma2,
                                   pMP->mnId + nOffset, (*vIt)->mnId*MAGIC2, true, delta, &optimizer, &mTl2r );
                vpEdges.push_back(porter);
#endif
                vnIndexEdge.push_back(make_pair(*vIt, jim));
                vpMapPointEdge.push_back(pMP);
            }
        }
    }

    vector<MapPoint*> vpMPs = mpCurrentFrame->GetMapPointMatches();
    int jim=0;
    for(vector<MapPoint*>::const_iterator itMP=vpMPs.begin(), itEndMP=vpMPs.end();
        itMP!=itEndMP; ++itMP, ++jim)
    {
        MapPoint* pMP = *itMP;
        if(pMP && (pMP->mnTrackReferenceForFrame==mpCurrentFrame->mnId) //&& (!pMP->isBad())
                && (pMP->mnObservationsInDoubleWindow>=2) )
            // if valid pointer, and in local map of points, and not marked as bad by optimizer
            // and has been observed AND used for optimization by at least X keyframes
        {
            //SET left EDGE
            Eigen::Matrix<double,2,1> obs;
            cv::KeyPoint kpUn = mpCurrentFrame->mvKeysUn[jim];
            obs << kpUn.pt.x, kpUn.pt.y;
//            assert(kpUn.octave==0);
            const float invSigma2 = mpCurrentFrame->GetInvSigma2(kpUn.octave);
            Eigen::Matrix2d infoMat=Eigen::Matrix2d::Identity()*invSigma2;

            G2oEdgeProjectXYZ2UV* porter=addObsToG2o(obs, infoMat, pMP->mnId + nOffset,
                                                     mpCurrentFrame->mnId*MAGIC2, true, delta, &optimizer );
            vpEdges.push_back(porter);
#ifndef MONO
            // set right edge
            kpUn = mpCurrentFrame->mvRightKeysUn[jim];
            obs << kpUn.pt.x, kpUn.pt.y;
//            assert(kpUn.octave==0);
            porter=addObsToG2o(obs, infoMat, pMP->mnId + nOffset, mpCurrentFrame->mnId*MAGIC2, true, delta, &optimizer, &mTl2r );
            vpEdges.push_back(porter);
#endif
            vnIndexEdge.push_back(make_pair(mpCurrentFrame.get(), jim));
            vpMapPointEdge.push_back(pMP);
            mpCurrentFrame->mvbOutlier[jim] = false;
            nInitialCorrespondences++;
        }
    }

    optimizer.initializeOptimization();
    const bool STRUCTURE_ONLY=true;
    if (STRUCTURE_ONLY)
    {
        //    cout << "Performing structure-only BA:" << endl;
        g2o::StructureOnlySolver<3> structure_only_ba;
        g2o::OptimizableGraph::VertexContainer points;
        for (g2o::OptimizableGraph::VertexIDMap::const_iterator it = optimizer.vertices().begin(); it != optimizer.vertices().end(); ++it) {
            g2o::OptimizableGraph::Vertex* v = static_cast<g2o::OptimizableGraph::Vertex*>(it->second);
            if (v->dimension() == 3)
                points.push_back(v);
        }
        structure_only_ba.calc(points, 5);
    }

    optimizer.optimize(5);

/// Emprically the following outlier removal process does not contribute much
#if 0
    // Check inlier observations, draw inspiration from LocalBundleAdjustment and PoseOptimization
    for(size_t i=0, iend=vpEdges.size(); i<iend;i++)
    {
        G2oEdgeProjectXYZ2UV* e = vpEdges[i];
        MapPoint* pMP = vpMapPointEdge[i/2];
        if(pMP->isBad())
            continue;

        if(e->chi2()>delta2 || !e->isDepthPositive())
        {
            Frame* pFi = vnIndexEdge[i/2].first;
            int idx= vnIndexEdge[i/2].second;
            if(pFi==mpCurrentFrame.get())
            {
                pFi->mvbOutlier[idx]=true;
                e->setInformation(Eigen::Matrix2d::Identity()*1e-10);
            }
            else{
                pFi->EraseMapPointMatch(idx);
                pMP->EraseObservation(pFi);
                optimizer.removeEdge(e);
                vpEdges[i]=NULL;
            }
        }
    }

    // Optimize again without the outliers
    optimizer.initializeOptimization();
    optimizer.optimize(3);
#endif
    int nBad=0;
    // Check inlier observations, this section draws inspiration from LocalBundleAdjustment and PoseOptimization by Raul
#ifdef MONO
    for(size_t i=0, iend=vpEdges.size(); i<iend;i++)
    {
        G2oEdgeProjectXYZ2UV* e = vpEdges[i];
        MapPoint* pMP = vpMapPointEdge[i];
        if(!e)
            continue;
        if(pMP->isBad())
            continue;
        Frame* pFi = vnIndexEdge[i].first;
        int idx= vnIndexEdge[i].second;
        if(pFi==mpCurrentFrame.get())
        {
            if(pFi->mvbOutlier[idx]){
                const float invSigma2 = pFi->GetInvSigma2(0);
                e->setInformation(Eigen::Matrix2d::Identity()*invSigma2);
                e->computeError();
            }
            if(e->chi2()>delta2)
            {
                pFi->mvbOutlier[idx]=true;
                // e->setInformation(Eigen::Matrix2d::Identity()*1e-10);
                nBad++;
            }
            else
            {
                pFi->mvbOutlier[idx]=false;
            }
        }
        else{
            if(e->chi2()>delta2 || !e->isDepthPositive())
            {
                pFi->EraseMapPointMatch(idx);
                pMP->EraseObservation(pFi);
            }
        }
    }
#else
    bool bLeftErased=false;
    for(size_t i=0, iend=vpEdges.size(); i<iend;i++)
    {
        G2oEdgeProjectXYZ2UV* e = vpEdges[i];
        MapPoint* pMP = vpMapPointEdge[i/MAGIC2];
        if(!e)
            continue;
        if(pMP->isBad())
            continue;
        Frame* pFi = vnIndexEdge[i/MAGIC2].first;
        int idx= vnIndexEdge[i/MAGIC2].second;
        if(i%MAGIC2==0)
            bLeftErased=false;
        if(pFi==mpCurrentFrame.get())
        {
            if(pFi->mvbOutlier[idx]){
                const float invSigma2 = pFi->GetInvSigma2(0);;
                e->setInformation(Eigen::Matrix2d::Identity()*invSigma2);
                e->computeError();
            }
            if(e->chi2()>delta2)
            {
                if(i%MAGIC2==0)
                    bLeftErased=true;
                else if(bLeftErased)
                    continue;
                pFi->mvbOutlier[idx]=true;
                // e->setInformation(Eigen::Matrix2d::Identity()*1e-10);
                nBad++;
            }
            else
            {
                pFi->mvbOutlier[idx]=false;
            }
        }
        else{
            if(e->chi2()>delta2 || !e->isDepthPositive())
            {
                if(i%MAGIC2==0)
                    bLeftErased=true;
                else if(bLeftErased)
                    continue;
                pFi->EraseMapPointMatch(idx);
//                pMP->EraseObservation(pFi);
            }
        }
    }
#endif
    if(mpLocalMapper->isStopped() || mpLocalMapper->stopRequested())
    {        //only update states of the current frame
        G2oVertexSE3 * vSE3= static_cast<G2oVertexSE3*>( GET_MAP_ELEM( mpCurrentFrame->mnId*MAGIC2, optimizer.vertices()));
        mpCurrentFrame->SetPose(vSE3->estimate());
        if(mbUseIMUData){
            G2oVertexSpeedBias * vsb= static_cast<G2oVertexSpeedBias*>( GET_MAP_ELEM( mpCurrentFrame->mnId*MAGIC2+1, optimizer.vertices()));
            mpCurrentFrame->speed_bias = vsb->estimate();
        }
    }
    else
        restoreDataFromG2o(optimizer);
//    cout<< "nInitCorresp and nBad"<< nInitialCorrespondences<<" "<<nBad<<endl;
    return nBad;
    //N.B. g2o will delete _algorithm, _parameters, EdgeSet and VertexIDMap upon destruction of SparseOptimizer
}

// task:double window optimization to refine poses and points of frames
// in temporal window of T consecutive frames
// (may include keyframes), spatial window of S keyframes, and the current frame
// vStereoMatches are matched point features between left and right image of current stereo pair

bool Tracking::TrackLocalMapDWO()
{
    // Tracking from previous frame or relocalisation was succesfull and we have an estimation
    // of the camera pose and some map points tracked in the frame.
    // Update Local Map and Track

    // Update Local Map
    // This is for visualization
    mpMap->SetReferenceMapPoints(mvpLocalMapPoints);    
    // Update
    UpdateReferenceKeyFramesAndPoints();

    // Search Local MapPoints
    int nObs=-1;
#ifdef MONO
    nObs=SearchReferencePointsInFrustum();
#else
    nObs=SearchReferencePointsInFrustumStereo();
#endif
    // Optimize Pose
   // mnMatchesInliers = Optimizer::PoseOptimization(mpCurrentFrame.get());
    int nBad = LocalOptimize();
    mnMatchesInliers= nObs-nBad;
    cout<<"Inliers after LO:"<< mnMatchesInliers<<endl;
#if 0
    if(nObs!=mnMatchesInliers)
    {
        int yum=0, zinc=0, iota=0;
        for(size_t i=0; i<mpCurrentFrame->mvpMapPoints.size(); ++i)
        {
            MapPoint *pMP=mpCurrentFrame->mvpMapPoints[i];
            if(pMP && !pMP->isBad())
            {
                ++yum;
                if(pMP->mnTrackReferenceForFrame == mpCurrentFrame->mnId)
                {
                    ++zinc;
                    if(pMP->mnObservationsInDoubleWindow >=3)
                        ++iota;
                }
            }
        }
        cout<<"yum zinc iota:"<<yum<<" "<<zinc<<" "<<iota<<endl;

    }
#endif
    // Update MapPoints Statistics
    for(size_t i=0; i<mpCurrentFrame->mvpMapPoints.size(); i++)
        if(mpCurrentFrame->mvpMapPoints[i])
        {
            if(!mpCurrentFrame->mvbOutlier[i])
                mpCurrentFrame->mvpMapPoints[i]->IncreaseFound();
        }

    // Decide if the tracking was succesful
    // More restrictive if there was a relocalization recently
    if(mpCurrentFrame->mnId<mnLastRelocFrameId+mMaxFrames && mnMatchesInliers<50)
        return false;

    if(mnMatchesInliers<12)
        return false;
    else
        return true;
}


bool Tracking::NeedNewKeyFrame()
{
    // If Local Mapping is freezed by a Loop Closure do not insert keyframes
    // Huai: TODO: is it possible to process new keyframes when loop closing is running? But it would be quite involved
    if(mpLocalMapper->isStopped() || mpLocalMapper->stopRequested())
        return false;

    // Not insert keyframes if not enough frames from last relocalisation have passed
    if(mpCurrentFrame->mnId<mnLastRelocFrameId+mMaxFrames && mpMap->KeyFramesInMap()>mMaxFrames)
        return false;

    // Reference KeyFrame MapPoints
    int nRefMatches = mpReferenceKF->TrackedMapPoints();

    // Local Mapping accept keyframes?
    bool bLocalMappingIdle = mpLocalMapper->AcceptKeyFrames();

    // Condition 1a: More than "MaxFrames" have passed from last keyframe insertion
    const bool c1a = mpCurrentFrame->mnId>=mnLastKeyFrameId+mMaxFrames;
    // Condition 1b: More than "MinFrames" have passed and Local Mapping is idle
    const bool c1b = mpCurrentFrame->mnId>=mnLastKeyFrameId+mMinFrames && bLocalMappingIdle;
    // Condition 2: Less than 90% of points than reference keyframe and enough inliers
    const float fPercent =0.9;
    const bool c2 = mnMatchesInliers<nRefMatches*fPercent && mnMatchesInliers>15;

    if((c1a||c1b)&&c2)
    {
        // If the mapping accepts keyframes insert, otherwise send a signal to interrupt BA, but not insert yet
        if(bLocalMappingIdle)
        {
            return true;
        }
        else
        {
            mpLocalMapper->InterruptBA();
            return false;
        }
    }
    else
        return false;
}
bool Tracking::NeedNewKeyFrameStereo()
{
    // FAQ: can keyframes be created during loop closure but not processing by local mapping?
    // If Local Mapping is freezed by a Loop Closure do not insert keyframes
    if(mpLocalMapper->isStopped() || mpLocalMapper->stopRequested())
        return false;

    // Not insert keyframes if not enough frames from last relocalisation have passed
    if(mpCurrentFrame->mnId<mnLastRelocFrameId+mMaxFrames && mpMap->KeyFramesInMap()>mMaxFrames)
        return false;

    // Reference KeyFrame MapPoints
    int nRefMatches = mpReferenceKF->TrackedMapPoints();

    // Local Mapping accept keyframes?
    bool bLocalMappingIdle = mpLocalMapper->AcceptKeyFrames();

    // Condition 1a: More than "MaxFrames" have passed from last keyframe insertion
    const bool c1a = mpCurrentFrame->mnId>=mnLastKeyFrameId+mMaxFrames;
    // Condition 1b: More than "MinFrames" have passed and Local Mapping is idle
    const bool c1b = mpCurrentFrame->mnId>=mnLastKeyFrameId+mMinFrames && bLocalMappingIdle;
    // Condition 2: Less than fPercent of points than reference keyframe and enough inliers
    const float fPercent =0.6;
    const bool c2 = (mnMatchesInliers< nRefMatches*fPercent) && mnMatchesInliers>15;
    int num_matchless_cells= point_stats.numFeatureLessCorners3x3(5);
    // need KF if 2/9 of the image is void of matches as in ScaViSLAM
    if((c1a||c1b)&&(c2 || num_matchless_cells>2))
    {
        // If the mapping accepts keyframes insert, otherwise send a signal to interrupt BA, but not insert yet
        if(bLocalMappingIdle)
        {
            return true;
        }
        else
        {
            mpLocalMapper->InterruptBA();
            return false;
        }
    }
    else
        return false;
}
// update poses of frames in the temporal window (excluding keyframes) and the current frame
void Tracking::UpdatePoseOfFrameInTemporalWindow()
{
    Sophus::SE3d Tnew2old= mpLoopClosing->GetTnew2old();
    for(deque<Frame*>::const_iterator qIt=mvpTemporalFrames.begin(), qEndIt= mvpTemporalFrames.end(); qIt!=qEndIt; ++qIt)
    {
        if((*qIt)->isKeyFrame())//updated in loop closure
            continue;
        (*qIt)->SetPose((*qIt)->GetPose()*Tnew2old);
    }
}
void Tracking::CreateNewKeyFrame()
{
    KeyFrame* pKF = new KeyFrame(*mpCurrentFrame,mpMap,mpKeyFrameDB);
    mnLastKeyFrameId = mpCurrentFrame->mnId;
    mpLastKeyFrame = pKF;
}

int Tracking::SearchReferencePointsInFrustum()
{
    int numObservs=0;
    // Do not search map points already matched
    for(vector<MapPoint*>::iterator vit=mpCurrentFrame->mvpMapPoints.begin(), vend=mpCurrentFrame->mvpMapPoints.end(); vit!=vend; vit++)
    {
        MapPoint* pMP = *vit;
        if(pMP)
        {
            if(pMP->isBad())
            {
                *vit = NULL;
            }
            else
            {
                pMP->IncreaseVisible();
                pMP->mnLastFrameSeen = mpCurrentFrame->mnId;
                pMP->mbTrackInView = false;
                ++numObservs;
            }
        }
    }

    mpCurrentFrame->UpdatePoseMatrices();// because in isInFrustum mRcw and mtcw is used

    int nToMatch=0;

    // Project points in frame and check its visibility
    for(vector<MapPoint*>::iterator vit=mvpLocalMapPoints.begin(), vend=mvpLocalMapPoints.end(); vit!=vend; vit++)
    {
        MapPoint* pMP = *vit;
        if(pMP->mnLastFrameSeen == mpCurrentFrame->mnId)
            continue;
        if(pMP->isBad())
            continue;        
        // Project (this fills MapPoint variables for matching)
        if(mpCurrentFrame->isInFrustum(pMP,0.5))
        {
            pMP->IncreaseVisible();
            nToMatch++;
        }
    }    


    if(nToMatch>0)
    {
        ORBmatcher matcher(0.8);
        int th = 1;
        // If the camera has been relocalised recently, perform a coarser search
        if(mpCurrentFrame->mnId<mnLastRelocFrameId+2)
            th=5;
        int nMatches=matcher.SearchByProjection(*mpCurrentFrame,mvpLocalMapPoints,th);
        numObservs+= nMatches;
    }
    return numObservs;
}
// match local map points with stereo matches between stereo images of current frame
// return how many points in the local map is observed in the current frame
// for now, we first check image space consistency to remove outlier,
// then match features between local map and current frame
// TODO: alternatively, we may first match features, then check image space consistency
// of these matches as suggest by Leutenegger et al., IJRR 2014
int Tracking::SearchReferencePointsInFrustumStereo()
{
    int numObservs=0;
    // Do not search map points already matched
    for(vector<MapPoint*>::iterator vit=mpCurrentFrame->mvpMapPoints.begin(), vend=mpCurrentFrame->mvpMapPoints.end();
        vit!=vend; ++vit)
    {
        MapPoint* pMP = *vit;
        if(pMP)
        {
            if(pMP->isBad())
            {
                *vit = NULL;
            }
            else
            {
                pMP->IncreaseVisible();
                pMP->mnLastFrameSeen = mpCurrentFrame->mnId;
                pMP->mbTrackInView = false;
//                ++pMP->mnObservationsInDoubleWindow; //Huai: count in the observation of current frame
                ++numObservs;
            }
        }
    }

    mpCurrentFrame->UpdatePoseMatrices();// because in isInFrustum mRcw and mtcw is used

    int nToMatch=0;
    // Project points in frame and check its visibility
    for(vector<MapPoint*>::iterator vit=mvpLocalMapPoints.begin(), vend=mvpLocalMapPoints.end(); vit!=vend; vit++)
    {
        MapPoint* pMP = *vit;
        if(pMP->mnLastFrameSeen == mpCurrentFrame->mnId)
            continue;
        if(pMP->isBad())
            continue;
        // Project (this fills MapPoint variables for matching)
        if(mpCurrentFrame->isInFrustumStereo(pMP,0.5))
        {
            pMP->IncreaseVisible();
            nToMatch++;
        }
    }

    if(nToMatch>0)
    {
        ORBmatcher matcher(0.8);
        int th = 1;
        // If the camera has been relocalised recently, perform a coarser search
        if(mpCurrentFrame->mnId<mnLastRelocFrameId+2)
            th=5;
        int nMatches= matcher.SearchByProjectionStereo(*mpCurrentFrame,mvpLocalMapPoints,th);
        numObservs+= nMatches;
    }
    return numObservs;
}

void Tracking::UpdateReference()
{    
    // This is for visualization
    mpMap->SetReferenceMapPoints(mvpLocalMapPoints);

    // Update
    UpdateReferenceKeyFrames();
    UpdateReferencePoints();
}

void Tracking::UpdateReferencePoints()
{
    mvpLocalMapPoints.clear();

    for(vector<KeyFrame*>::iterator itKF=mvpLocalKeyFrames.begin(), itEndKF=mvpLocalKeyFrames.end(); itKF!=itEndKF; itKF++)
    {
        KeyFrame* pKF = *itKF;
        vector<MapPoint*> vpMPs = pKF->GetMapPointMatches();

        for(vector<MapPoint*>::iterator itMP=vpMPs.begin(), itEndMP=vpMPs.end(); itMP!=itEndMP; itMP++)
        {
            MapPoint* pMP = *itMP;
            if(!pMP)
                continue;
            if(pMP->mnTrackReferenceForFrame==mpCurrentFrame->mnId)
                continue;
            if(!pMP->isBad())
            {
                mvpLocalMapPoints.push_back(pMP);
                pMP->mnTrackReferenceForFrame=mpCurrentFrame->mnId;
            }
        }
    }
}


void Tracking::UpdateReferenceKeyFrames()
{
    // Each map point vote for the keyframes in which it has been observed
    map<KeyFrame*,int> keyframeCounter;
    for(size_t i=0, iend=mpCurrentFrame->mvpMapPoints.size(); i<iend;i++)
    {
        if(mpCurrentFrame->mvpMapPoints[i])
        {
            MapPoint* pMP = mpCurrentFrame->mvpMapPoints[i];
            if(!pMP->isBad())
            {
                map<KeyFrame*,size_t> observations = pMP->GetObservations();
                for(map<KeyFrame*,size_t>::iterator it=observations.begin(), itend=observations.end(); it!=itend; it++)
                    keyframeCounter[it->first]++;
            }
            else
            {
                mpCurrentFrame->mvpMapPoints[i]=NULL;
            }
        }
    }

    int max=0;
    KeyFrame* pKFmax=NULL;

    mvpLocalKeyFrames.clear();
    mvpLocalKeyFrames.reserve(3*keyframeCounter.size());

    // All keyframes that observe a map point are included in the local map. Also check which keyframe shares most points
    for(map<KeyFrame*,int>::iterator it=keyframeCounter.begin(), itEnd=keyframeCounter.end(); it!=itEnd; it++)
    {      
        KeyFrame* pKF = it->first;

        if(pKF->isBad())
            continue;

        if(it->second>max)
        {
            max=it->second;
            pKFmax=pKF;
        }

        mvpLocalKeyFrames.push_back(pKF);
        pKF->mnTrackReferenceForFrame = mpCurrentFrame->mnId;
    }


    // Include also some not-already-included keyframes that are neighbors to already-included keyframes
    for(vector<KeyFrame*>::iterator itKF=mvpLocalKeyFrames.begin(), itEndKF=mvpLocalKeyFrames.end(); itKF!=itEndKF; itKF++)
    {
        // Limit the number of keyframes
        if(mvpLocalKeyFrames.size()>80)
            break;

        KeyFrame* pKF = *itKF;

        vector<KeyFrame*> vNeighs = pKF->GetBestCovisibilityKeyFrames(10);

        for(vector<KeyFrame*>::iterator itNeighKF=vNeighs.begin(), itEndNeighKF=vNeighs.end(); itNeighKF!=itEndNeighKF; itNeighKF++)
        {
            KeyFrame* pNeighKF = *itNeighKF;
            if(!pNeighKF->isBad())
            {
                if(pNeighKF->mnTrackReferenceForFrame!=mpCurrentFrame->mnId)
                {
                    mvpLocalKeyFrames.push_back(pNeighKF);
                    pNeighKF->mnTrackReferenceForFrame=mpCurrentFrame->mnId;
                    break;
                }
            }
        }

    }

    mpReferenceKF = pKFmax;
}

// update the temporal window,
// construct the spatial window of S keyframes excluding keyframes that are in the temporal window
// mark first estimates for variables

void Tracking::UpdateReferenceKeyFramesAndPoints()
{
    struct CompareKeyframe
    {
        bool operator()(std::pair<int, void*> a, std::pair<int, void*> b)
        {
            return a.first<b.first;
        }
    };
    //update temporal window
    if(mvpTemporalFrames.size() <= mnTemporalWinSize)
    {
//         assert(mvpOldLocalKeyFrames.size()==0 && mvpLocalKeyFrames.size()==0);
    }
    else
    {
        //remove observations for the frame that slips out
        Frame *pFrontFrame=mvpTemporalFrames.front();
        if(pFrontFrame->isKeyFrame()){
            ((KeyFrame*)pFrontFrame)->SetErase();
			pFrontFrame->SetFirstEstimate();
		}
        else
        {                  
            delete pFrontFrame;
        }       
        mvpTemporalFrames.pop_front();
    //    mvpTemporalFrames.front()->SetFirstEstimate();
    }
    //construct spatial window based on MapPoints in a Frame
    // Each map point vote for the keyframes in which it has been observed
    map<KeyFrame*,int> frameCounter;
    Frame * pF= mpCurrentFrame;
    
    for(size_t i=0, iend=pF->mvpMapPoints.size(); i<iend;i++)
    {
        MapPoint* pMP = pF->mvpMapPoints[i];
        if(pMP)
        {
            if(!pMP->isBad())
            {
                map<KeyFrame*,size_t> observations = pMP->GetObservations();
                for(map<KeyFrame*,size_t>::iterator it=observations.begin(), itend=observations.end(); it!=itend; it++)
                    frameCounter[it->first]++;
            }
            else
            {
                pF->mvpMapPoints[i]=NULL;
            }
        }
    }

    std::priority_queue<std::pair<int, KeyFrame*>, vector<std::pair<int, KeyFrame *> >, CompareKeyframe > swappedCounter;
    for(map<KeyFrame*,int>::iterator it=frameCounter.begin(), itEnd=frameCounter.end(); it!=itEnd; it++)
    {     
        swappedCounter.push(std::make_pair(it->second, (KeyFrame*)(it->first)));
    }
    if(!swappedCounter.empty())
        mpReferenceKF=swappedCounter.top().second;
    mvpOldLocalKeyFrames=mvpLocalKeyFrames;
    for(vector<KeyFrame*>::iterator iter=mvpOldLocalKeyFrames.begin();iter!=mvpOldLocalKeyFrames.end();++iter)
         (*iter)->SetErase();
    mvpLocalKeyFrames.clear();
    mvpLocalKeyFrames.reserve(mnSpatialWinSize);

    for(deque<Frame*>::iterator qIt=mvpTemporalFrames.begin(), qEndIt= mvpTemporalFrames.end();
        qIt!=qEndIt; ++qIt)
    {
        if((*qIt)->isKeyFrame()){
            ((KeyFrame*)(*qIt))->mnTrackReferenceForFrame= mpCurrentFrame->mnId;
            ((KeyFrame*)(*qIt))->SetNotErase();
        }
    }
    // S keyframes that observe a map point are included in the local map.
    int numKeyFrames=0;
    while(!swappedCounter.empty()){
        std::pair<int, KeyFrame*> it= swappedCounter.top();
        swappedCounter.pop();
        if(it.second->mnTrackReferenceForFrame == mpCurrentFrame->mnId){//exclude keyframes in the temporal window
            continue;
        }
        else{
            KeyFrame* pKF = it.second;
            if(pKF->isBad())
                continue;
            mvpLocalKeyFrames.push_back(pKF);
            pKF->SetNotErase();
            pKF->mnTrackReferenceForFrame = mpCurrentFrame->mnId;
            ++numKeyFrames;
            if(numKeyFrames==mnSpatialWinSize)
                break;
        }
    }

    if(numKeyFrames<mnSpatialWinSize){
        // Include also some not-already-included keyframes that are neighbors to already-included keyframes
        for(vector<KeyFrame*>::iterator itKF=mvpLocalKeyFrames.begin(), itEndKF=mvpLocalKeyFrames.end(); itKF!=itEndKF; itKF++)
        {
            KeyFrame* pKF = *itKF;
            vector<KeyFrame*> vNeighs = pKF->GetBestCovisibilityKeyFrames(4);
            for(vector<KeyFrame*>::iterator itNeighKF=vNeighs.begin(), itEndNeighKF=vNeighs.end(); itNeighKF!=itEndNeighKF; itNeighKF++)
            {
                KeyFrame* pNeighKF = *itNeighKF;
                if(!pNeighKF->isBad())
                {
                    if(pNeighKF->mnTrackReferenceForFrame!=mpCurrentFrame->mnId)
                    {
                        mvpLocalKeyFrames.push_back(pNeighKF);
                        pNeighKF->SetNotErase();
                        pNeighKF->mnTrackReferenceForFrame=mpCurrentFrame->mnId;
                        ++numKeyFrames;
                        break;//Huai: at most one is chosen?
                    }
                }
            }
            if(numKeyFrames== mnSpatialWinSize)
                break;
        }
    }
    assert(numKeyFrames<=mnSpatialWinSize);
    // given keyframes in temporal window, update map points which are to be searched for in the current frame
    mvpLocalMapPoints.clear();
    mvpLocalMapPoints.reserve(mnFeatures);
    for(deque<Frame*>::iterator itF=mvpTemporalFrames.begin(), itEndF=mvpTemporalFrames.end(); itF!=itEndF; ++itF)
    {
        if(!((*itF)->isKeyFrame())) continue;
        vector<MapPoint*> vpMPs = ((KeyFrame*)(*itF))->GetMapPointMatches();

        for(vector<MapPoint*>::iterator itMP=vpMPs.begin(), itEndMP=vpMPs.end(); itMP!=itEndMP; ++itMP)
        {
            MapPoint* pMP = *itMP;
            if(!pMP)
                continue;
            if(pMP->mnTrackReferenceForFrame==mpCurrentFrame->mnId){
                ++pMP->mnObservationsInDoubleWindow;
                continue;
            }
            if(!pMP->isBad())
            {
                mvpLocalMapPoints.push_back(pMP);
                pMP->mnTrackReferenceForFrame=mpCurrentFrame->mnId;
                pMP->mnObservationsInDoubleWindow=1;
            }
        }
    }
    // given keyframes in spatial window, update map points which are to be searched for in the current frame
    for(vector<KeyFrame*>::iterator itKF=mvpLocalKeyFrames.begin(), itEndKF=mvpLocalKeyFrames.end(); itKF!=itEndKF; itKF++)
    {
        KeyFrame* pKF = *itKF;
        vector<MapPoint*> vpMPs = pKF->GetMapPointMatches();
        for(vector<MapPoint*>::iterator itMP=vpMPs.begin(), itEndMP=vpMPs.end(); itMP!=itEndMP; itMP++)
        {
            MapPoint* pMP = *itMP;
            if(!pMP)
                continue;
            if(pMP->mnTrackReferenceForFrame==mpCurrentFrame->mnId){
                ++pMP->mnObservationsInDoubleWindow;
                continue;
            }
            if(!pMP->isBad())
            {
                mvpLocalMapPoints.push_back(pMP);
                pMP->mnTrackReferenceForFrame=mpCurrentFrame->mnId;
                pMP->mnObservationsInDoubleWindow=1;
            }
        }
    }

    // FEJ technique subjects to a constant condition: all MapPoints not observed by the double window are marked as having first estimate
    for(size_t jean=0; jean<mvpOldLocalKeyFrames.size(); ++jean){
        size_t kate=0;
        for(; kate<mvpLocalKeyFrames.size(); ++kate){
            if(mvpOldLocalKeyFrames[jean]== mvpLocalKeyFrames[kate])
                break;
        }
        if(kate==mvpLocalKeyFrames.size())//jean slips out of spatial window
        {
            vector<MapPoint*> vpMPs = mvpOldLocalKeyFrames[jean]->GetMapPointMatches();

            for(vector<MapPoint*>::iterator itMP=vpMPs.begin(), itEndMP=vpMPs.end(); itMP!=itEndMP; ++itMP)
            {
                MapPoint* pMP = *itMP;
                if(!pMP)
                    continue;
                if(pMP->mnTrackReferenceForFrame==mpCurrentFrame->mnId)
                    continue;
                if(!pMP->isBad() )
                {
                    pMP->SetFirstEstimate();
                }
            }
        }
    }
}

bool Tracking::Relocalisation()
{
    mvpOldLocalKeyFrames.clear();
    mvpLocalKeyFrames.clear();
    for(deque<Frame*>::iterator qIt= mvpTemporalFrames.begin(); qIt!=mvpTemporalFrames.end(); ++qIt)
    {
        if((*qIt)->isKeyFrame()==false){            
            delete (*qIt);
        }
    }
    mvpTemporalFrames.clear();
    // Compute Bag of Words Vector
    mpCurrentFrame->ComputeBoW();

    // Relocalisation is performed when tracking is lost and forced at some stages during loop closing
    // Track Lost: Query KeyFrame Database for keyframe candidates for relocalisation
    vector<KeyFrame*> vpCandidateKFs;
    if(!RelocalisationRequested())
        vpCandidateKFs= mpKeyFrameDB->DetectRelocalisationCandidates(mpCurrentFrame.get());
    else // Forced Relocalisation: Relocate against local window around last keyframe
    { 
        boost::mutex::scoped_lock lock(mMutexForceRelocalisation);
        mbForceRelocalisation = false;
        vpCandidateKFs.reserve(10);
        vpCandidateKFs = mpLastKeyFrame->GetBestCovisibilityKeyFrames(9);
        vpCandidateKFs.push_back(mpLastKeyFrame);
    }

    if(vpCandidateKFs.empty())
        return false;

    const int nKFs = vpCandidateKFs.size();

    // We perform first an ORB matching with each candidate
    // If enough matches are found we setup a PnP solver
    ORBmatcher matcher(0.75,true);

    vector<PnPsolver*> vpPnPsolvers;
    vpPnPsolvers.resize(nKFs);

    vector<vector<MapPoint*> > vvpMapPointMatches;
    vvpMapPointMatches.resize(nKFs);

    vector<bool> vbDiscarded;
    vbDiscarded.resize(nKFs);

    int nCandidates=0;

    for(size_t i=0; i<vpCandidateKFs.size(); i++)
    {
        KeyFrame* pKF = vpCandidateKFs[i];
        if(pKF->isBad())
            vbDiscarded[i] = true;
        else
        {
            int nmatches = matcher.SearchByBoW(pKF,*mpCurrentFrame,vvpMapPointMatches[i]);
            if(nmatches<15)
            {
                vbDiscarded[i] = true;
                continue;
            }
            else
            {
                PnPsolver* pSolver = new PnPsolver(*mpCurrentFrame,vvpMapPointMatches[i]);
                pSolver->SetRansacParameters(0.99,10,300,4,0.5,5.991);
                vpPnPsolvers[i] = pSolver;
                nCandidates++;
            }
        }        
    }

    // perform some iterations of P4P RANSAC
    // Until we found a camera pose supported by enough inliers
    bool bMatch = false;
    ORBmatcher matcher2(0.9,true);

    while(nCandidates>0 && !bMatch)
    {
        for(size_t i=0; i<vpCandidateKFs.size(); i++)
        {
            if(vbDiscarded[i])
                continue;

            // Perform 5 Ransac Iterations
            vector<bool> vbInliers;
            int nInliers;
            bool bNoMore;

            PnPsolver* pSolver = vpPnPsolvers[i];
            cv::Mat Tcw = pSolver->iterate(5,bNoMore,vbInliers,nInliers);

            // If Ransac reachs max. iterations discard keyframe
            if(bNoMore)
            {
                vbDiscarded[i]=true;
                nCandidates--;
            }

            // If a Camera Pose is computed, optimize
            if(!Tcw.empty())
            {
                mpCurrentFrame->SetPose(Converter::toSE3d(Tcw));

                set<MapPoint*> sFound;

                for(size_t j=0; j<vbInliers.size(); j++)
                {
                    if(vbInliers[j])
                    {
                        mpCurrentFrame->mvpMapPoints[j]=vvpMapPointMatches[i][j];
                        sFound.insert(vvpMapPointMatches[i][j]);
                    }
                    else
                        mpCurrentFrame->mvpMapPoints[j]=NULL;
                }

                int nGood = Optimizer::PoseOptimization(mpCurrentFrame.get());

                if(nGood<10)
                    continue;

                for(size_t io =0, ioend=mpCurrentFrame->mvbOutlier.size(); io<ioend; io++)
                    if(mpCurrentFrame->mvbOutlier[io])
                        mpCurrentFrame->mvpMapPoints[io]=NULL;

                // If few inliers, search by projection in a coarse window and optimize again
                if(nGood<50)
                {
                    int nadditional =matcher2.SearchByProjection(*mpCurrentFrame,vpCandidateKFs[i],sFound,10,100);

                    if(nadditional+nGood>=50)
                    {
                        nGood = Optimizer::PoseOptimization(mpCurrentFrame.get());

                        // If many inliers but still not enough, search by projection again in a narrower window
                        // the camera has been already optimized with many points
                        if(nGood>30 && nGood<50)
                        {
                            sFound.clear();
                            for(size_t ip =0, ipend=mpCurrentFrame->mvpMapPoints.size(); ip<ipend; ip++)
                                if(mpCurrentFrame->mvpMapPoints[ip])
                                    sFound.insert(mpCurrentFrame->mvpMapPoints[ip]);
                            nadditional =matcher2.SearchByProjection(*mpCurrentFrame,vpCandidateKFs[i],sFound,3,64);

                            // Final optimization
                            if(nGood+nadditional>=50)
                            {
                                nGood = Optimizer::PoseOptimization(mpCurrentFrame.get());

                                for(size_t io =0; io<mpCurrentFrame->mvbOutlier.size(); io++)
                                    if(mpCurrentFrame->mvbOutlier[io])
                                        mpCurrentFrame->mvpMapPoints[io]=NULL;
                            }
                        }
                    }
                }


                // If the pose is supported by enough inliers stop ransacs and continue
                if(nGood>=50)
                {                    
                    bMatch = true;
                    break;
                }
            }
        }
    }

    if(!bMatch)
    {
        return false;
    }
    else
    {
        mnLastRelocFrameId = mpCurrentFrame->mnId;
        return true;
    }

}
// forced and/or requested relocalization is only done by loop closing
void Tracking::ForceRelocalisation()
{
    boost::mutex::scoped_lock lock(mMutexForceRelocalisation);
    mbForceRelocalisation = true;
    mnLastRelocFrameId = mpCurrentFrame->mnId;
}

bool Tracking::RelocalisationRequested()
{
    boost::mutex::scoped_lock lock(mMutexForceRelocalisation);
    return mbForceRelocalisation;
}
void Tracking::Reset()
{
    {
        boost::mutex::scoped_lock lock(mMutexReset);
        mbPublisherStopped = false;
        mbReseting = true;
    }

    // Wait until publishers are stopped
//    ros::Rate r(500);
    while(1)
    {
        {
            boost::mutex::scoped_lock lock(mMutexReset);
            if(mbPublisherStopped)
                break;
        }
            boost::this_thread::sleep(boost::posix_time::milliseconds(1));
//        r.sleep();
    }
    for(deque<Frame*>::iterator qIt= mvpTemporalFrames.begin(); qIt!=mvpTemporalFrames.end(); ++qIt)
    {
        if((*qIt)->isKeyFrame()==false)
            delete (*qIt);
    }
    mvpTemporalFrames.clear();
    // Reset Local Mapping
    mpLocalMapper->RequestReset();
    // Reset Loop Closing
    mpLoopClosing->RequestReset();
    // Clear BoW Database
    mpKeyFrameDB->clear();
    // Clear Map (this erase MapPoints and KeyFrames)
    mpMap->clear();

    KeyFrame::nNextKeyId = 0;
    Frame::nNextId = 0;
    mState = NOT_INITIALIZED;

    {
        boost::mutex::scoped_lock lock(mMutexReset);
        mbReseting = false;
    }
}

void Tracking::CheckResetByPublishers()
{
    bool bReseting = false;

    {
        boost::mutex::scoped_lock lock(mMutexReset);
        bReseting = mbReseting;
    }

    if(bReseting)
    {
        boost::mutex::scoped_lock lock(mMutexReset);
        mbPublisherStopped = true;
    }

    // Hold until reset is finished
//    ros::Rate r(500);
    while(1)
    {
        {
            boost::mutex::scoped_lock lock(mMutexReset);
            if(!mbReseting)
            {
                mbPublisherStopped=false;
                break;
            }
        }
            boost::this_thread::sleep(boost::posix_time::milliseconds(1));
//        r.sleep();
    }
}
bool Tracking::isInTemporalWindow(const Frame* pFrame)const
{
    return (pFrame->mnId>=mvpTemporalFrames.front()->mnId&&
            pFrame->mnId<=mvpTemporalFrames.back()->mnId);
}


} //namespace ORB_SLAM
